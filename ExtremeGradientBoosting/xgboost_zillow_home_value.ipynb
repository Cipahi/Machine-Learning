{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Xgboost Parameter Tuning \n",
    "###### using Zillow Home Value from Kaggle as an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read in raw data, remember download the data from https://www.kaggle.com/c/zillow-prize-1/data and change path that fit in your \n",
    "PATH = '/Users/johnnychiu/Desktop/MyFiles/learning/kaggle/9.Zillow-Home-Value-Prediction'\n",
    "properties = pd.read_csv(PATH + '/input/properties_2016.csv')\n",
    "train = pd.read_csv(PATH + '/input/train_2016_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parcelid                          int64\n",
       "airconditioningtypeid           float64\n",
       "architecturalstyletypeid        float64\n",
       "basementsqft                    float64\n",
       "bathroomcnt                     float64\n",
       "bedroomcnt                      float64\n",
       "buildingclasstypeid             float64\n",
       "buildingqualitytypeid           float64\n",
       "calculatedbathnbr               float64\n",
       "decktypeid                      float64\n",
       "finishedfloor1squarefeet        float64\n",
       "calculatedfinishedsquarefeet    float64\n",
       "finishedsquarefeet12            float64\n",
       "finishedsquarefeet13            float64\n",
       "finishedsquarefeet15            float64\n",
       "finishedsquarefeet50            float64\n",
       "finishedsquarefeet6             float64\n",
       "fips                            float64\n",
       "fireplacecnt                    float64\n",
       "fullbathcnt                     float64\n",
       "garagecarcnt                    float64\n",
       "garagetotalsqft                 float64\n",
       "hashottuborspa                   object\n",
       "heatingorsystemtypeid           float64\n",
       "latitude                        float64\n",
       "longitude                       float64\n",
       "lotsizesquarefeet               float64\n",
       "poolcnt                         float64\n",
       "poolsizesum                     float64\n",
       "pooltypeid10                    float64\n",
       "pooltypeid2                     float64\n",
       "pooltypeid7                     float64\n",
       "propertycountylandusecode        object\n",
       "propertylandusetypeid           float64\n",
       "propertyzoningdesc               object\n",
       "rawcensustractandblock          float64\n",
       "regionidcity                    float64\n",
       "regionidcounty                  float64\n",
       "regionidneighborhood            float64\n",
       "regionidzip                     float64\n",
       "roomcnt                         float64\n",
       "storytypeid                     float64\n",
       "threequarterbathnbr             float64\n",
       "typeconstructiontypeid          float64\n",
       "unitcnt                         float64\n",
       "yardbuildingsqft17              float64\n",
       "yardbuildingsqft26              float64\n",
       "yearbuilt                       float64\n",
       "numberofstories                 float64\n",
       "fireplaceflag                    object\n",
       "structuretaxvaluedollarcnt      float64\n",
       "taxvaluedollarcnt               float64\n",
       "assessmentyear                  float64\n",
       "landtaxvaluedollarcnt           float64\n",
       "taxamount                       float64\n",
       "taxdelinquencyflag               object\n",
       "taxdelinquencyyear              float64\n",
       "censustractandblock             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "Before we start, we would like to transform the dataframe for a bit for us to apply that to xgboost. Using the code from the kernel(https://www.kaggle.com/aharless/xgb-w-o-outliers-lgb-with-outliers-combined/code), we managed to do it. Keep in mind that this part isn't the main focus of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for XGBoost ...\n",
      "Shape train: (90275, 57)\n",
      "Shape test: (2985217, 57)\n",
      "After removing outliers:\n",
      "Shape train: (88525, 57)\n",
      "Shape test: (2985217, 57)\n"
     ]
    }
   ],
   "source": [
    "# process data for xgboost\n",
    "\n",
    "print(\"\\nProcessing data for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c] = properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid'], axis=1)\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "# drop out ouliers & downsize the train data by filter extreme cases\n",
    "train_df = train_df[train_df.logerror > -0.4]\n",
    "train_df = train_df[train_df.logerror < 0.418]\n",
    "\n",
    "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print('After removing outliers:')\n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsizing the dataframe\n",
    "The original train size is huge. We take a random sample of it for the purpose of illustrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop out ouliers & downsize the train data by random sampling\n",
    "import random\n",
    "random.seed(33)\n",
    "sel_parcelid = random.sample(train_df.parcelid.unique(), 200)\n",
    "train_df = train_df[train_df['parcelid'].isin(sel_parcelid)].reset_index(drop=True)\n",
    "\n",
    "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "After we transformed our dataframe, we can now start to do the parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "###### description\n",
    "* Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3\n",
    "* should work for different problems. Determine the optimum number of trees for this learning rate.\n",
    "* XGBoost has a very useful function called as \"cv\" which performs cross-validation at each boosting iteration and thus\n",
    "* returns the optimum number of trees required. \n",
    "\n",
    "###### note\n",
    "* to determine the optimum number of trees for this learning rate, the parameter for the \"optimum number of trees\" is\n",
    "* called n_estimators in XGBClassifier; num_boost_round in xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    "    learning_rate=0.2,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:linear',\n",
    "    n_jobs=4,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_n_estimators(alg, x_train, y_train, cv_folds=5, early_stopping_rounds=10, fitting_model=False):\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                      metrics='mae', early_stopping_rounds=early_stopping_rounds)\n",
    "    print cvresult\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    if fitting_model==True:\n",
    "        # Fit the algorithm on the data\n",
    "        alg.fit(x_train, y_train, eval_metric='mae')\n",
    "\n",
    "        # Predict training set:\n",
    "        # dtrain_predictions = alg.predict(x_train)\n",
    "        dtrain_predprob = alg.predict_proba(x_train)[:, 1]\n",
    "\n",
    "        # Print model report:\n",
    "        print \"\\nModel Report\"\n",
    "        # print \"Accuracy : %.4g\" % metrics.accuracy_score(y_train, dtrain_predictions)\n",
    "        # print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob)\n",
    "        # http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n",
    "        print \"MAE (Train): %f\" % metrics.mean_squared_error(y_train, dtrain_predprob)\n",
    "\n",
    "    return alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
      "0        0.393911      0.000265        0.393915       0.000048\n",
      "1        0.315719      0.000293        0.315702       0.000028\n",
      "2        0.254037      0.000287        0.253986       0.000054\n",
      "3        0.205583      0.000276        0.205486       0.000073\n",
      "4        0.167691      0.000234        0.167546       0.000065\n",
      "5        0.138250      0.000232        0.138057       0.000065\n",
      "6        0.115566      0.000254        0.115335       0.000075\n",
      "7        0.098282      0.000278        0.098006       0.000093\n",
      "8        0.085276      0.000300        0.084966       0.000082\n",
      "9        0.075682      0.000261        0.075331       0.000091\n",
      "10       0.068708      0.000252        0.068324       0.000066\n",
      "11       0.063783      0.000231        0.063363       0.000059\n",
      "12       0.060324      0.000195        0.059879       0.000065\n",
      "13       0.057940      0.000183        0.057477       0.000047\n",
      "14       0.056315      0.000179        0.055811       0.000041\n",
      "15       0.055208      0.000166        0.054673       0.000047\n",
      "16       0.054455      0.000161        0.053881       0.000047\n",
      "17       0.053951      0.000153        0.053343       0.000048\n",
      "18       0.053610      0.000156        0.052970       0.000052\n",
      "19       0.053373      0.000157        0.052698       0.000061\n",
      "20       0.053211      0.000170        0.052506       0.000059\n",
      "21       0.053101      0.000170        0.052370       0.000066\n",
      "22       0.053035      0.000179        0.052265       0.000071\n",
      "23       0.052969      0.000177        0.052181       0.000074\n",
      "24       0.052924      0.000168        0.052113       0.000081\n",
      "25       0.052898      0.000166        0.052060       0.000081\n",
      "26       0.052879      0.000168        0.052016       0.000081\n",
      "27       0.052866      0.000165        0.051977       0.000082\n",
      "28       0.052854      0.000162        0.051937       0.000077\n",
      "29       0.052838      0.000164        0.051898       0.000079\n",
      "30       0.052834      0.000160        0.051868       0.000078\n",
      "31       0.052829      0.000156        0.051844       0.000074\n",
      "32       0.052832      0.000156        0.051821       0.000070\n",
      "33       0.052831      0.000151        0.051794       0.000069\n",
      "34       0.052827      0.000154        0.051766       0.000066\n",
      "35       0.052826      0.000143        0.051741       0.000066\n"
     ]
    }
   ],
   "source": [
    "xgb1=find_n_estimators(xgb1, x_train, y_train, fitting_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Tune max_depth and min_child_weight\n",
    "###### description\n",
    "* We tune these first as they will have the highest impact on model outcome. To start with, let's set wider ranges\n",
    "* and then we will perform another iteration for smaller ranges.\n",
    "\n",
    "###### note\n",
    "* GridSearchCV documentation -> http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "* scoring parameters -> http://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb1,\n",
    " param_grid = param_test1, scoring='neg_mean_squared_error',iid=False, cv=5)\n",
    "gsearch1.fit(x_train, y_train)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearch_parameter_tuning(xgb, param_test):\n",
    "    gsearch1 = GridSearchCV(estimator = xgb1,\n",
    "    param_grid = param_test1, scoring='neg_mean_squared_error',iid=False, cv=5)\n",
    "    gsearch1.fit(x_train, y_train)\n",
    "    print('cv result:{}'.format(gsearch1.cv_results_)) \n",
    "    print('best parameters:{}'.format(gsearch1.best_params_)) \n",
    "    print('best score:{}'.format(gsearch1.best_score_)) \n",
    "    \n",
    "    for key in param_test.keys():        \n",
    "        xgb.set_params(key=gsearch1.best_params_[key])\n",
    "\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb2 = gridsearch_parameter_tuning(xgb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go one step deeper and look for optimum values. We'll search for values 1 above and below the optimum values,\n",
    "because we took an interval of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[3,4,5,6],\n",
    " 'min_child_weight':[1,2,3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb2 = gridsearch_parameter_tuning(xgb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Tune gamma\n",
    "###### description\n",
    "* Now lets tune gamma value using the parameters already tuned above. Gamma can take various values but I'll check\n",
    "* for 5 values here. You can go into more precise values as.\n",
    "###### note\n",
    "* gamma [default=0] A node is split only when the resulting split gives a positive reduction in the loss function.\n",
    "* Gamma specifies the minimum loss reduction required to make a split. Higher gamma makes the algorithm conservative.\n",
    "* The values can vary depending on the loss function and should be tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "xgb3 = gridsearch_parameter_tuning(xgb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Tune subsample and colsample_bytree\n",
    "###### description\n",
    "* The next step would be try different subsample and colsample_bytree values. Lets do this in 2 stages as well and\n",
    "* take values 0.6,0.7,0.8,0.9 for both to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "xgb4 = gridsearch_parameter_tuning(xgb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should try values in 0.05 interval around the optimum value we just got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(3,7)]\n",
    "}\n",
    "xgb5 = gridsearch_parameter_tuning(xgb4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Tuning Regularization Parameters\n",
    "###### description\n",
    "* Next step is to apply regularization to reduce overfitting. Though many people don't use this parameters much as\n",
    "* gamma provides a substantial way of controlling complexity. But we should always try it. I'll tune 'reg_alpha'\n",
    "* value here and leave it up to you to try different values of 'reg_lambda'.\n",
    "* Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "xgb6 = gridsearch_parameter_tuning(xgb5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 0.005, 0.01, 0.03, 0.05]\n",
    "}\n",
    "xgb7 = gridsearch_parameter_tuning(xgb6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6: Reducing Learning Rate\n",
    "###### description\n",
    "* Lastly, we should lower the learning rate and add more trees. Lets use the cv function of XGBoost to do the job again.\n",
    "* Lower the learning rate and decide the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb7.set_params(n_estimators=100)\n",
    "xgb7.set_params(learning_rate = 0.1)\n",
    "modelfit(xgb7, x_train, y_train, fitting_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb7.set_params(n_estimators=200)\n",
    "xgb7.set_params(learning_rate = 0.05)\n",
    "modelfit(xgb7, x_train, y_train, fitting_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb7.set_params(n_estimators=300)\n",
    "xgb7.set_params(learning_rate = 0.03)\n",
    "modelfit(xgb7, x_train, y_train, fitting_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parameter tuning, our score decreases from ? to ?. These are the parameters that we have tried our best to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "new_xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'num_boost_round': 105,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 2,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'objective': 'reg:linear',\n",
    "    'nthread': 4,\n",
    "    'scale_pos_weight': 1,\n",
    "    'seed': 27,\n",
    "    'alpha': 0.01,\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "print(\"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(new_xgb_params), dtrain, num_boost_round=105)\n",
    "\n",
    "print(\"\\nPredicting with XGBoost ...\")\n",
    "xgb_pred = model.predict(dtest)\n",
    "\n",
    "print(\"\\nXGBoost predictions:\")\n",
    "print(pd.DataFrame(xgb_pred).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreparing results for write ...\")\n",
    "y_pred = []\n",
    "\n",
    "for i, predict in enumerate(xgb_pred):\n",
    "    y_pred.append(str(round(predict, 4)))\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "                       '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "                       '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\nWriting results to disk ...\")\n",
    "output.to_csv(\n",
    "    PATH + '/20170726_XGB-LGB-combined/_submission/sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')),\n",
    "    index=False)\n",
    "\n",
    "print(\"\\nFinished ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference\n",
    "* https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "* https://www.kaggle.com/aharless/xgb-w-o-outliers-lgb-with-outliers-combined/code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}