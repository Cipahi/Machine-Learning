{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [Dataset](#data)\n",
    "* [NLP Process](#process)\n",
    "* [Some Further Explaination](#explain)\n",
    "    * [N-gram Model](#ngram)\n",
    "    * [TF-IDF](#tfidf)\n",
    "* [Sentiment Analysis Implementation](#implement)\n",
    "    * Unigrams (absence/presence)\n",
    "    * Unigrams with frequency count\n",
    "    * Unigrams (only adjectives/adverbs)\n",
    "    * Unigrams (sublinear tf-idf)\n",
    "    * Bigrams (absence/presence)\n",
    "    * [Other function to do text preprocessing](#other_function)\n",
    "    * [Conclusion](#conclusion)    \n",
    "* [Reference](#refer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johnny 2018-12-26 00:45:15 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "\n",
      "pandas 0.20.3\n",
      "numpy 1.13.3\n",
      "nltk 3.2.4\n",
      "sklearn 0.19.1\n",
      "seaborn 0.8.0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, re, os\n",
    "from nltk import pos_tag\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline  \n",
    "\n",
    "%watermark -a 'Johnny' -d -t -v -p pandas,numpy,nltk,sklearn,seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='intro'>Introduction</a>\n",
    "\n",
    "Sentiment Analysis is a very popular text analytics application. Here, we are going to use sentiment anaylsis that automatlically classifies the sentiment of a movie review as either positive or negative. \n",
    "\n",
    "Similar to from the [previous post about Näive Bayes](http://nbviewer.jupyter.org/github/johnnychiuchiu/Machine-Learning/blob/master/NaiveBayes/naiveBayesTextClassification.ipynb), we'll also use Näive Bayes. The idea and topic we want to focus more in this post, however, is to experiment with several ways to do text pre-processing before fitting Näive Bayes model. Some of the [ideas](http://nbviewer.jupyter.org/github/johnnychiuchiu/Machine-Learning/blob/master/NaiveBayes/naiveBayesTextClassification.ipynb#improve) were also mentioned in the previous post, \n",
    "\n",
    "In this post, while doing an end-to-end sentiment anaylsis, we want to focus more on different ways to do text preprocessing and why we choose a particular way to do it (i.e., pros and cons), with the goal of having a better classification outcome. \n",
    "\n",
    "Some of the topics includes\n",
    "* Text Preprocessing\n",
    "    * Keep capitalization or not\n",
    "    * Removing punctuation or not\n",
    "    * Removing stopwords or not\n",
    "    * Different ways to do Tokenization \n",
    "    * Stemming / Lemmatization    \n",
    "* Part of Speech (POS) Tagging\n",
    "* N-gram\n",
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dataset'>Data Set</a>\n",
    "\n",
    "The dataset used in this project is the [polarity dataset v2.0 (3.0Mb, click to download)](http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz): 1000 positive and 1000 negative movie reviews. Introduced in Pang/Lee ACL 2004 and released June 2004, it is drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB.\n",
    "\n",
    "In this post, we divide the corpus with a ratio 90:10 split for training and testing (200 review files are used as test – i.e., files that start with cv9) and we keep an even distribution of positive and negative labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filepath, review_type):    \n",
    "    \"\"\"This funtion reads in the data, parses it, and separates it into \n",
    "    the appropriate train and test splits. Data is read in in UTF-8, and is\n",
    "    parsed by removing all punctuation. Any review that begins with the \n",
    "    filename 'cv9' is considered to be part of the test set.\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to the corpus\n",
    "        review_type: type of review included in the data within this filepath\n",
    "    Returns:\n",
    "        Returns four separate lists. The test set corpus, the test set labels,\n",
    "        the training set corpus, and the training set labels. \n",
    "        \n",
    "    The function is copied from here (https://github.com/spalmerg/Sentiment-Analysis/blob/master/Sentiment%20Analysis.ipynb)    \n",
    "    \"\"\"\n",
    "    test, test_labels = [], []\n",
    "    train, train_labels = [], []\n",
    "    \n",
    "    for filename in os.listdir(filepath):\n",
    "        with open(filepath + filename, 'rb') as review:\n",
    "            txt = review.read().decode('utf8', 'surrogateescape')\n",
    "            txt = txt.replace(\"--\", \"\").replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "            translator = str.maketrans('', '', string.punctuation)\n",
    "            txt = txt.translate(translator)\n",
    "            txt = txt.split()\n",
    "            txt = ' '.join(txt)\n",
    "            if filename.startswith('cv9'):\n",
    "                test.append(txt)\n",
    "                test_labels.append(review_type)\n",
    "            else: \n",
    "                train.append(txt)\n",
    "                train_labels.append(review_type)\n",
    "    return(test, test_labels, train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in positive and negative reviews\n",
    "neg_test, neg_test_labels, neg_train, neg_train_labels = read_data('review_polarity/txt_sentoken/neg/', 'Negative')\n",
    "pos_test, pos_test_labels, pos_train, pos_train_labels = read_data('review_polarity/txt_sentoken/pos/', 'Positive')\n",
    "\n",
    "# combine training sets\n",
    "train_labels = neg_train_labels + pos_train_labels\n",
    "train = neg_train + pos_train\n",
    "\n",
    "# combine test sets\n",
    "test_labels = neg_test_labels + pos_test_labels\n",
    "test = neg_test + pos_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **First look at the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An exmaple of a positive review**: everyone loves the movie *You've got mail*, including the person who wrote this review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youve got mail works alot better than it deserves to in order to make the film a success all they had to do was cast two extremely popular and attractive stars have them share the screen for about two hours and then collect the profits no real acting was involved and there is not an original or inventive bone in its body its basically a complete re shoot of the shop around the corner only adding a few modern twists essentially it goes against and defies all concepts of good contemporary filmmaking its overly sentimental and at times terribly mushy not to mention very manipulative but oh how enjoyable that manipulation is but there must be something other than the casting and manipulation that makes the movie work as well as it does because i absolutely hated the previous ryanhanks teaming sleepless in seattle it couldnt have been the directing because both films were helmed by the same woman i havent quite yet figured out what i liked so much about youve got mail but then again is that really important if you like something so much why even question it again the storyline is as cliched as they come tom hanks plays joe fox the insanely likeable owner of a discount book chain and meg ryan plays kathleen kelley the even more insanely likeable proprietor of a family run childrens book shop called in a nice homage the shop around the corner fox and kelley soon become bitter rivals because the new fox books store is opening up right across the block from the small business little do they know they are already in love with each other over the internet only neither party knows the other persons true identity the rest of the story isnt important because all it does is serve as a mere backdrop for the two stars to share the screen sure there are some mildly interesting subplots but they all fail in comparison to the utter cuteness of the main relationship all of this of course leads up to the predictable climax but as foreseeable as the ending is its so damn cute and well done that i doubt any movie in the entire year contains a scene the evokes as much pure joy as this part does when ryan discovers the true identity of her online love i was filled with such for lack of a better word happiness that for the first time all year i actually left the theater smiling\n"
     ]
    }
   ],
   "source": [
    "print(pos_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An exmaple of a negative review**: we can tell this guy didn't like the movie at all just based on the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thats exactly how long the movie felt to me there werent even nine laughs in nine months its a terrible mess of a movie starring a terrible mess of a man mr hugh grant a huge dork its not the whole oral sexprostitution thing referring to grant not me that bugs me its the fact that grant is annoying not just adam sandler annoying were talking jim carrey annoying since when do eye flutters and nervous smiles pass for acting but on the other hand since when do really bad slapstick a fistfight in the delivery room culminating in grants head in joan cusacks lapa scene he paid 60 to have included in the movie and obscene double entendres robin williams the obstetrician tells grants pregnant girlfriend she has a big pussy referring of course to the size of the cat hairs on her coat but nonetheless grant paid 60 to have the exchange included in the movie pass for comedy nine months is a predictable cookie cutter movie with no originality in humor or plot hugh grant plays a successful child psychiatrist why a child psychologist so the scriptwriters could inject the following unfunny exchange kid my dads an asshole grant flutters eyelashes offers a nervous smile then responds in his annoying english accent and i think i actually have talent attitude could you possibly elaborate on that kid my dads a huge asshole more like a hugh asshole but thats beside the point which is nine months includes too many needlessly stupid jokes that get laughs from the ten year olds in the audience while everyone else shakes his or her head in disbelief so anyway grant finds out his girlfriend is pregnant and does his usual reaction fluttered eyelashes nervous smiles this paves the way for every possible pregnancychild birth gag in the book especially since grants equally annoying friends wife is also pregnant the annoying friend is played by tom arnold who provides most of the cacophonous slapstick none of which is funny such as a scene where arnold beats up a costumed arnie the dinosaur you draw your own parallels on that one in a toy store the only interesting character in the movie is played by jeff goldblum who should have hid himself away somewhere after the dreadful hideaway as an artist with a fear of and simultaneous longing for commitment not even robin williams who plays a russian doctor who has recently decided to switch from veterinary medicine to obstetrics has much humor his is a one joke character the old foreign guy who mispronounces english stereotype did someone say yakov smirnov thats my favorite vodka by the way hence the line now its time to take a look at your volvo another nasty but unamusing joke except this one goes right over the ten year olds heads while the adults simultaneously groan nine months is a complete failure low on laughs and intelligence and high on loud unfunny slapstick failed jokes and other uninspired lunacy hugh grants sunset boulevard arrest please no caught with his pants down jokes may bring more people into the theaters but they certainly wont leave with a smile on their faces not after 90 minutes of grants nervous smiles everything in the movie is so forced so unauthentic that anyone with an i q over 80 sorry hugh will know they wasted their money on an unfulfilled desire but at least they didnt spend 60 bucks for it'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_train[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Statistics**: These reviews seems to be more serious and much more lengthy then I thought before checking them. Let's also take a look at some of the statistics about review length. We can see from below that the average length of these reviews are ~3600 characters, with the longest one of 14k; shortest one with only 86 characters. Overall, we can also see that positive reviews are generally longer (based on median), which would be different from product reviews, where people would tend to write longer reviews if they hate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1800.000000\n",
      "mean      3631.452222\n",
      "std       1605.006488\n",
      "min         86.000000\n",
      "25%       2551.500000\n",
      "50%       3397.500000\n",
      "75%       4372.500000\n",
      "max      14174.000000\n",
      "Name: Review Length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x109f33898>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEFCAYAAAAfRLtkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3X9UVPed//HnAIKBgSppzca4xBD1\nGBdIA0Zwo6zWWkzapJHYoLQkFs+2dVsVs0kgGND42xjJdmWN8ScnGOqPRqNp3TQriRI1omeaKLBq\n17TqEbX+QA0zIj9m5vuHXyYSIVwN8wPm9TjHc+7c+TDznvHOvOZz7+d+rsnpdDoRERExIMDbBYiI\nSOeh0BAREcMUGiIiYphCQ0REDFNoiIiIYUHeLsDdLBaLt0sQEemUEhISblrX5UMDWn/hIiLStrZ+\ncGv3lIiIGKbQEBERwxQaIiJimEJDREQMU2iISKdXUVFBRUWFt8vwCwoNEen0SkpKKCkp8XYZfkGh\nISKdWkVFBZWVlVRWVqq34QEKDRHp1G7sYai34X5uDY2DBw+SkZHRYt17771HWlqa6/bGjRtJTU3l\n6aef5qOPPgKgpqaGzMxM0tPTycrKoq6urs22IiLiOW4LjZUrV/Lyyy9TX1/vWnf48GF+//vf03zd\np/Pnz1NcXMz69etZvXo1BQUFNDQ0sGzZMn70ox9RUlLCoEGD2LBhQ5ttRcS/paent7os7uG20IiK\nimLp0qWu25cuXeK1114jNzfXte7QoUM89NBDBAcHEx4eTlRUFEeOHMFisTB8+HAAkpOT2bt3b5tt\nRcS/xcbGEhMTQ0xMDLGxsd4up8tz29xTKSkpnDp1CgC73c6MGTPIzc0lJCTE1cZqtRIeHu66HRYW\nhtVqbbE+LCyM2traNtsaoUkLRbq25vnl9Fl3P49MWFhVVcWJEyeYNWsW9fX1HDt2jHnz5pGUlITN\nZnO1s9lshIeHYzabsdlsdO/eHZvNRkREhGvdV9saoQkLRbo2fcY7nlcnLIyLi+OPf/wjxcXFFBQU\n0K9fP2bMmEFcXBwWi4X6+npqa2v5/PPPGTBgAPHx8ezatQuAsrIyEhIS2mwrIiKe49Wp0b/zne+Q\nkZFBeno6TqeT6dOnExISwuTJk8nOzmbjxo307NmTJUuWEBoa2mpbERHxHJOzeShTF2WxWNR1FRG5\nRW19d+rkPhERMUyhISIihik0RETEMIWGiIgYptAQERHDFBoiImKYQkNERAxTaIiIiGEKDRERMUyh\nISIihik0RETEMIWGiHR6FRUVVFRUeLsMv6DQEJFOr6SkhJKSEm+X4RcUGiLSqVVUVFBZWUllZaV6\nGx6g0BCRTu3GHoZ6G+6n0BAREcMUGiLSqaWnp7e6LO7h1cu9ioh8U7GxsYSGhrqWxb3U0xCRTq2i\nooKrV69y9epVHQj3AIWGiHRqOhDuWW4NjYMHD5KRkQHA4cOHSU9PJyMjg0mTJnHhwgUANm7cSGpq\nKk8//TQfffQRADU1NWRmZpKenk5WVhZ1dXVtthUREc9xW2isXLmSl19+mfr6egDmzZtHXl4excXF\njB49mpUrV3L+/HmKi4tZv349q1evpqCggIaGBpYtW8aPfvQjSkpKGDRoEBs2bGizrYj4Nx0I9yy3\nhUZUVBRLly513S4oKOCBBx4AwG63ExISwqFDh3jooYcIDg4mPDycqKgojhw5gsViYfjw4QAkJyez\nd+/eNtuKiH+LjY0lJiaGmJgYHQj3ALeNnkpJSeHUqVOu27169QLgz3/+M+vWrePtt9/m448/Jjw8\n3NUmLCwMq9WK1Wp1rQ8LC6O2trbFuhvbGmGxWDriJYmIj0pISAD0WfcEjw653b59O2+88QYrVqwg\nMjISs9mMzWZz3W+z2QgPD3et7969OzabjYiIiDbbGtG8QYlI16TPeMdrK4A9Nnpq69atrFu3juLi\nYv7xH/8RgLi4OCwWC/X19dTW1vL5558zYMAA4uPj2bVrFwBlZWUkJCS02VZERDzHIz0Nu93OvHnz\nuPvuu5kyZQoADz/8MFOnTiUjI4P09HScTifTp08nJCSEyZMnk52dzcaNG+nZsydLliwhNDS01bYi\nIuI5JqfT6fR2Ee5ksVjUdRURuUVtfXfq5D4RETFMoSEiIoYpNERExDCFhoiIGKbQEBERwxQaIiJi\nmEJDREQMU2iIiIhhCg0RETFMoSEinV5FRYUu9eohCg0R6fRKSkp0qVcPUWiISKdWUVFBZWUllZWV\n6m14gEJDDNMuAPFFN/Yw1NtwP4WGGKZdACKi0BBDtAtAfFV6enqry+IeCg0xRLsAxFfFxsYSExND\nTEwMsbGx3i6ny/PoNcJFRNwhKSnJ2yX4DfU0xBDtAhBftm/fPvbt2+ftMvyCQkNEOjUdb/MshYYY\nomMa4qu0bXqWW0Pj4MGDZGRkAHDixAkmTJhAeno6M2fOxOFwAFBYWMi4ceMYP348hw4duuW2IuLf\nrFZrq8viHm4LjZUrV/Lyyy9TX18PwIIFC8jKyqKkpASn00lpaSlVVVXs37+fTZs2UVBQwCuvvHLL\nbcUzdExDfJXJZGp1WdzDbaERFRXF0qVLXberqqoYMmQIAMnJyezduxeLxcKwYcMwmUz07t0bu91O\nTU3NLbUVz9CwRvFVTqez1WVxD7cNuU1JSeHUqVOu206n0/UrICwsjNraWqxWKz169HC1aV5/K20j\nIyPbrcVisXTUy/JrCQkJgN5P8S1Xrlxpsazt0708dp5GQMCXnRqbzUZERARmsxmbzdZifXh4+C21\nNaL5y06+meDgYAD1NMSnNO8Cb17W571jtBW+Hhs9NWjQIMrLywEoKytj8ODBxMfHs3v3bhwOB6dP\nn8bhcBAZGXlLbcVzVqxYwYoVK7xdhkgLvXr1anVZ3MNjPY3s7Gzy8vIoKCggOjqalJQUAgMDGTx4\nMGlpaTgcDvLz82+5rXhGRUUFx48fdy2rtyG+4he/+AW5ubmuZXEvk7OLHzmyWCzqrnaAKVOmuEKj\nb9++LQY5iHhTRUWFKzTmz5+vHzQdpK3vTkO7p65evcqZM2c4ffq065/4l3PnzrW6LOJtOrnPs9rd\nPVVYWMjq1avp2bOna53JZKK0tNSthYlvueuuu/jb3/7mWhYR/9RuaGzevJkPP/ywRWiI/xk1ahSr\nVq1yLYv4ivT0dNfuKZ146n7t7p7q1auX4aGt0nXdOIOoZhMV8V9t9jQKCwsBiIiIIC0tjeTkZAID\nA133/+Y3v3F/dSIi7fjqMY0FCxZ4sZqur92eRlxcHCNHjmwRGOJ/NPeU+CpNWOhZbfY0mnsSW7Zs\nYezYsS3ue/vtt91blYiIQZqw0LPaDI2ioiKsVivr16+nurratd5ut/Pee+/x05/+1CMFim/QLgDx\nVZqw0LPa3D3Vt2/fVtcHBwezcOFCd9UjInJLrl271uqyuEebPY0RI0YwYsQIHn30Ue6//35P1iQ+\nSMMaxVddunSp1WVxj3bP05g8eTJ2u91122Qy0b17d6Kjo8nOzuaee+5xa4HiG5qvp9G8LOIrdEzD\ns9oNjeTkZPr06cO4ceMA2LZtGxUVFXzve99jxowZFBUVubtG8RHqYYgv6tGjB2fPnnUti3u1O+TW\nYrEwceJEzGYzZrOZ9PR0jh49yujRo1tc/ERExBu6d+/e6rK4R7uhERAQwMcff+y6/fHHHxMcHMyF\nCxdoampya3HiW0pKSjQhnPgcs9nc6rK4R7u7pxYsWEBOTg7PP/88cP3a3wsXLmTDhg1kZma6vUDx\nDRUVFVRWVrqWdVxDfEVUVJRr24yKivJyNV2f4etpXLlyhcDAwE6X5LqeRsd46aWXXB/MmJgYnach\nPmPcuHGuS76GhITw+9//3ssVdQ1tfXe229P43//9X5YvX86VK1danDjz1ltvdWyFIiK3oaGhodVl\ncY92QyM7O5u0tDT69++v4Wx+LCkpydXTSEpK8nI1Il8KCgqisbHRtSzu1e473L17d372s595ohbx\nYV+dGv3HP/6xF6sR+VKfPn1cFwjr06ePl6vp+todPTVs2DCKi4v529/+psu9+rEb5x+7cVnE2268\nKJguEOZ+7fY0tm7dCsDatWtd6273cq+NjY3k5ORQXV1NQEAAc+bMISgoiJycHEwmE/3792fmzJkE\nBARQWFjIzp07CQoKIjc3l7i4OE6cONFqW3G/y5cvt7os4m03fheVlpaqF+xm7YbGhx9+2GFPtmvX\nLpqamli/fj179uzhP/7jP2hsbCQrK4vExETy8/MpLS2ld+/e7N+/n02bNnHmzBmmTJnCO++8w4IF\nC25qO3r06A6rT0Q6nxv3fGgviPu1+zP9ypUrvPzyyzzzzDNcvnyZl156iS+++OK2nuy+++7Dbrfj\ncDiwWq0EBQVRVVXFkCFDgOtTluzduxeLxcKwYcMwmUz07t0bu91OTU1Nq23FM26cnkFTNYgvcTgc\nrS6Le7Tb08jLy+ORRx7h0KFDhIaG0qtXL55//nlWrFhxy08WGhpKdXU1jz76KJcuXWL58uUcOHDA\nNSorLCyM2tparFZriy+m5vVOp/OmtkZYLJZbrlVaioiIcM0gGhERofdUfMaNE6ra7XZtm27Wbmic\nOnWKtLQ0fve73xEcHMz06dN54oknbuvJioqKGDZsGP/+7//OmTNnePbZZ11D5QBsNhsRERGYzWZs\nNluL9eHh4S2OXzS3NUIn931zwcHBrqnRf/nLX+qMcPEZX70Ikz7vHaOt8G03NAIDA6mtrXX9wj9+\n/PhtH3yOiIigW7duAHzrW9+iqamJQYMGUV5eTmJiImVlZSQlJREVFcXixYuZNGkSZ8+exeFwEBkZ\n2Wpb8YzY2FjX/7sCQ260Zs0a9uzZ47Xn/2poTJo0yWu1ADzyyCNdeoqldkNj6tSpZGRkcObMGf7t\n3/6Nzz77jHnz5t3Wk02cOJHc3FzS09NpbGxk+vTpxMTEkJeXR0FBAdHR0aSkpBAYGMjgwYNJS0vD\n4XCQn58PXD/R8KttxTO2bt3q2l+8detWjVARnxEWFubaMxEWFublaro+Q3NP1dTUcOjQIex2Ow8+\n+CDf/va3PVFbh9DcUx0jLS2Nq1evAtePTW3YsMHLFYl86fHHHwfgvffe83IlXcdtzz0FEBkZyYgR\nI1y3H3/8cf3niIjPUA/Dc27r4MSpU6c6ug7xcTdetU9X8BNfExYWpuDwkNsKDU1c6H+io6NbXRYR\n/6I5OMSQG8/LuZ1zdESka2jzmMbAgQNb7VHceIKd+I8bd0lq96SI/2ozNI4cOeLJOsTH3Xg9eF0b\nXsR/afeUiIgYptAQQ26cBUDT0Yv4r3Y//TdOBib+SzOJiggYOLlv1KhRxMfHM2LECJKTkzUttoiI\nH2s3NHbs2IHFYqGsrIy1a9cSGhrKiBEj+Nd//VdP1Cc+wmQyuSaG0+g5Ef/V7u6poKAg+vfvT2xs\nLPHx8VRXV/P+++97ojbxITeebaszb0X8V7s9jccee4wvvviCxx57jKFDhzJt2jTD17GQrqO+vr7V\nZRHxL+2GxrPPPsu+ffvYv38/Fy9e5OLFiyQmJtK3b18PlCe+4qtXRxMR/9RuaKSlpbmua7Ft2zaW\nLVvGrFmzOHz4sCfqE7x/kZvWePNCN139Ijcivqzd0Fi/fj2ffPIJhw4dYuDAgWRmZraYJl38Q0RE\nBJcvX3Yti4h/ajc0jh07xk9+8hMWL15McHCwJ2qSr8jMzPSJX9bNV+srLi72ciUi4i3tjp568cUX\nqaioIC8vD6vVSmFhIQ0NDZ6oTXxMRESEehkifq7d0Jg9ezZ1dXVUVVURGBjIyZMnyc3N9URt4mOC\ng4PV2xTxc+2GRlVVFc899xxBQUHccccdLFq0SDPgioj4qXaPaZhMJhoaGlxnAV+6dOkbnRH85ptv\n8uGHH9LY2MiECRMYMmQIOTk5mEwm+vfvz8yZMwkICKCwsJCdO3cSFBREbm4ucXFxnDhxotW2IiLi\nGe1+4z7zzDP8/Oc/5/z588ybN4+nnnqKZ5999raerLy8nE8//ZTf/e53FBcXc/bsWRYsWEBWVhYl\nJSU4nU5KS0upqqpi//79bNq0iYKCAl555RWAVtuKiIjntNvTePLJJ4mJiaG8vBy73c4bb7zBwIED\nb+vJdu/ezYABA/j1r3+N1WrlxRdfZOPGjQwZMgSA5ORk9uzZw3333cewYcMwmUz07t0bu91OTU0N\nVVVVN7UdPXr0bdUiIiK3rs3Q+Oijjxg5ciTvvvsu8OV8Q0eOHOHIkSM8+eSTt/xkly5d4vTp0yxf\nvpxTp04xefLkFpePDQsLo7a2FqvV2mI23eb1rbU1wmKx3HKtcrPm6UP0foqv0bbpOW2GRkVFBSNH\njqS8vLzV+28nNHr06EF0dDTBwcFER0cTEhLC2bNnXffbbDYiIiIwm83YbLYW68PDw1scv2hua0RC\nQsIt1yo3CwkJAfR+iu/Rttnx2grgNkNj6tSpwPXraYwYMYKgoHb3ZLUrISGBt956i5///OecO3eO\nuro6hg4dSnl5OYmJiZSVlZGUlERUVBSLFy9m0qRJnD17FofDQWRkJIMGDbqprYiIeE67SbBt2zZm\nz57NyJEjeeKJJ75Rko8cOZIDBw4wbtw4nE4n+fn59OnTh7y8PAoKCoiOjiYlJYXAwEAGDx7smvMq\nPz8fgOzs7JvaioiI55iczVfW+RpWq5UdO3bw3//935w8eZIxY8Ywbdo0T9T3jVksFnVZO0jzJIWr\nV6/2ciUiLWnb7HhtfXca2udkNptJSEjg7NmznDlzhk8//bTDCxQREd/XbmisXbuWP/zhDzQ0NPDE\nE0+wYsUK/uEf/sETtYmIiI9pNzT+/ve/M3fuXB544AFP1CMiIj6s3TPCc3JyOHbsGK+//jp1dXWu\n8zZERMT/tBsar732Grt27eKDDz6gqamJd955h4ULF3qiNhER8THthsbu3btZvHgxISEhhIeHs3bt\nWsrKyjxRm4iI+Jh2Q6P5LOzm6TsaGho0s6yIiJ9q90D4mDFjyMrK4sqVKxQVFbFt2zZ++MMfeqI2\nERHxMe2Gxi9+8Qs+/vhjevfuzZkzZ5gyZQojR470RG0iIuJjvjY0/vrXvxIWFsbw4cMZPnw4ABcv\nXiQ/P5/Zs2d7pEAREfEdbR6cWLp0KU899RRjxoxh7969wPVT9EePHk11dbXHChQREd/RZk/j3Xff\n5U9/+hPnzp3jP//zP1mzZg1///vf+e1vf+vqdYiIiH9pMzTCwsLo1asXvXr14tChQzz55JO8+eab\nBAYGerI+EWnDiy++yMWLF71dhk+4cOEC8OXEhf7uzjvv5NVXX3XLY7cZGjcOq+3Zsyc5OTluKUBE\nbs/Fixc5d/4cAXd882vddHaOgOuTdV+w1ni5Eu9z1DW59fHb3Nqaz8sA6N69u1uLEJHbE3BHED3H\nRHm7DPEhl94/6dbHbzM0/u///o9Ro0YB1yctbF5uvk53aWmpWwsTERHf02Zo/OlPf/JkHSIi0gm0\nGRr33HOPJ+sQEZFOQJNIiYiIYQoNERExzCtj9S5evEhqaipr1qwhKCiInJwcTCYT/fv3Z+bMmQQE\nBFBYWMjOnTsJCgoiNzeXuLg4Tpw40Wpbd9JY+C9pLHxL7hwLL+KrPB4ajY2N5Ofnu4bxLliwgKys\nLBITE8nPz6e0tJTevXuzf/9+Nm3a5Jok8Z133mm17ejRo91a78WLFzl37jymbne49Xk6A+f/75ie\nv2T1ciXe52ys83YJIl7h8dBYtGgR48ePZ8WKFQBUVVUxZMgQAJKTk9mzZw/33Xcfw4YNw2Qy0bt3\nb+x2OzU1Na22dXdoAJi63YG53xNufx7pPKzHtnm7BBGv8GhobN68mcjISIYPH+4KjebzPuD61CW1\ntbVYrVZ69Ojh+rvm9a21NcJisdx2zfX19bf9t9K11dfXf6NtqyOeX6Q17tw2PRoa77zzDiaTiU8+\n+YTDhw+TnZ1NTc2Xp/3bbDYiIiIwm83YbLYW68PDw1scv2hua0RCQsJt1xwSEgJXG2/776XrCgkJ\n+UbbVkc8f22jrf2G4nc6YttsK3Q8Onrq7bffZt26dRQXF/PAAw+waNEikpOTKS8vB6CsrIzBgwcT\nHx/P7t27cTgcnD59GofDQWRkJIMGDbqprYiIeI7XZzrLzs4mLy+PgoICoqOjSUlJITAwkMGDB5OW\nlobD4SA/P7/NtiIi4jleC43i4mLX8rp16266f8qUKUyZMqXFuvvuu6/VtiL+yGq14qhrcvsEddK5\nOOqasOK+EY46uU9ERAzz+u4pEbk9ZrOZazRoanRp4dL7JzGbzW57fPU0RETEMIWGiIgYptAQERHD\ndEyjHVarFWdjnaaNkBacjXVYNQWX+CH1NERExDD1NNphNpupa0QTFkoL1mPb3DpCRcRXqachIiKG\nKTRERMQwhYaIiBim0BAREcMUGiIiYphGT4l0Yprl9jpHgx2AgOBAL1fifY66JnDjwD6Fhkgndeed\nd3q7BJ9x4cIFAL5tjvRyJT7A7N5tQ6FhgM4Iv85pbwDAFBjs5Uq8z9lYh1t/zhnw6quvevX5fcmk\nSZMAWL16tZcr6foUGu3Qr7kvuX7N9dRJbWDWtiF+SaHRDv2a+5J+zYmIRk+JiIhhCg0RETHMo7un\nGhsbyc3Npbq6moaGBiZPnky/fv3IycnBZDLRv39/Zs6cSUBAAIWFhezcuZOgoCByc3OJi4vjxIkT\nrbYVERHP8Og37rZt2+jRowclJSWsXLmSOXPmsGDBArKysigpKcHpdFJaWkpVVRX79+9n06ZNFBQU\n8MorrwC02lZERDzHo6ExZswYpk2b5rodGBhIVVUVQ4YMASA5OZm9e/disVgYNmwYJpOJ3r17Y7fb\nqampabWtiIh4jkd3T4WFhQHXr4Y3depUsrKyWLRoESaTyXV/bW0tVquVHj16tPi72tpanE7nTW2N\nsFgsHfxK/FN9fT2g91N8j7ZNz/H4kNszZ87w61//mvT0dB5//HEWL17sus9msxEREYHZbMZms7VY\nHx4e3uL4RXNbIxISEjruBfixkJAQQO+n+B5tmx2vrQD26O6pCxcukJmZyQsvvMC4ceMAGDRoEOXl\n5QCUlZUxePBg4uPj2b17Nw6Hg9OnT+NwOIiMjGy1rYiIeI5HexrLly/niy++YNmyZSxbtgyAGTNm\nMHfuXAoKCoiOjiYlJYXAwEAGDx5MWloaDoeD/Px8ALKzs8nLy2vRVkREPMfkdDqd3i7CnSwWi7qs\nHURnhIuv0rbZ8dr67tRJDiIiYphCQ0REDFNoiIiIYQoNERExTKEhIiKGKTRERMQwhYaIiBim0BAR\nEcMUGiIiYphCQ0REDFNoiIiIYQoNERExTKEhIiKGefwiTCLStaxZs4Y9e/Z4tYYLFy4AX852602P\nPPIImZmZ3i7DbRQaItLpde/e3dsl+A2Fhoh8I5mZmV36l7W0pGMaIiJimEJDREQMU2iIiIhhCg0R\nETGs0x0IdzgczJo1i6NHjxIcHMzcuXO59957vV2WiIhf6HShsWPHDhoaGtiwYQOfffYZCxcu5I03\n3vB2WW7lC+PgwXfGwnf1cfAivqzThYbFYmH48OEAfPe736WystLLFfkPjYUXkU4XGlarFbPZ7Lod\nGBhIU1MTQUFtvxSLxeKJ0tzmwQcf5MEHH/R2GT6ls/+finRWnS40zGYzNpvNddvhcHxtYAAkJCS4\nuywRkS6lrR9mnW70VHx8PGVlZQB89tlnDBgwwMsViYj4j07X0xg9ejR79uxh/PjxOJ1O5s+f7+2S\nRET8RqcLjYCAAGbPnu3tMkRE/FKn2z0lIiLeo9AQERHDFBoiImKYQkNERAzrdAfCb4dOBBMR6Rgm\np9Pp9HYRIiLSOWj3lIiIGKbQEBERwxQaIiJimEJDREQMU2iIiIhhCg0RETFMoeEHysvLGTx4MGfO\nnHGte+2119i8efM3fuz6+no2bdoEwObNmyktLf3Gjyn+p7y8nKFDh5KRkUFGRgZPP/00xcXFt/QY\nv/nNbwA4evQoBw4cAGD69Ok0NDR0eL3+TKHhJ7p168ZLL71ER5+Wc/78eVdopKamMmrUqA59fPEf\nSUlJFBcXU1xczLp161i7di1ffPGF4b8vLCwE4IMPPuDYsWMAvP766wQHB7ulXn/lF2eEy/UPpMPh\n4O233+ZnP/uZa31xcTF/+MMfMJlMPPbYYzzzzDOcOHGCnJwcgoKCuOeee6iurnZ9kD/44AOampoI\nDw9n6dKlLF++nGPHjlFYWIjT6eTb3/42x48fZ+DAgYwdO5bz58/zy1/+ks2bN7NkyRIOHDiA0+lk\n4sSJPProo158R8SXWa1WAgIC+Mtf/sKSJUsIDAwkJCSEOXPmcOeddzJt2jSsVivXrl3jhRdeIDEx\nkUceeYTNmzezZcsWunXrxj/90z+RlZXFtm3bGDt2LFu3biU0NJRVq1YRFBRESkoKeXl51NfXux77\n7rvv9vZL93nqafiRWbNmUVRUxPHjxwGoq6tj+/btlJSUUFJSwo4dO/jrX//Kq6++yq9+9SuKi4uJ\nj48Hrl9W9/LlyxQVFVFSUkJTUxMVFRX86le/ol+/fq5dAwBPP/00W7ZsAWDr1q2kpqaya9cuTp06\nxfr163nrrbdYvnz5Lf2KlK5v3759ZGRk8Mwzz/DCCy+Ql5fH/Pnzyc/PZ926dUyYMIGFCxdy8uRJ\nLly4wPLly1myZAnXrl1zPcZdd93F2LFjmThxInFxccD1XvYPfvADPvjgAwC2b9/Oj3/8YxYtWkRG\nRgbFxcVMmjSJ1157zSuvu7NRT8OP9OzZk9zcXHJycoiPj+fq1aucPn2aiRMnAnDlyhVOnjzJ559/\nzkMPPQRcv776e++9R0BAAN26deO5554jNDSUs2fP0tTU1Orz3H///djtdqqrq9m+fTtFRUVs2LCB\nqqoqMjIyAGhqauL06dNERER45LWL70tKSuL1119vsW7GjBk88MADADz88MMsWbKE/v3789Of/pTn\nnnuOpqYm1zb1dX7yk58wa9YsoqOj6du3Lz179uQvf/kLb775JqtWrcLpdNKtWze3vK6uRqHhZ773\nve/xP//zP2zZssXVS1i1ahUQaKuzAAADwElEQVQmk4mioiIGDBjAgAED+PTTT/mXf/kXDh48CMCR\nI0fYsWMHmzZtoq6ujtTUVJxOJwEBATgcjpueZ9y4cSxevJh+/foRERFBdHQ0iYmJzJkzB4fDwbJl\ny+jTp4+nX750Mr169eLIkSMMHDiQAwcO0LdvX44ePYrNZmPFihWcO3eO8ePHM3LkSNffmEymm7bJ\nvn374nQ6WbVqFRMmTAAgOjqazMxM4uPj+fzzz10Hz+XrKTT80IwZM9i3bx/h4eEMHTqUCRMm0NDQ\nQFxcHHfddRfPP/88ubm5rFmzhvDwcIKCgrj33nu54447SE1NJTg4mO985zucO3eOhx56iMbGRhYv\nXkz37t1dzzFmzBjmzZvHG2+8AVwPq/3795Oens7Vq1f5/ve/j9ls9tZbIJ3E3LlzmTNnDk6nk8DA\nQObPn0+vXr34r//6L9599126devG1KlTW/xNTEwMr776Kvfff3+L9ePGjeO3v/0tSUlJAGRnZzNr\n1izq6+u5du0aM2bM8Njr6sw0y63cZNu2bTz44IPce++9bNq0iT//+c8sWLDA22WJiA9QT0Nucvfd\ndzN9+nTuuOMOAgICmD9/vrdLEhEfoZ6GiIgYpiG3IiJimEJDREQMU2iIiIhhCg2RDlReXv61J5vl\n5OTc0kSR7T2eiKcpNERExDCFhogb7N+/nwkTJjB27FhGjRrFjh07XPft3LmT1NRUHn/8cbZv3w6A\n3W5nwYIFjB07lieeeIKioiIvVS7y9XSehogbrFu3jrlz53L//ffzySefMH/+fL7//e8D1yeK3Lhx\nIxcvXuSpp57i4YcfdoXKli1baGhoYNKkScTExHjzJYi0SqEh4gaLFy/mo48+4v333+fgwYPYbDbX\nfWPHjiUoKIi77rqL7373uxw8eJBPPvmEw4cPs2/fPgCuXr3K0aNH6devn7degkirFBoibpCenk5i\nYiKJiYkMHTqU559/3nVfYGCga9nhcNCtWzfsdjsvvPACP/jBDwCoqakhLCyMzz77zOO1i3wdHdMQ\n6WCXL1/m+PHjTJs2jeTkZEpLS7Hb7a77//jHP+J0OqmurqayspLY2FiSkpLYuHEjjY2N2Gw20tPT\nFRjik9TTEOlgPXr04J//+Z/54Q9/SFBQEElJSVy7do2rV68CEBoaSmpqKk1NTcyePZvIyEjGjx/P\niRMnGDt2LE1NTaSmppKYmEh5ebmXX41IS5p7SkREDNPuKRERMUyhISIihik0RETEMIWGiIgYptAQ\nERHDFBoiImKYQkNERAz7f0BRmHeGxiZCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10deefda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the document length\n",
    "neg_train_length = [len(i) for i in neg_train]\n",
    "pos_train_length = [len(i) for i in pos_train]\n",
    "\n",
    "# make it into a dataframe\n",
    "doc_length_df = pd.DataFrame(neg_train_length+pos_train_length, columns=['Review Length'])\n",
    "doc_length_df['label'] = ['Negative']*900 + ['Positive']*900\n",
    "doc_length_df.head()\n",
    "print(doc_length_df['Review Length'].describe())\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=\"label\", y='Review Length', data=doc_length_df)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='process'>NLP Process</a>\n",
    "\n",
    "1. **Read Text Files**\n",
    "2. **Transformation: Tokenization** \n",
    "    * splitting a string into its desired constituent parts; an indispensable component in almost any NLP application.\n",
    "    * This is a very important process in sentiment analysis. Sentiment information is often sparsely and unusually represented; E.g.: a single cluster of punctuation like :-) might tell the whole story.\n",
    "    * careful tokenization pays-off, especially where there is relatively little training data available; for a lot of data: less important (enough data for the model to learn that, e.g., happy and happy, are basically both the same token).\n",
    "    * Some commonly used strategies includes:\n",
    "        * **Whitespace tokenization**: obtain each token by whitespace, where a token is the smallest unit when doing text analytics, can be a number, punctuation, or a word\n",
    "        * **Treebank-style tokenization**: this approach would usually result in more units than whitespace tokenization since almost all tokens that involve punctuation are split apart — URLs, Twitter mark-up, phone numbers, dates, email addresses ... Thus, emoticons are collapsed with their component parts, URLs are not constituents, and Twitter mark-up is lost.\n",
    "        * **Sentiment-aware tokenization**: how to implement this would be on case-by-case basis. Some rules for twitter for example, would be \n",
    "            * not to separate twitter handle even it includes punctuation mark\n",
    "            * letters at the edges (\\*\\*\\*\\*, s\\*\\*\\*t), which is good to treat them as tokens\n",
    "            * keep Lenghtening by character repetition (it was sooooo good!)\n",
    "3. **Transformation- Stemming/Lemmatization**: a method for identifying distinct word forms (for reduction of vocabulary size). Stemming/lemmatization could help with sentiment analysis, in theory. **Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma\n",
    "    * Example:\n",
    "        * Stemming: happy, happier, happiest -> happ\n",
    "        * Lemmatizing: happy, happier, happiest -> happy\n",
    "    * Note:\n",
    "        * In practice, these off-the-shelf stemming algorithms can weaken sentiment systems.\n",
    "        * Which stemmer or lemmatization to use is often times trial and error.\n",
    "    * Available Stemmers:\n",
    "        * [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/)\n",
    "            * Problem: collapses sentiment distinctions, by mapping two words with different sentiment into the same stemmed form;\n",
    "            * E.g.: captivation (pos); captive (neg); both become -> captiv\n",
    "        * [Lancaster Stemmer](http://www.lancaster.ac.uk/scc/)\n",
    "            * it is arguably even more problematic than the Porter stemmer, since it collapses even more words of differing sentiment.\n",
    "            * E.g.: meaningful (pos); mean(neg); both become -> mean\n",
    "        * [WordNet Lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "            * high-precision stemming functionality; it requires (word, part-of-speech tag) pairs; \n",
    "            * Problem: it collapses tense, aspect, and number marking. E.g.: (exclaims, v)  exclaim \n",
    "4. **Transformation: others** \n",
    "    * Converting to lower case\n",
    "    * Removing punctuation\n",
    "    * Removing numbers\n",
    "    * Removing stopwords\n",
    "    * Removing tokens based on tf-idf (for example, remove words that occurred in less than 5 documents and those that occurred in more than 80% of documents.)\n",
    "    * Tagging and keeping only some type of part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.)\n",
    "5. **Vectorization**: \n",
    "    * N-grams: Unigrams vs Bigrams\n",
    "    * Frequency Count vs absence/presence\n",
    "6. **Fit Näive Bayes model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='explain'>Some Further Explaination</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='ngram'>N-gram Model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is N-gram? N-gram Model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Please turn your homework ..}$\n",
    "\n",
    "How do we, if we ever need to guess, know what is the word that will most likely to show up next? If we have a large enough document or corpus (a collection of documents), we can probably know it by calculating conditional probabilities for the following words such as\n",
    "\n",
    "$$P(in|Please turn your homework)$$\n",
    "\n",
    "$$P(out|Please turn your homework)$$\n",
    "\n",
    "$$P(on|Please turn your homework)$$\n",
    "\n",
    "$$\\dots$$\n",
    "\n",
    "Then the word having the largest probability would be the word that is most likely to show up next. This is the essential idea of N-gram Model. In our case, the N-gram (an n-gram is a contiguous sequence of n items from a given sample of text or speech) we use for this language modeling is four-gram. Of course, we can also use unigram, bigram, trigram, ...etc\n",
    "\n",
    "Models that assign probabilities to sequences of words are called **language model**. When we use N-gram for language modeling, independence assumptions are made so that each word depends only on the last n − 1 words. This assumption is important because it massively simplifies the problem of estimating the language model from data. In addition, because of the open nature of language, it is common to group words unknown to the language model together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture below is copied from [Speech and Language Processing Chap 3: N-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf), which demos how to estimate bigram probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of each word is the number of document (in this case, a sentence is a document) is obtained by counting number of sentences having that word (not the total number of the word, even though in our case both turn out to be the same). For example, there are 3 sentence having the word `I`, which is the same as the total occurence of `I`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/bigram1.png\" style=\"width: 500px;height: 180px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, what we see is some summary statistics from a sample of 9332 sentences of the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California. \n",
    "\n",
    "Figure 3.1 shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project. Note that the majority of the values are zero. In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.\n",
    "Figure 3.2 shows the bigram probabilities after normalization using unigram count for these selected words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/bigram2-1.png\" style=\"width: 400px;height: 50px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/bigram2-2.png\" style=\"width: 400px;height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Language Identification**: given a text, output the langugage used. Of course, we would need to have training probability for different language first.\n",
    "* **Spelling Correction**: when the probability of a certain N-gram is too low comparing to the training probability from a very large corpus, then it is likely that that particular term is a type. For example, The probabilty of $P(their|are)$, i.e., \"their are\", would be very low.\n",
    "* **Word Breaking**: breaking a sentence into smaller pieces based on the N-gram probability. Identifying collocation using [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) is something related.\n",
    "* **Text Summarization**: Here is a paper that uses Web N-gram models for text summarization: [Micropinion Generation: An Unsupervised Approach to Generating Ultra-Concise Summaries of Opinions](http://kavita-ganesan.com/micropinion-generation/#.XCRTjs9Kh0s)\n",
    "* **Machine Translation**: \n",
    "    * For example, as part of the machine translation process we might have built the following set of potential rough whatever-language to English translations:\n",
    "        * he introduced reporters to the main contents of the statement\n",
    "        * he briefed to reporters the main contents of the statement\n",
    "        * **he briefed reporters on the main contents of the statement**\n",
    "    * A probabilistic model of word sequences could suggest that *briefed reporters* on is a more probable English phrase than *briefed to reporters* (which has an awkward to after briefed) or *introduced reporters to* (which uses a verb that is less fluent English in this context), allowing us to correctly select the boldfaced sentence above.\n",
    "* **Augmentative Communication**: Probabilities are also important for augmentative communication (Newell et al.,\n",
    "1998) systems. People like the late physicist Stephen Hawking who are unable to physically talk or sign can instead use simple movements to select words from a menu to be spoken by the system. Word prediction can be used to suggest likely words for the menu. (the paragraph is copied from [Speech and Language Processing Chap 3: N-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **N-gram Model vs Näive Bayes**\n",
    "\n",
    "To my understanding, usually a Näive Bayes model refer to a unigram language model. Essentially they are the same. Independence assumptions are made so that the probability of the sentence of belonging to a certain class depends only on the probability product of the words in the sentence. \n",
    "\n",
    "On a high level, Näive Bayes can be seen as a special case of N-gram Modeling. Näive Bayes can be used only for text classification whereas N-gram modeling can be used for many different applications includeing those introduce above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Implementation**\n",
    "\n",
    "Here we would like to provide an example of how to implemment N-gram from scratch for **Language Identification**. Here is the description:\n",
    "\n",
    "```\n",
    "Language Identification, which is the problem of taking as input a text in an unknown language and determine what language it is written in. N-gram models are very effective solutions for this problem as well.\n",
    "\n",
    "For training, use the English, French, and Italian texts made available (see the Assignment2 folder). For test, use the file LangId.test provided in the language_identification folder as well. For each of the following questions, the output of your program has to contain a list of \n",
    "[line_id] [language] pairs, starting with line 1. For instance,\n",
    "\n",
    "   1 English\n",
    "   2 Italian\n",
    "   ...\n",
    "```\n",
    "\n",
    "The following codes are the implementation of N-gram using **Add-one Smoothing** and **Good-Turing Smoothing**, both are ways to allocation probabilities to unknown words."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Overall Process\n",
    "1. Read Files (English, Italian, French) and lower case it\n",
    "2. Train Transformation: separate into words by space\n",
    "3. Get the unigram count for each language as dict. ({'i':1000, 'a':2000, ...})\n",
    "    * counting number of sentences having that word\n",
    "4. Get the bigram count for each language as dict, separating by space. ({'i want': 100, 'i go': 200, ...})\n",
    "    * counting number of sentences having that bigram\n",
    "5. Take test file as input; separate into sentence\n",
    "6. Identify language used per sentence for the input text \n",
    "    * Get bigrams from each sentence for test data. \n",
    "    * For each sentence, calculate the probability using exp(log_p1+log_p2+...) for each language. \n",
    "    * Also generate the final output as\n",
    "        1 English \n",
    "        2 Italian\n",
    "        ...\n",
    "    \n",
    "7. Compare the result with LangId.sol to get accuracy\n",
    "\n",
    "Some Notes\n",
    "* unigram/bigram count is the number of time a unigram/bigram shows up in the sentence not the total count\n",
    "* V is how many type of words we have plus 1, representing unknown words (Shouldn't take the cases in the test into account when calculating the V). It also means that all the words doesn't show up in the train are treated as the same \"unknown\" word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     292\n",
       "False      8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Johnny Chiu, Master of Science in Analytics\n",
    "Goal: \n",
    "Implement a word bigram model using add-one smoothing, which learns word bigram probabilities from the training data.\n",
    "Then apply the models to determine the most likely language for each sentence in the test file.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Q2():\n",
    "    def read_files(self):\n",
    "        \"\"\"read English, French, and Italian training text; also lower case them\"\"\"\n",
    "        \n",
    "        f = open('language_identification/LangId.train.English', 'rb')\n",
    "        train_english = f.read().decode('utf8', 'backslashreplace')\n",
    "        f.close()\n",
    "\n",
    "        f = open('language_identification/LangId.train.French', 'rb')\n",
    "        train_french = f.read().decode('utf8', 'backslashreplace')\n",
    "        f.close()\n",
    "\n",
    "        f = open('language_identification/LangId.train.Italian', 'rb')\n",
    "        train_italian = f.read().decode('utf8', 'backslashreplace')\n",
    "        f.close()\n",
    "\n",
    "        return {'english': train_english.lower(), 'french': train_french.lower(),'italian': train_italian.lower()}      \n",
    "    \n",
    "    def get_sentence_probability(self, term_dict, unigram_language, bigram_language):\n",
    "        \"\"\"calculate the probability according to the input sentence and language using 2-gram\n",
    "\n",
    "        Args:\n",
    "            term_dict(dict): contains bigram count of a sentence\n",
    "            unigram_language(dict): contains unigram count of the training data in a specific language\n",
    "            bigram_language(dict): contains bigram count of the training data in a specific language\n",
    "\n",
    "        Return:\n",
    "            the probability of this input dict is a certain language\n",
    "        \"\"\"\n",
    "\n",
    "        # set initial value and get total vocabulary size for smoothing\n",
    "        log_sum = 0\n",
    "        V = len(set(unigram_language.keys()))+1\n",
    "\n",
    "        # get what the overall probability is for each sentence using add-one smoothing to estimate the bigram probability\n",
    "        for i in list(term_dict.keys()):\n",
    "            if i in bigram_language.keys():\n",
    "                denominator = unigram_language[i.split(\" \")[0]]+V # plus V for add-one smoothing\n",
    "                nominator = bigram_language[i]+1 # plus 1 for add-one smoothing\n",
    "                prob = float(nominator)/denominator\n",
    "                log_sum += np.log(prob) * term_dict[i]\n",
    "            else:\n",
    "                unigram_count = unigram_language[i.split(\" \")[0]] if i.split(\" \")[0] in unigram_language.keys() else 0\n",
    "                prob = 1/(unigram_count+V)\n",
    "                log_sum += np.log(prob) * term_dict[i]\n",
    "\n",
    "        return(np.exp(log_sum))\n",
    "    \n",
    "    def identify_language(self, term_dict):\n",
    "        \"\"\"identify what the language is for the input text\"\"\"\n",
    "    \n",
    "        prob_list = [self.get_sentence_probability(term_dict, unigram_english, bigram_english), \n",
    "                     self.get_sentence_probability(term_dict, unigram_french, bigram_french), \n",
    "                     self.get_sentence_probability(term_dict, unigram_italian, bigram_italian)]\n",
    "        top_index = prob_list.index(max(prob_list))\n",
    "\n",
    "        if top_index == 0:\n",
    "            language = 'English'\n",
    "        elif top_index == 1:\n",
    "            language = 'French'\n",
    "        elif top_index == 2:\n",
    "            language = 'Italian'\n",
    "        else:\n",
    "            language = 'Others'\n",
    "\n",
    "        return(language)\n",
    "    \n",
    "    def get_language_all_sentence(self, test):\n",
    "        \"\"\"identify language used per sentence for the input text\n",
    "\n",
    "        Args:\n",
    "            test(list of lists): each item in the list is a string in a sentence\n",
    "\n",
    "        Return:\n",
    "            a dataframe with [id, language]\n",
    "        \"\"\"\n",
    "\n",
    "        # identify language used per sentence\n",
    "        language_list = []\n",
    "        n=2\n",
    "        for s in test:\n",
    "            bigram_in_test_sentence = [' '.join(s[i : i + n]) for i in range(len(s)-n+1)]\n",
    "            bigram_dict = {c: bigram_in_test_sentence.count(c) for c in set(bigram_in_test_sentence)}                \n",
    "            language = self.identify_language(bigram_dict)\n",
    "            language_list.append(language)\n",
    "\n",
    "        final_df = pd.DataFrame({'id': list(range(1,len(test)+1)),\n",
    "                                'identified_language': language_list})\n",
    "\n",
    "        return(final_df)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    q2 = Q2()\n",
    "    \n",
    "    ### 1. Read Files (English, Italian, French)\n",
    "    train = q2.read_files()\n",
    "  \n",
    "    ### 2. Train Transformation: separate into words by space\n",
    "    # list of lists, each item in the list is a single word\n",
    "    train_english = [s.split(\" \") for s in train['english'].splitlines()]\n",
    "    train_french = [s.split(\" \") for s in train['french'].splitlines()]\n",
    "    train_italian = [s.split(\" \") for s in train['italian'].splitlines()]\n",
    "    \n",
    "    # list containing every single word\n",
    "    word_english = [item for sublist in train_english for item in sublist]\n",
    "    word_french = [item for sublist in train_french for item in sublist]\n",
    "    word_italian = [item for sublist in train_italian for item in sublist]\n",
    "    \n",
    "    ### 3. Get the unigram count for each language as dict (counting number of sentences having that word)\n",
    "    unigram_english = {w: sum(w in sentence for sentence in train_english) for w in set(word_english)}\n",
    "    unigram_french = {w: sum(w in sentence for sentence in train_french) for w in set(word_french)}\n",
    "    unigram_italian = {w: sum(w in sentence for sentence in train_italian) for w in set(word_italian)}\n",
    "        \n",
    "    ### 4. Get the bigram count for each language as dict, separating by space.\n",
    "    # list of lists, with each item represent bigram within each sentence\n",
    "    n=2\n",
    "    train_bigram_english = [[' '.join(sentence[i : i + n]) for i in range(len(sentence)-n+1)] for sentence in train_english]\n",
    "    train_bigram_french = [[' '.join(sentence[i : i + n]) for i in range(len(sentence)-n+1)] for sentence in train_french]\n",
    "    train_bigram_italian = [[' '.join(sentence[i : i + n]) for i in range(len(sentence)-n+1)] for sentence in train_italian]\n",
    "    \n",
    "    # list containing every bigram\n",
    "    word_2_english = [item for sublist in train_bigram_english for item in sublist]\n",
    "    word_2_french = [item for sublist in train_bigram_french for item in sublist]\n",
    "    word_2_italian = [item for sublist in train_bigram_italian for item in sublist]\n",
    "    \n",
    "    # dictionary containing the bigram count in each sentence\n",
    "    bigram_english = {w: sum(w in sentence for sentence in train_bigram_english) for w in set(word_2_english)}\n",
    "    bigram_french = {w: sum(w in sentence for sentence in train_bigram_french) for w in set(word_2_french)}\n",
    "    bigram_italian = {w: sum(w in sentence for sentence in train_bigram_italian) for w in set(word_2_italian)}\n",
    "              \n",
    "    ### 5. Take test file as input, separate into sentence\n",
    "    f = open('language_identification/LangId.test', 'rb')\n",
    "    test_original = f.read().decode('utf8', 'backslashreplace').lower()\n",
    "    test = [s.split(\" \") for s in test_original.splitlines()]\n",
    "    f.close()\n",
    "\n",
    "    ### 6. Identify language used per sentence for the input text\n",
    "    final_df1 = q2.get_language_all_sentence(test)\n",
    "    \n",
    "    ### 7. Compare the result with LangId.sol to get accuracy\n",
    "    validation = pd.read_csv('language_identification/LangId.sol', sep=\" \", header=None)\n",
    "    validation.columns = [\"id\", \"actual_language\"]\n",
    "    accuracy_df1 = pd.merge(final_df1, validation, on='id')\n",
    "    pd.Series(accuracy_df1.identified_language == accuracy_df1.actual_language).value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q3 Process\n",
    "1. Read Files (English, Italian, French) and lower case it\n",
    "2. Train Transformation: separate original text into list(sentetnces) of lists(words in each sentence). Each item in the later list is a word\n",
    "3. Get the unigram count for each language as dict. ({'i':1000, 'a':2000, ...})\n",
    "    * The value is the counting of the sentence, where this unigram shows up (a word show up twice in 1 sentence should be counted only once)\n",
    "4. Get the bigram count for each language as dict, separating by space. ({'i want': 100, 'i go': 200, ...})\n",
    "    * The value is the counting of the sentence, where this bigram shows up (a bigram show up twice in 1 sentence should be counted only once)\n",
    " \n",
    "5. Take test file as input; separate into sentence\n",
    "6. Identify language used per sentence for the input text \n",
    "    * Get bigrams from each sentence for test data. \n",
    "    * For each sentence, calculate the probability using exp(log_p1+log_p2+...) for each language. \n",
    "        * p1 (some bigram, \"i want\") is GT_bigram_prob / unigram_MLE_prob\n",
    "            * GT_bigram_prob = (c+1)N_r1/(N*N_r)\n",
    "                * c=bigram count of \"i want\"\n",
    "                * N_r = number of bigrams with the same frequency c\n",
    "                * N_r+1 = number of bigrams with the same frequency c+1\n",
    "                * N = total count of all the bigrams in the training (the sum of bigram count of all the bigram in training. \"i want\":2, 'want to': 5. The N would be 7)\n",
    "                * Note: If N_r+1 = 0, then use the original bigram_count / N as probability\n",
    "            * unigram_MLE_prob = unigram_count / {total count of all the unigram in the training}\n",
    "    * Also generate the final output as\n",
    "        1 English \n",
    "        2 Italian\n",
    "        ...\n",
    "    \n",
    "7. Compare the result with LangId.sol to get accuracy\n",
    "\n",
    "Some Notes\n",
    "* For freq of freq table for bigram, the count for frequency 0 should be all the unseen cases, which is {unique unigram count}*2 - {observed unique bigram}\n",
    "* For the unigram probability, since the freq 0 count for the freq of freq table is unknown (don't know how to estimate this, since the number of unseen unigram should be infinite. Can't use the number of unseen word in test either, since we shouldn't use any info from test when \"training\", i.e., calculating all the unigram/bigram probability.) For unigram probability, we can just use the MLE estimate, which is the number of times a unigram token show up in the sentence (counting number of times it shows up / all the sentence we have), this captures the likelihood of each word showing up in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Johnny Chiu, Master of Science in Analytics\n",
    "Goal: \n",
    "Implement a word bigram model using good-turing smoothing, which learns word bigram probabilities from the training data.\n",
    "Then apply the models to determine the most likely language for each sentence in the test file.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Q3():\n",
    "    def read_files(self):\n",
    "        \"\"\"read English, French, and Italian training text; also lower case them\"\"\"\n",
    "        \n",
    "        f = open('../data/LangId.train.English', 'rb')\n",
    "        train_english = f.read().decode('utf8', 'backslashreplace')\n",
    "        f.close()\n",
    "\n",
    "        f = open('../data/LangId.train.French', 'rb')\n",
    "        train_french = f.read().decode('utf8', 'backslashreplace')\n",
    "        f.close()\n",
    "\n",
    "        f = open('../data/LangId.train.Italian', 'rb')\n",
    "        train_italian = f.read().decode('utf8', 'backslashreplace')\n",
    "        f.close()\n",
    "\n",
    "        return {'english': train_english.lower(), 'french': train_french.lower(),'italian': train_italian.lower()}      \n",
    "    \n",
    "    def good_turing(self, term, ngram_language, language_freq, N):\n",
    "        \"\"\"get the update prob using good turing smoothing\"\"\"\n",
    "\n",
    "        # get the frequency count for the input term\n",
    "        c = ngram_language[term] if term in ngram_language.keys() else 0\n",
    "\n",
    "        # get freq of freq\n",
    "        N_r = language_freq[c] if c in language_freq.keys() else 0\n",
    "        N_r1 = language_freq[c+1] if c+1 in language_freq.keys() else 0\n",
    "\n",
    "        # get the GT count\n",
    "        N_gt = (c+1)*(N_r1/(N*N_r))\n",
    "\n",
    "        # get the probability using GT count. If N_gt is 0, then use the original MLE estimate\n",
    "        update_prob = c/N if N_gt == 0 else N_gt\n",
    "\n",
    "        return(update_prob)\n",
    "    \n",
    "    def get_sentence_probability(self, term_dict, unigram_language, bigram_language):\n",
    "        \"\"\"calculate the probability according to the input sentence and language using 2-gram\n",
    "\n",
    "        Args:\n",
    "            term_dict(dict): contains bigram count of a sentence\n",
    "            unigram_language(dict): contains unigram count of the training data in a specific language\n",
    "            bigram_language(dict): contains bigram count of the training data in a specific language\n",
    "\n",
    "        Return:\n",
    "            the probability of this input dict is a certain language\n",
    "        \"\"\"\n",
    "\n",
    "        # set initial value and get total vocabulary size for smoothing\n",
    "        log_sum = 0\n",
    "\n",
    "        # get bigram frequency table\n",
    "        bigram_language_freq = {num: list(bigram_language.values()).count(num) for num in set(bigram_language.values())}\n",
    "        bigram_zero_freq = (len(unigram_language))**2 - len(bigram_language)\n",
    "        bigram_language_freq[0] = bigram_zero_freq\n",
    "\n",
    "        # get N for unigram and bigram, where N is the total number of observed nGram, where each number represent a ngram show up or not in a sentence\n",
    "        N_uni = sum(unigram_language.values())\n",
    "        N_bi = sum(bigram_language.values())\n",
    "\n",
    "        # get what the overall probability is for each sentence using good-turing smoothing to estimate the bigram probability\n",
    "        # use MLE estimate to get the unigram probability. For unknown tokens, use 1/N for probability\n",
    "        for i in list(term_dict.keys()):                \n",
    "            nominator = self.good_turing(i, bigram_language, bigram_language_freq, N_bi)\n",
    "            denominator = unigram_language[i.split(\" \")[0]]/N_uni if i.split(\" \")[0] in unigram_language.keys() else 1/N_uni\n",
    "\n",
    "            prob = float(nominator)/denominator\n",
    "            log_sum += np.log(prob) * term_dict[i]\n",
    "\n",
    "        return(np.exp(log_sum))\n",
    "    \n",
    "    def identify_language(self, term_dict):\n",
    "        \"\"\"identify what the language is for the input text\"\"\"\n",
    "    \n",
    "        prob_list = [self.get_sentence_probability(term_dict, unigram_english, bigram_english), \n",
    "                     self.get_sentence_probability(term_dict, unigram_french, bigram_french), \n",
    "                     self.get_sentence_probability(term_dict, unigram_italian, bigram_italian)]\n",
    "        top_index = prob_list.index(max(prob_list))\n",
    "\n",
    "        if top_index == 0:\n",
    "            language = 'English'\n",
    "        elif top_index == 1:\n",
    "            language = 'French'\n",
    "        elif top_index == 2:\n",
    "            language = 'Italian'\n",
    "        else:\n",
    "            language = 'Others'\n",
    "\n",
    "        return(language)\n",
    "    \n",
    "    def get_language_all_sentence(self, test):\n",
    "        \"\"\"identify language used per sentence for the input text\n",
    "\n",
    "        Args:\n",
    "            test(list): list of lists, each item in the list is a single word\n",
    "\n",
    "        Return:\n",
    "            a dataframe with [id, language]\n",
    "        \"\"\"\n",
    "\n",
    "        # identify language used per sentence\n",
    "        language_list = []\n",
    "        for s in test:\n",
    "            bigram_in_test_sentence = [' '.join(s[i : i + n]) for i in range(len(s)-n+1)]\n",
    "            bigram_dict = {c: bigram_in_test_sentence.count(c) for c in set(bigram_in_test_sentence)}   \n",
    "            language = self.identify_language(bigram_dict)\n",
    "            language_list.append(language)\n",
    "\n",
    "        final_df = pd.DataFrame({'id': list(range(1,len(test)+1)),\n",
    "                                'identified_language': language_list})\n",
    "\n",
    "        return(final_df)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    q3 = Q3()\n",
    "    \n",
    "    ### 1. Read Files (English, Italian, French)\n",
    "    train = q3.read_files()\n",
    "  \n",
    "    ### 2. Train Transformation: separate into words by space\n",
    "    # list of lists, each item in the list is a single word\n",
    "    train_english = [s.split(\" \") for s in train['english'].splitlines()]\n",
    "    train_french = [s.split(\" \") for s in train['french'].splitlines()]\n",
    "    train_italian = [s.split(\" \") for s in train['italian'].splitlines()]    \n",
    "    \n",
    "    # list containing every single word\n",
    "    word_english = [item for sublist in train_english for item in sublist]\n",
    "    word_french = [item for sublist in train_french for item in sublist]\n",
    "    word_italian = [item for sublist in train_italian for item in sublist]\n",
    "        \n",
    "    ### 3. Get the unigram count for each language as dict (counting number of sentences having that word)\n",
    "    unigram_english = {w: sum(w in sentence for sentence in train_english) for w in set(word_english)}\n",
    "    unigram_french = {w: sum(w in sentence for sentence in train_french) for w in set(word_french)}\n",
    "    unigram_italian = {w: sum(w in sentence for sentence in train_italian) for w in set(word_italian)}\n",
    "        \n",
    "    ### 4. Get the bigram count for each language as dict, separating by space.\n",
    "    # list of lists, with each item represent bigram within each sentence\n",
    "    n=2    \n",
    "    train_bigram_english = [[' '.join(sentence[i : i + n]) for i in range(len(sentence)-n+1)] for sentence in train_english]\n",
    "    train_bigram_french = [[' '.join(sentence[i : i + n]) for i in range(len(sentence)-n+1)] for sentence in train_french]\n",
    "    train_bigram_italian = [[' '.join(sentence[i : i + n]) for i in range(len(sentence)-n+1)] for sentence in train_italian]    \n",
    "\n",
    "    # list containing every bigram\n",
    "    word_2_english = [item for sublist in train_bigram_english for item in sublist]\n",
    "    word_2_french = [item for sublist in train_bigram_french for item in sublist]\n",
    "    word_2_italian = [item for sublist in train_bigram_italian for item in sublist]    \n",
    "\n",
    "    # dictionary containing the bigram count in each sentence\n",
    "    bigram_english = {w: sum(w in sentence for sentence in train_bigram_english) for w in set(word_2_english)}\n",
    "    bigram_french = {w: sum(w in sentence for sentence in train_bigram_french) for w in set(word_2_french)}\n",
    "    bigram_italian = {w: sum(w in sentence for sentence in train_bigram_italian) for w in set(word_2_italian)}\n",
    "              \n",
    "    ### 5. Take test file as input, separate into sentence\n",
    "    f = open('language_identification/LangId.test', 'rb')\n",
    "    test_original = f.read().decode('utf8', 'backslashreplace').lower()\n",
    "    test = [s.split(\" \") for s in test_original.splitlines()]\n",
    "    f.close()\n",
    "\n",
    "    ### 6. Identify language used per sentence for the input text\n",
    "    final_df2 = q3.get_language_all_sentence(test)\n",
    "    \n",
    "    ### 7. Compare the result with LangId.sol to get accuracy\n",
    "    validation = pd.read_csv('language_identification/LangId.sol', sep=\" \", header=None)\n",
    "    validation.columns = [\"id\", \"actual_language\"]\n",
    "    accuracy_df2 = pd.merge(final_df2, validation, on='id')\n",
    "    pd.Series(accuracy_df2.identified_language == accuracy_df2.actual_language).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='tfidf'>TF-IDF</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Most of the following contents are  copied from [here](http://www.tfidf.com/))\n",
    "\n",
    "> **What is TF-IDF?**\n",
    "\n",
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus (which is the union of the document that we have for the analysis). The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "\n",
    "In short, TF-IDF is a function of\n",
    "* **Input**: a collection of documents (i.e., a corpus)\n",
    "* **Output**: give a weight for each of the word (or token) for each of the document\n",
    "\n",
    "> **How to Compute**\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "* TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "$$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}$$\n",
    "\n",
    "* IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "$$IDF(t) = ln(\\frac{\\text{Total number of documents}}{\\text{Number of documents with term t in it}})$$\n",
    "\n",
    "In short, the TF-IDF weight for a single word would be \n",
    "\n",
    "$$ TFIDF(t) = TF(t) * IDF(t)$$\n",
    "\n",
    "See below for a simple example.\n",
    "\n",
    "> **Example**\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "> **Variation**\n",
    "\n",
    "Other than the one we introducted above, there are many different version of TF and IDF function, as shown in the following table from [wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf):\n",
    "\n",
    "<img src=\"pic/tf.png\" style=\"width: 300px;height: 250px;\"/>\n",
    "<img src=\"pic/idf.png\" style=\"width: 400px;height: 240px;\"/>\n",
    "\n",
    "One common used variation of TF-IDF is call **sublinear TF-IDF**. Instead of using the TF function introduced above, sublinear TF-IDF use the following TF function (call it WF)\n",
    "\n",
    "$$WF(t) = 1 + log(count(t))$$\n",
    "\n",
    "The idea behind this function is that it seems unlikely that twenty occurrences of a term in a document truly carry twenty times the significance of a single occurrence, which is exactly how the original function capture. Therefore, instead of assigning the term with high frequency with weight that is twenty times larger than the single ones, it use a sublinear function (log function) to down-weight those terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **implement TF-IDF using sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfVectorizer` from sklearn use the following function to calculate TF-IDF\n",
    "\n",
    "$$\n",
    "TF(t) =\n",
    "  \\begin{cases}\n",
    "    count(t) = \\text{Number of times term t appears in a document}  & \\quad \\text{if sublinear_tf} \\text{ is False (default)}\\\\\n",
    "    1 + log(count(t))  & \\quad \\text{if sublinear_tf} \\text{ is True}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "IDF(t) =\n",
    "  \\begin{cases}\n",
    "    ln(\\frac{N+1}{df+1})+1       & \\quad \\text{if smooth_idf} \\text{ is True (default)}\\\\\n",
    "    ln(\\frac{N}{df})+1  & \\quad \\text{if smooth_idf} \\text{ is False}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$N=\\text{Total number of documents}$$\n",
    "$$df=\\text{Number of documents with term t in it}$$\n",
    "\n",
    "Also, the default for `norm` is `l2`, i.e., idf vectors are normalized by the Euclidean norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy documents\n",
    "doc1 = \"This is doc one\"\n",
    "doc2 = \"This is doc two\"\n",
    "doc3 = \"This is doc three\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "this  :  0.412858572062\n",
      "is  :  0.412858572062\n",
      "doc  :  0.412858572062\n",
      "one  :  0.699030327257\n",
      "this  :  0.412858572062\n",
      "is  :  0.412858572062\n",
      "doc  :  0.412858572062\n",
      "two  :  0.699030327257\n",
      "this  :  0.412858572062\n",
      "is  :  0.412858572062\n",
      "doc  :  0.412858572062\n",
      "three  :  0.699030327257\n"
     ]
    }
   ],
   "source": [
    "# create tf-idf object\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "print(tf_idf_vectorizer)\n",
    "\n",
    "# fit tf-idf\n",
    "results = tf_idf_vectorizer.fit_transform([doc1, doc2, doc3])\n",
    "\n",
    "# see the calculated result\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "for (doc, col) in zip(results.nonzero()[0], results.nonzero()[1]):\n",
    "    print (feature_names[col], ' : ', results[doc, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **implement TF-IDF from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeTF(word_list):\n",
    "    \"\"\"use word count as term frequency\"\"\"\n",
    "    word_count_dict = dict.fromkeys(VOCABULARY, 0)\n",
    "    for word in word_list:\n",
    "        word_count_dict[word] += 1\n",
    "    return word_count_dict\n",
    "\n",
    "def computeIDFCount(word_count_dict_list):\n",
    "    \"\"\"get the document frequency of each word\"\"\"\n",
    "    id_count_dict = dict.fromkeys(VOCABULARY, 0)\n",
    "    for word_count_dict in word_count_dict_list:\n",
    "        for word, count in word_count_dict.items():\n",
    "            if count > 0:\n",
    "                id_count_dict[word] += 1\n",
    "    return id_count_dict\n",
    "\n",
    "def computeIDF(word_count_dict_list, smooth_idf=True):\n",
    "    \"\"\"get the inverse document frequency for the input words\"\"\"\n",
    "    id_count_dict = computeIDFCount(word_count_dict_list)\n",
    "    idf_dict = {}\n",
    "    for word, id_count in id_count_dict.items():        \n",
    "        idf_dict[word] = math.log((DOC_COUNT +1)/(float(id_count)+1))+1 if smooth_idf==True else math.log((DOC_COUNT)/(float(id_count)))+1\n",
    "    return idf_dict\n",
    "\n",
    "def computeTFIDF(tf_dict, idf_dict):\n",
    "    \"\"\"compute tf-idf weight based on the input tf and idf dictionary\"\"\"\n",
    "    tf_idf_dict = {}\n",
    "    for word, tf in tf_dict.items():\n",
    "        tf_idf_dict[word] = tf * idf_dict[word]\n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This</th>\n",
       "      <th>doc</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>three</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   This  doc   is       one     three       two\n",
       "0   1.0  1.0  1.0  1.693147  0.000000  0.000000\n",
       "1   1.0  1.0  1.0  0.000000  0.000000  1.693147\n",
       "2   1.0  1.0  1.0  0.000000  1.693147  0.000000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC_COUNT = 3\n",
    "\n",
    "# tokenize it using space\n",
    "words1 = word_tokenize(doc1)\n",
    "words2 = word_tokenize(doc2)\n",
    "words3 = word_tokenize(doc3)\n",
    "\n",
    "# compute term frequency\n",
    "tf_dict_1 = computeTF(words1)\n",
    "tf_dict_2 = computeTF(words2)\n",
    "tf_dict_3 = computeTF(words3)\n",
    "\n",
    "pd.DataFrame([tf_dict_1, tf_dict_2, tf_dict_3])\n",
    "\n",
    "# get the document frequency of each word \n",
    "id_count_dict = computeIDFCount([word_count_dict_1, word_count_dict_2, word_count_dict_3])\n",
    "\n",
    "# get the idf\n",
    "idf_dict = computeIDF([word_count_dict_1, word_count_dict_2, word_count_dict_3])\n",
    "\n",
    "tf_idf_dict_1 = computeTFIDF(tf_dict_1, idf_dict)\n",
    "tf_idf_dict_2 = computeTFIDF(tf_dict_2, idf_dict)\n",
    "tf_idf_dict_3 = computeTFIDF(tf_dict_3, idf_dict)\n",
    "\n",
    "# after normalization by row the value would be the same as the TF-IDF implemented by sklearn\n",
    "pd.DataFrame([tf_idf_dict_1, tf_idf_dict_2, tf_idf_dict_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='implement'>Sentiment Analysis Implementation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Read Text Files**\n",
    "2. **Transformation: Tokenization**: this is done using the some Sklearn functions with default setting  pa, for example:\n",
    "    * CountVectorizer, TfidfVectorizer: the `token_pattern` uses \"\"Regular expression denoting what constitutes a “token”. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\"\"\n",
    "\n",
    "3. **Transformation: Stemming/Lemmatization**: here we doesn't do any stemming or try PorterStemmer\n",
    "4. **Transformation: others**: \n",
    "    * **Converting to lower case**: the document is provided in lower case.\n",
    "    * **Removing punctuation**: we remove all the punctuation\n",
    "    * **Removing numbers**: we didn't remove any numbers, which we can probably can since number probably wouldn't inform positive or negative sentiment.\n",
    "    * **Removing stopwords**: we remove stopwords for all the models and see what are the most informative words based on total weight. The value of the weight is different according to the model. For example, For model Unigrams with frequency count, the weight for \"Movie\" would be the total count of Movie divided by total number of document in each class (P(Movie|Positive), where the nominator would just be the count of the word \"Movie\"). If we don't remove stopwords, for most of the models (except TF-IDF) the word with the highest weight would just be those stopwords. (Hence we remove them).\n",
    "    * **Removing tokens based on tf-idf (for example, remove words that occurred in less than 5 documents and those that occurred in more than 80% of documents.)**: this is implemented below.\n",
    "    * **Tagging and keeping only some type of part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.)**: this is implemented below as well.\n",
    "5. **Vectorization**: \n",
    "    * **N-grams**: here we try both **Unigrams** and **Bigrams** as the input of Näive Bayes.\n",
    "    * We try both **Frequency Count** and **absence/presence**\n",
    "6. **Fit Näive Bayes model**\n",
    "\n",
    "There would be too many variation if we try all of the combination. The variation that we could try are\n",
    "\n",
    "1. Different types of stemming or lemmatizing\n",
    "2. Removing punctuation\n",
    "3. Removing number\n",
    "4. Removing stopwords\n",
    "5. Removing based on TF-IDF\n",
    "6. POS filtering\n",
    "7. Unigram or Bigram for Näive Bayes\n",
    "8. requency count or binary for Näive Bayes\n",
    "\n",
    "There are 8 different types of things that we can try, which will be at least corresponding to $2^{8}=256$ different types of variation. Here we say \"at least\" because we know that there are many different types of stemming and lemmatizing approach. In our case, we only implement 10 out of those variations, which are:\n",
    "\n",
    "* M1: Unigrams (absence/presence)\n",
    "* M2: Unigrams with frequency count\n",
    "* M3: Unigrams (only adjectives/adverbs)\n",
    "* M4: Unigrams (sublinear tf-idf)\n",
    "* M5: Bigrams (absence/presence)\n",
    "\n",
    "* M6: Unigrams (absence/presence) + PorterStemmer\n",
    "* M7: Unigrams with frequency count + PorterStemmer\n",
    "* M8: Unigrams (only adjectives/adverbs) + PorterStemmer\n",
    "* M9: Unigrams (sublinear tf-idf) + PorterStemmer\n",
    "* M10: Bigrams (absence/presence) + PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def show_most_informative_features(vectorizer, classifier, n=10):\n",
    "    \"\"\"\n",
    "    Show the total weight of each of the features. \n",
    "    The word with the highest total weight would have the largest effect when calculating the probability, \n",
    "    and hence the word is informative.\n",
    "    \"\"\"\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()  \n",
    "    topn_pos_class = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_neg_class = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]    \n",
    "\n",
    "    print(\"Important words in positive reviews\")\n",
    "    for coef, feature in topn_pos_class:\n",
    "        print(class_labels[1], coef, feature) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in negative reviews\")\n",
    "    for coef, feature in topn_neg_class:\n",
    "        print(class_labels[0], coef, feature)\n",
    "\n",
    "def model_fit_eval(vectorizer, train, test, transformation = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vectorizer(CountVectorizer object): the way the that text document is vectorized. \n",
    "        train(list): training documents\n",
    "        test(list): test documents  \n",
    "        transformation(function): the function applied to the document before they are being vectorized\n",
    "        \n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    # training features\n",
    "    train_features = vectorizer.fit_transform([transformation(doc) if transformation is not None else doc for doc in train])\n",
    "\n",
    "    # setup a naive bayes classifier \n",
    "    nb_clf = MultinomialNB()\n",
    "    nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "    # test set features\n",
    "    test_features = vectorizer.transform([transformation(doc) if transformation is not None else doc for doc in test])\n",
    "\n",
    "    # predict\n",
    "    predictions = nb_clf.predict(test_features)\n",
    "\n",
    "    # determine the accuracy of the model\n",
    "    accuracy = accuracy_score(predictions, test_labels)\n",
    "    print(\"Accuracy = \", accuracy*100, \"%\")\n",
    "\n",
    "    # display the most informative features\n",
    "    show_most_informative_features(vectorizer, nb_clf, 5)\n",
    "\n",
    "def retain_adverbs_adjectives(corpus):\n",
    "    \"\"\"\n",
    "    retain only adjective and adverbs using POS tagging\n",
    "    reference: https://cs.nyu.edu/grishman/jet/guide/PennPOS.html\n",
    "    \"\"\"\n",
    "    adj_adv_pos_tags = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    tokenized = word_tokenize(corpus)\n",
    "    tags = pos_tag(tokenized)\n",
    "    result = [word[0] for word in tags if word[1] in adj_adv_pos_tags]\n",
    "    result = ' '.join(result)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M1: Unigrams (absence/presence)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  86.5 %\n",
      "Important words in positive reviews\n",
      "Positive 794.0 film\n",
      "Positive 660.0 movie\n",
      "Positive 653.0 like\n",
      "Positive 578.0 just\n",
      "Positive 576.0 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 762.0 film\n",
      "Negative 725.0 movie\n",
      "Negative 693.0 like\n",
      "Negative 612.0 just\n",
      "Negative 554.0 time\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, stop_words='english')#, max_df=0.8\n",
    "model_fit_eval(vectorizer, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M2: Unigrams with frequency count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  81.5 %\n",
      "Important words in positive reviews\n",
      "Positive 4412.0 film\n",
      "Positive 2134.0 movie\n",
      "Positive 1637.0 like\n",
      "Positive 1200.0 just\n",
      "Positive 1123.0 good\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 3621.0 film\n",
      "Negative 2783.0 movie\n",
      "Negative 1689.0 like\n",
      "Negative 1391.0 just\n",
      "Negative 1042.0 time\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "model_fit_eval(vectorizer, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M3: Unigrams (only adjectives/adverbs)**\n",
    "\n",
    "Vectorization usign the frequency of the occurrences of the words after removing everything but adverbs and adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  82.5 %\n",
      "Important words in positive reviews\n",
      "Positive 1200.0 just\n",
      "Positive 1110.0 good\n",
      "Positive 738.0 best\n",
      "Positive 694.0 really\n",
      "Positive 693.0 little\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 1391.0 just\n",
      "Negative 1024.0 good\n",
      "Negative 936.0 bad\n",
      "Negative 715.0 really\n",
      "Negative 660.0 little\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "model_fit_eval(vectorizer, train, test, retain_adverbs_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M4: Unigrams (sublinear tf-idf)**\n",
    "\n",
    "Using sub linear tf-idf as the weight of words in the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  85.0 %\n",
      "Important words in positive reviews\n",
      "Positive 21.9548560775 movie\n",
      "Positive 18.7050070359 like\n",
      "Positive 16.7859967278 story\n",
      "Positive 16.7808049541 life\n",
      "Positive 16.6982165202 just\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 26.6824696025 movie\n",
      "Negative 21.1182914648 like\n",
      "Negative 19.7021648118 just\n",
      "Negative 18.6116812853 bad\n",
      "Negative 16.9585533231 good\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "model_fit_eval(vectorizer, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M5: Bigrams (absence/presence)**\n",
    "\n",
    "Use binary indicators for whether a bigram is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  82.5 %\n",
      "Important words in positive reviews\n",
      "Positive 108.0 special effects\n",
      "Positive 76.0 ive seen\n",
      "Positive 75.0 year old\n",
      "Positive 73.0 new york\n",
      "Positive 63.0 takes place\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 112.0 special effects\n",
      "Negative 70.0 new york\n",
      "Negative 68.0 year old\n",
      "Negative 68.0 looks like\n",
      "Negative 60.0 look like\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2), binary=True, stop_words='english')\n",
    "model_fit_eval(vectorizer, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Redo M1-M5 but with stemming (use Porter’s stemmer)**\n",
    "\n",
    "Let's call them M6-M10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemmer(review):\n",
    "    \"\"\"stem the input review\"\"\"    \n",
    "    port_stemmer = PorterStemmer()\n",
    "    tokenized = word_tokenize(review)\n",
    "    stemmed = [port_stemmer.stem(word) for word in tokenized]\n",
    "    stemmed = ' '.join(stemmed)\n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the porter stemmer to the training and test sets\n",
    "train_stemmed = [stemmer(review) for review in train]\n",
    "test_stemmed = [stemmer(review) for review in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M6: Unigrams (absence/presence) + PorterStemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  85.5 %\n",
      "Important words in positive reviews\n",
      "Positive 684.0 like\n",
      "Positive 665.0 wa\n",
      "Positive 662.0 time\n",
      "Positive 661.0 make\n",
      "Positive 650.0 charact\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 719.0 like\n",
      "Negative 688.0 wa\n",
      "Negative 641.0 charact\n",
      "Negative 639.0 make\n",
      "Negative 622.0 time\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, max_df=0.8, stop_words='english')\n",
    "model_fit_eval(vectorizer, train_stemmed, test_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M7: Unigrams with frequency count + PorterStemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  82.0 %\n",
      "Important words in positive reviews\n",
      "Positive 5534.0 film\n",
      "Positive 5019.0 hi\n",
      "Positive 4156.0 thi\n",
      "Positive 2774.0 movi\n",
      "Positive 2330.0 ha\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 4512.0 film\n",
      "Negative 4422.0 thi\n",
      "Negative 3598.0 hi\n",
      "Negative 3395.0 movi\n",
      "Negative 2198.0 wa\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "model_fit_eval(vectorizer, train_stemmed, test_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M8: Unigrams (only adjectives/adverbs) + PorterStemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  82.0 %\n",
      "Important words in positive reviews\n",
      "Positive 1873.0 hi\n",
      "Positive 1243.0 thi\n",
      "Positive 1200.0 just\n",
      "Positive 1134.0 good\n",
      "Positive 736.0 best\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 1391.0 just\n",
      "Negative 1360.0 hi\n",
      "Negative 1292.0 thi\n",
      "Negative 1042.0 good\n",
      "Negative 944.0 bad\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "model_fit_eval(vectorizer, train_stemmed, test_stemmed, retain_adverbs_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M9: Unigrams (sublinear tf-idf) + PorterStemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  85.5 %\n",
      "Important words in positive reviews\n",
      "Positive 22.794618033 wa\n",
      "Positive 21.0819293613 charact\n",
      "Positive 20.976549597 like\n",
      "Positive 19.9104065757 make\n",
      "Positive 19.6801508875 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 24.8602761209 wa\n",
      "Negative 23.5683590901 like\n",
      "Negative 21.5263185952 charact\n",
      "Negative 20.958060194 just\n",
      "Negative 19.8974624979 make\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "model_fit_eval(vectorizer, train_stemmed, test_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M10: Bigrams (absence/presence) + PorterStemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  84.5 %\n",
      "Important words in positive reviews\n",
      "Positive 374.0 thi film\n",
      "Positive 229.0 thi movi\n",
      "Positive 111.0 special effect\n",
      "Positive 108.0 film wa\n",
      "Positive 107.0 hi wife\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 348.0 thi film\n",
      "Negative 296.0 thi movi\n",
      "Negative 145.0 look like\n",
      "Negative 124.0 like thi\n",
      "Negative 116.0 special effect\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2), binary=True, stop_words='english')\n",
    "model_fit_eval(vectorizer, train_stemmed, test_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<a id='other_function'>Other function to do text preprocessing</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# create English stop words list (you can always define your own stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"Function to remove stop words from sentences & lemmatize verbs and nouns. \n",
    "    Args:\n",
    "        doc(str): a string with any format        \n",
    "    \"\"\"\n",
    "    # tokenize using NLTK’s recommended word tokenizer \n",
    "    # (currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\n",
    "    # https://www.nltk.org/api/nltk.tokenize.html\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    print('tokenized: ',tokenized)\n",
    "    \n",
    "    # remove punctuation mark\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    print('punctuation_free: ',punctuation_free)\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x) and x not in stop_words]\n",
    "    print('stop_free: ',stop_free)\n",
    "\n",
    "    # lemmatize verbs\n",
    "    lemma_verb = [lemmatizer.lemmatize(word,'v') for word in stop_free]\n",
    "    print('lemma_verb: ',lemma_verb)    \n",
    "    \n",
    "    # lemmatize nouns\n",
    "    lemma_noun = [lemmatizer.lemmatize(word,'n') for word in lemma_verb]\n",
    "    print('lemma_noun: ',lemma_noun)\n",
    "    \n",
    "    # keep only token with length longer than 2\n",
    "    y = [s for s in lemma_noun if len(s) > 2]\n",
    "    print('length>2: ',y)    \n",
    "    \n",
    "    # Apply POS tagging\n",
    "    # reference for the POS acronym: https://cs.nyu.edu/grishman/jet/guide/PennPOS.html\n",
    "    word_posTags = pos_tag(y)\n",
    "    pos_tags = [t[1] for t in word_posTags]\n",
    "    print('pos_tags: ',pos_tags)   \n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized:  ['i', 'ate', 'a', 'lot', 'of', 'apples', ',', 'and', 'so', 'did', 'my', 'mom', '!']\n",
      "punctuation_free:  ['i', 'ate', 'a', 'lot', 'of', 'apples', 'and', 'so', 'did', 'my', 'mom']\n",
      "stop_free:  ['ate', 'lot', 'apples', 'mom']\n",
      "lemma_verb:  ['eat', 'lot', 'apples', 'mom']\n",
      "lemma_noun:  ['eat', 'lot', 'apple', 'mom']\n",
      "length>2:  ['eat', 'lot', 'apple', 'mom']\n",
      "pos_tags:  ['NN', 'NN', 'NN', 'NN']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eat', 'lot', 'apple', 'mom']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see from the result that the POS tagging is sooo wrong\n",
    "clean('I ate a lot of apples, and so did my mom!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 'VB')]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['go'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<a id='conclusion'>Conclusion</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model ID| Model |  Test Accuracy  | Top 5 Informative Words in Positive Reviews | Top 5 Informative Words in Negative Reviews |\n",
    "|------|------|------|\n",
    "|M1|Unigrams (absence/presence)|86.5% |film, movie, like, just, time |film, movie, like, just, time |\n",
    "|M2|Unigrams with frequency count|81.5% |film, movie, like, just, good |film, movie, like, just, time |\n",
    "|M3|Unigrams (only adjectives/adverbs)|82.5% |just, good, best, really, little |just, good, bad, really, little |\n",
    "|M4|Unigrams (sublinear tf-idf)|85% |movie, like, story, life, just |movie, like, kust, bad, good |\n",
    "|M5|Bigrams (absence/presence)|82.5% |special effects, ive seen, year old, new york, takes place |special effects, new york, year old, looks like, look like |\n",
    "|M6|Unigrams (absence/presence) + PorterStemmer|85.5% |like, wa, time, make, chacact |like, wa, chacact, make, time  |\n",
    "|M7|Unigrams with frequency count + PorterStemmer|82% |film, hi, thi, movi, ha |film, thi, hi, movi, wa |\n",
    "|M8|Unigrams (only adjectives/adverbs) + PorterStemmer|82% |hi, thi, just, good, best |just, hi, thi, good, bad |\n",
    "|M9|Unigrams (sublinear tf-idf) + PorterStemmer|85.5% |wa, charact, like, make, time |wa, like, charact, just, make |\n",
    "|M10|Bigrams (absence/presence) + PorterStemmer|84.5% |the film, the movi, special effect, film wa, hi wife |thi film, thi movi, look like, like thi, special effect |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model among M1-M5 is M1 and the worst performance model is M2. Also, the top 5 informative words is captured by using the total weight of each word in each class. The word with the highest total weight would have the largest effect when calculating the probability, and hence the word is informative. For the best model M1, it turns out that for both positive and negative reviews, most of the informative words are pretty similar, whereas for other models such as M3 (Unigrams (only adjectives/adverbs), the model capture \"best\", and \"bad\" as most informative words for positive and negative reviews separately, which makes more sense.\n",
    "\n",
    "For M6-M10, which are the models using the stemming text, the overall performance doesn't improve much (pretty similar to M1-M5 actually). Also, the informative words become a little harder to read. In this case, stemming might not worth doing.\n",
    "\n",
    "In conclusion, our best model result in 86.5% test accuracy, which is not a bad starting for a movie review sentiment analysis. As mentioned, there are many other things we can try to make the performance better. For the purpose of this post, we'll stop it and leave them for future attempts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='refer'>Reference</a>\n",
    "\n",
    "* [Stanford NLP: Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "* [Text analysis: basic workflow](https://cfss.uchicago.edu/text001_workflow.html)\n",
    "* [Doc: TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "* [Doc: TfidfTransformer (clear document of how tf and idf is being implemented)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)\n",
    "* [TF-IDF.com](http://www.tfidf.com/)\n",
    "* [Wiki: TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "* [How are TF-IDF calculated by the scikit-learn TfidfVectorizer](https://stackoverflow.com/questions/36966019/how-aretf-idf-calculated-by-the-scikit-learn-tfidfvectorizer)\n",
    "* [Speech and Language Processing Chap 3: N-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "* [What is the difference between n-gram models and the Naive Bayes? How do they interact to each other?](https://www.quora.com/What-is-the-difference-between-n-gram-models-and-the-Naive-Bayes-How-do-they-interact-to-each-other)\n",
    "* [What are N-Grams?](http://text-analytics101.rxnlp.com/2014/11/what-are-n-grams.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
