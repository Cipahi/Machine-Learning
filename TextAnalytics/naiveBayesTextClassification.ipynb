{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [Simple Example](#example)\n",
    "* [Bayes Theorem](#bayes)\n",
    "* [Naive Bayes classifier](#nb)\n",
    "* [Vector Space Model](#vc)\n",
    "    * [Bernoulli Naive Bayes](#bernoulli)\n",
    "    * [Multinomial Naive Bayes](#multinomial)\n",
    "    * [Laplace Smoothing](#laplace)\n",
    "    * [Bernoulli vs Multinomial Naive Bayes](#bernoullevsmultinomial)  \n",
    "* [Implementation using Sklearn](#implement)\n",
    "* [Pros and Cons of Naive Bayes](#procon)\n",
    "* [Area for Improvement](#improve)\n",
    "* [Reference](#refer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johnny 2018-04-08 13:32:42 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "\n",
      "pandas 0.20.3\n",
      "sklearn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%watermark -a 'Johnny' -d -t -v -p pandas,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='intro'>Introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a newbie for text analytics, I have heard several times that Naive Bayes is the most simple way to implement a simple text classification model. The motivation of this notebook is to learn and also summarize how to implement a simple naive bayes model to a binary text classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='example'>Simple Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same example, a news topic text classification.,from the blog, [A practical explanation of a Naive Bayes classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/#advanced-techniques), let's see how can Naive Bayes help us solve the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sports</td>\n",
       "      <td>A great game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sports</td>\n",
       "      <td>The election was over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Very clean match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>A clean but forgettable game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not sports</td>\n",
       "      <td>It was a close election</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                          text\n",
       "0      Sports                  A great game\n",
       "1  Not sports         The election was over\n",
       "2      Sports              Very clean match\n",
       "3      Sports  A clean but forgettable game\n",
       "4  Not sports       It was a close election"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': ['A great game',\n",
    "                            'The election was over',\n",
    "                            'Very clean match',\n",
    "                            'A clean but forgettable game',\n",
    "                            'It was a close election'],\n",
    "                   'category': ['Sports', 'Not sports', 'Sports', 'Sports', 'Not sports']})                 \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use know whether a news should be sports related or not based on the text, which is the news content. Suppose we have a new article with the content `a very close game`, how do we know if this article is sports related or not?\n",
    "\n",
    "Actually, what we really want to know is v. If this probability is over some threshold, say 0.5, then we say that the new article is sports related. However, how do we get this probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayes'>Bayes Theorem</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bayes Theorem, we can know that\n",
    "\n",
    "$P( \\text{Sports | a very close game} )$ = $\\frac{P( \\text{a very close game | Sports}) \\times P(Sports)}{P( \\text{a very close game} )} \\propto P( \\text{a very close game | Sports}) \\times P(Sports)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question comes up, it rarely we will have another article with the samee content, i.e., `a very close game`, how do we calcualte $P(\\text{a very close game | Sports})$? This is where the **Naive** part of bayes theorem comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='nb'>Naive Bayes classifier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to conditional probability, we know that \n",
    "\n",
    "$P(\\text{a very close game | Sports}) = P(\\text{a | Sports}) * P(\\text{very | Sports, a}) * P(\\text{close | Sports, a, very}) * P(\\text{game | Sports, a, very, close})$ \n",
    "\n",
    "Naive Bayes classifier assumes that we know this article is Sports related or not, then knowing whether the word \"a\" appear in the article does not effect the probability of \"very\" appearing in the article. More specifically, the above equation will become:\n",
    "\n",
    "$P(\\text{a very close game | Sports}) = P(\\text{a | Sports}) * P(\\text{very | Sports}) * P(\\text{close | Sports}) * P(\\text{game | Sports})$ \n",
    "\n",
    "In other words, this means that we’re no longer looking at entire sentences, but rather at individual words. So for our purposes, “a very close game” is the same as “very game a close” and “game a very close”.\n",
    "\n",
    "Originally, $P(\\text{very | Sports, a})$ indicates that given that we know the article is sports related and with the word \"a\" in it, what the probabilty is that the word \"very\" shows up in the article as well. With the conditional independence assumption, this probabilty is then the same as the that given we know the article is sports related, what the probabillity is that the word \"very\" shows up in the article. In other words, we now only look at the probabilty of each of the single words rather than checking the whole sentence.\n",
    "\n",
    "With the above assumption, we can then start to see whether the sentence \"a very close game\" is sports related or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='vc'>Vector Space Model to represent document</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors.\n",
    "\n",
    "Documents are represented as vectors.\n",
    "\n",
    "$d_{j}=(w_{1,j},w_{2,j},\\dots ,w_{t,j})$\n",
    "\n",
    "where\n",
    "* j: the index of the document, in our case, we have 5 documents in our training data\n",
    "* t: the dimentiionality of the document.\n",
    "\n",
    "Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting.\n",
    "\n",
    "The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus).\n",
    "\n",
    "In our example, the dimentionality of our vector is 14, since there are 14 distinct words in our corpus, which is shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'great',\n",
       " 'game',\n",
       " 'the',\n",
       " 'election',\n",
       " 'was',\n",
       " 'over',\n",
       " 'very',\n",
       " 'clean',\n",
       " 'match',\n",
       " 'but',\n",
       " 'forgettable',\n",
       " 'it',\n",
       " 'close']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['a','great','game','the','election','was','over', 'very', 'clean', 'match', 'but', 'forgettable', 'it', 'close']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Naive Bayes, the two most commonly used method to calculate term weights are Bernoulli and Multinomial document model, both of which represent documents as a bag of words, using the Naive Bayes assumption. Both models represent documents using feature vectors whose components correspond to word types. \n",
    "\n",
    "* **Bernoulli document model**: a document is represented by a feature vector with binary elements taking value 1 if the corresponding word is present in the document and 0 if the word is not present.\n",
    "* **Multinomial document model**: a document is represented by a feature vector with integer elements whose value is the frequency of that word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='bernoulli'>Bernoulli Naive Bayes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the goal is to know $P(\\text{a very close game | Sports})$, more generally, we want to know\n",
    "\n",
    "$p(x_1, x_2, \\dots, x_{14} | y)= p(\\text{1,0,1,0,0,0,1,0,0,0,0,0,1 | y}) = \\prod_{i=1}^{14}p(x_i | y)$\n",
    "\n",
    "since the query `A very close game` equals to the following vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1=[1,0,1,0,0,0,1,0,0,0,0,0,1]\n",
    "q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bernoulli Naive Bayes, the term probability is based on \n",
    "\n",
    "$P(x_i \\text{ | y}) = P(\\text{i | y}) x_i + (1-P(\\text{i | y}))(1-x_i)$\n",
    "\n",
    "where\n",
    "* i: indicate a particular term in the corpus\n",
    "* $x_i$: whether this term is in the query of not. If the terms appears in the corpus, then it is 1; otherwise is 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For i=1:\n",
    "$P(x_1 \\text{ | y}) = P(\\text{a | y}) 1 + (1-P(\\text{i | y}))(1-1) = P(\\text{a | y}) = P(\\text{a | Sports})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the word likelihood $P(\\text{i | y})$, we can learn (estimate) these parameters from a training set of documents labelled with class D=y.\n",
    "\n",
    "$p(i∣D=y)=\\frac{n_y(i)}{N_y} = \\frac{\\text{#docs with the target single word}}{\\text{#docs for the class}}$\n",
    "\n",
    "Where:\n",
    "* $n_y(i)$ is the number of class D=y's document in which i is observed.\n",
    "* $N_y$ is the number of documents that belongs to class y.\n",
    "\n",
    "Hence, we know that $P(\\text{a | Sports})$ equals to 2/3, since the term `a` appears in 2 out of three documents labeled with Sports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, to calculate the final probability of this new query to be Sports related, we use the following equation\n",
    "\n",
    "$p(x_1, x_2, \\dots, x_{14})= \\prod_{i=1}^{14}p(x_i | y) = p(\\text{a | Sports}) \\times p(\\text{great | Sports}) \\times p(\\text{game | Sports}) \\dots \\times p(\\text{close | Sports}) = \\frac{2}{3} \\times (1-\\frac{1}{3}) \\times \\frac{2}{3} \\dots \\times \\frac{0}{3} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='multinomial'>Multinomial Naive Bayes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial distribution can be used to compute the probabilities in situations in which there are more than two possible outcomes. We throw coins for 10 times, assuming that it can have 3 outcomes, what's the prob that we'll have 3 ties, 3 head, 4 tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Multinomial Naive Bayes, instead of using whether if a term appears in a document or not as weight, it uses the number of time a term appears in the document as the weight in the vector. More specifically, the term probability is based on\n",
    "\n",
    "$P(x_i \\text{ | y}) = \\frac{N_{yi}}{N_y} = \\frac{\\text{#words}}{\\text{#total number of words from the class}}$\n",
    "\n",
    "where\n",
    "* $N_{yi}$ is the number of times feature i appears in a sample of class y in the training set T\n",
    "* $N_y$ is the total count of all features for class y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, to calculate the final probability of this new query to be Sports related, we use the following equation\n",
    "\n",
    "$p(x_1, x_2, \\dots, x_{14})= \\prod_{i=1}^{14}p(x_i | y) = p(\\text{a | Sports}) \\times p(\\text{great | Sports}) \\times p(\\text{game | Sports}) \\dots \\times p(\\text{close | Sports}) = \\frac{2}{11} \\times \\frac{1}{11} \\times \\frac{2}{11} \\dots \\times \\frac{0}{11} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='laplace'>Laplace Smoothing</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, some of the terms will be 0 using either Bernoulli or Multinomial method. For example, since the term \"close\" doesn't appear in any Sports article. That means that P(close | Sports) = 0. This is rather inconvenient since we are going to be multiplying it with the other probabilities, so we’ll end up with $P(\\text{a | Sports}) \\times P(\\text{very | Sports}) \\times 0 \\times P(\\text{game | Sports})$. This equals 0, since in a multiplication, if one of the terms is zero, the whole calculation is nullified. Doing things this way simply doesn’t give us any information at all, so we have to find a way around.\n",
    "\n",
    "A commonly used method is called [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). By using Add-one smooth, which is a special case of laplace smoothing, we basically add 1 to every count so it’s never zero. Therefore, the term probability becomes\n",
    "\n",
    "**Bernoulli**\n",
    "\n",
    "$p(i∣y)=\\frac{n_y(i)+1}{N_y+d}$\n",
    "where \n",
    "* **d**: the number of document that is with class D=y \n",
    "\n",
    "**Multinomial**\n",
    "\n",
    "$P(x_i \\text{ | y}) = \\frac{N_{yi}+1}{N_y+d}$\n",
    "\n",
    "* **d**: the dimensionality of our corpus, in our case, it equals 14\n",
    "\n",
    "Therefore, using the laplace smoothing, the final probability of the query set `A very close game` equals\n",
    "\n",
    "**Bernoulli** $p(x_1, x_2, \\dots, x_{14})= \\prod_{i=1}^{14}p(x_i | y) = p(\\text{a | Sports}) \\times p(\\text{great | Sports}) \\times p(\\text{game | Sports}) \\dots \\times p(\\text{close | Sports}) = \\frac{3}{4} \\times (1-\\frac{2}{4}) \\times \\frac{3}{4} \\dots \\times \\frac{1}{4} $\n",
    "\n",
    "**Multinomial** $p(x_1, x_2, \\dots, x_{14})= \\prod_{i=1}^{14}p(x_i | y) = p(\\text{a | Sports}) \\times p(\\text{great | Sports}) \\times p(\\text{game | Sports}) \\dots \\times p(\\text{close | Sports}) = \\frac{3}{25} \\times \\frac{2}{25} \\times \\frac{3}{25} \\dots \\times \\frac{1}{25} $\n",
    "\n",
    "Noted that to determine whether if the new query set should be with the class Sports or Not-Sports, we don't need to calculate the term probability of the terms that doesn't show up in the query, since the term probability will be the same in both $P(x_i \\text{ | Sports})$ and $P(x_i \\text{ | Not Sports})$. It can be neglect if we only want to know the classification result instead the actual probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='bernoullevsmultinomial'>Bernoulli vs Multinomial Naive Bayes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have different method to choose when using Naive Bayes Classifier, how can we choose which one to use?\n",
    "\n",
    "We have known that Bernoulli models the presence/absence of a feature. Multinomial models the number of counts of a feature. Therefore, the variant of Naive Bayes we use depends on the data. If our data consists of counts, the multinomial distribution may be an appropriate distribution for the likelihood, and thus multinomial Naive Bayes is appropriate.\n",
    "\n",
    "One thing to [note](https://datascience.stackexchange.com/questions/27624/difference-between-bernoulli-and-multinomial-naive-bayes) is that whereas the binomial distribution generalises the Bernoulli distribution across the number of trials, the multinoulli distribution generalises it across the number of outcomes, that is, rolling a dice instead of tossing a coin. In other words, the denominator to calculate the term probability from each method is different.\n",
    "\n",
    "If our term weight is a continuous value, another common method to use is [Gaussian Nayes Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB), where we assume that the features follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='implement'>Implementation using Sklearn</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiNB_fit(df, x_colname, y_colname):\n",
    "    \"\"\"\n",
    "    fit multinomial naive bayes model.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): a dataframe having the document and label\n",
    "        x_colname (str): the colname for the document column\n",
    "        y_colname (str): the colname for the label column       \n",
    "    \n",
    "    Returns:\n",
    "       nb: A Sklearn MultinomialNB object\n",
    "       vect: the corpus obtained from training data\n",
    "    \"\"\"\n",
    "    \n",
    "    ### get document and label\n",
    "    X_train = df['text']\n",
    "    y_train = df['category']\n",
    "    \n",
    "    ### vectorize the document for both train and test\n",
    "    vect = CountVectorizer()\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "    ### See the result of the vectorization\n",
    "    print('feature name: ', vect.get_feature_names())\n",
    "\n",
    "    # convert to dense array for better visualize representation\n",
    "    print('training:')\n",
    "    print(X_train_dtm.toarray())\n",
    "\n",
    "    ### Fit Multinomial NB model and predict the final probability\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "            \n",
    "    return nb, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiNB_predict(nb_model, vect, x_test, predict_class=True):\n",
    "    \"\"\"\n",
    "    predict the classification result using the input multinomial naive bayes model.\n",
    "    \n",
    "    Args:\n",
    "        nb_model (sklearn.naive_bayes.MultinomialNB): Sklearn MultinomialNB object\n",
    "        vect (CountVectorizer): the corpus obtained from training data\n",
    "        x_test (pd.Series): a pd.Series contains the document for testing\n",
    "        predict_class (bol): indicate whether we want to get the predicted class or probability\n",
    "    \n",
    "    Returns:\n",
    "        array contains predicted class or probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    ### vectorize the document for test\n",
    "\n",
    "    X_test_dtm = vect.transform(x_test)\n",
    "    \n",
    "    # convert to dense array for better visualize representation\n",
    "    print('\\ntesting:')\n",
    "    print(X_test_dtm.toarray()) \n",
    "    \n",
    "    ### predict result\n",
    "    if (predict_class==True):        \n",
    "        pred = nb.predict(X_test_dtm)\n",
    "    else:\n",
    "        pred = nb.predict_proba(X_test_dtm)\n",
    "                    \n",
    "    print('library implementation', pred)\n",
    "    \n",
    "    return pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sports</td>\n",
       "      <td>A great game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sports</td>\n",
       "      <td>The election was over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Very clean match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>A clean but forgettable game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not sports</td>\n",
       "      <td>It was a close election</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                          text\n",
       "0      Sports                  A great game\n",
       "1  Not sports         The election was over\n",
       "2      Sports              Very clean match\n",
       "3      Sports  A clean but forgettable game\n",
       "4  Not sports       It was a close election"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at our training data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very close game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A clean election was over</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text\n",
       "0          A very close game\n",
       "1  A clean election was over"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually set our testing data\n",
    "x_test = pd.DataFrame({'text': ['A very close game',\n",
    "                               'A clean election was over']})\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature name:  ['but', 'clean', 'close', 'election', 'forgettable', 'game', 'great', 'it', 'match', 'over', 'the', 'very', 'was']\n",
      "training:\n",
      "[[0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 1 0 1]\n",
      " [0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [1 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# fit Multinomial Naive Bayes Model\n",
    "nb, vect = multiNB_fit(df, x_colname='text', y_colname='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:\n",
      "[[0 0 1 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 1 0 0 1]]\n",
      "library implementation ['Sports' 'Not sports']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Sports', 'Not sports'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final classification label for testing data\n",
    "multiNB_predict(nb, vect, x_test['text'], predict_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:\n",
      "[[0 0 1 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 1 0 0 1]]\n",
      "library implementation [[ 0.2035071   0.7964929 ]\n",
      " [ 0.82812184  0.17187816]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.2035071 ,  0.7964929 ],\n",
       "       [ 0.82812184,  0.17187816]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final classification probabilities for testing data\n",
    "multiNB_predict(nb, vect, x_test['text'], predict_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='procon'>Pros and Cons of Nayes Bayes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "* Famously good at text classification. e.g. spam filtering. Or domains where you have many equally important features, which tends to be a problem for other kind of classifiers, in particular tree based algorithms.\n",
    "* No parameter tuning is required\n",
    "* Very simple, easy to implement and fast.\n",
    "* If the NB conditional independence assumption holds, then it will converge quicker than discriminative models like logistic regression(?). Even if the NB assumption doesn’t hold, it works great in practice.\n",
    "* Need less training data.\n",
    "* Highly scalable. It scales linearly with the number of predictors and data points.\n",
    "* Can be used for both binary and mult-iclass classification problems.\n",
    "* Can make probabilistic predictions.\n",
    "* Handles continuous and discrete data.\n",
    "* Not sensitive to irrelevant features.\n",
    "\n",
    "**Cons**\n",
    "* Conditional independence is not always a valid assumption, thus can be outperformed by other methods.\n",
    "* Predicted probabilities are not well-calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='improve'>Areas for Improvement</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the blog: [A practical explanation of a Naive Bayes classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/#advanced-techniques), there are many things that can be done to improve this basic model. These techniques allow Naive Bayes to perform at the same level as more advanced methods. Some of these techniques are:\n",
    "\n",
    "* **Removing stopwords**. These are common words that don’t really add anything to the categorization, such as a, able, either, else, ever and so on. So for our purposes, The election was over would be election over and a very close game would be very close game.\n",
    "* **Lemmatizing words**. This is grouping together different inflections of the same word. So election, elections, elected, and so on would be grouped together and counted as more appearances of the same word.\n",
    "* **Using n-grams**. Instead of counting single words like we did here, we could count sequences of words, like “clean match” and “close election”.\n",
    "* **Using TF-IDF**. Instead of just counting frequency we could do something more advanced like also penalizing words that appear frequently in most of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "* [A practical explanation of a Naive Bayes classifier (Multinomial Naive Bayes Example)](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/#advanced-techniques)\n",
    "* [scikit learn: Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "* [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "* [Pros of Naive Bayes](https://www.quora.com/What-are-the-advantages-of-using-a-naive-Bayes-for-classification)\n",
    "\n",
    "* [Difference between Bernoulli and Multinomial Naive Bayes](https://datascience.stackexchange.com/questions/27624/difference-between-bernoulli-and-multinomial-naive-bayes)\n",
    "* [Bernoulli and Multinomial Naive Bayes from scratch](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/text_classification/naive_bayes/naive_bayes.ipynb)\n",
    "* [Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model)\n",
    "* [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)\n",
    "* [Naive Bayes and Text Classification](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html#3_3_multivariate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
