{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* [Read Data](#read)\n",
    "* [EDA](#eda)\n",
    "* [Feature Creation and Preprocessing](#preprocess)\n",
    "* [Modeling](#model)\n",
    "* [Scoring](#score)\n",
    "* [Predicition](#predict)\n",
    "* [Reference](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV #train_test_split, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johnny 2017-12-17 14:06:48 \n",
      "\n",
      "CPython 3.6.2\n",
      "IPython 6.2.1\n",
      "\n",
      "pandas 0.20.3\n",
      "numpy 1.13.1\n",
      "sklearn 0.19.1\n",
      "watermark 1.5.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -a 'Johnny' -d -t -v -p pandas,numpy,sklearn,watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"read\">Read Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train data\n",
    "data_dir = os.path.join('..', 'data')\n",
    "path_train = os.path.join(data_dir, 'train.csv')\n",
    "train = pd.read_csv(path_train)\n",
    "train.head()\n",
    "\n",
    "# read test data\n",
    "path_test = os.path.join(data_dir, 'test.csv')\n",
    "test = pd.read_csv(path_test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"eda\">EDA</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **survival**:\tSurvival, \t[0 = No, 1 = Yes]\n",
    "\n",
    "* **pclass**: Ticket class,\t[1 = 1st, 2 = 2nd, 3 = 3rd]\n",
    "    * A proxy for socio-economic status (SES)\n",
    "        * 1st = Upper\n",
    "        * 2nd = Middle\n",
    "        * 3rd = Lower\n",
    "* **sex**: Sex\t\n",
    "* **Age**: Age in years. Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "* **sibsp**: Number of siblings / spouses aboard the Titanic. \n",
    "    * Sibling = brother, sister, stepbrother, stepsister\n",
    "    * Spouse = husband, wife (mistresses and fianc√©s were ignored)\t\n",
    "* **parch**: Number of parents / children aboard the Titanic\t\n",
    "    * Parent = mother, father\n",
    "    * Child = daughter, son, stepdaughter, stepson\n",
    "    * Note: Some children travelled only with a nanny, therefore parch=0 for them.\n",
    "* **ticket**: Ticket number\t\n",
    "* **fare**: Passenger fare\t\n",
    "* **cabin**: Cabin number\t\n",
    "* **embarked**: Port of Embarkation, [C = Cherbourg, Q = Queenstown, S = Southampton]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many survivals in our train data?**\n",
    "\n",
    "We see that the number of survivals in the training data is not very unbalanced. Luckily, we don't need to deal with the unbalance issue in the problem.\n",
    "\n",
    "<img src=\"pic/Survived.png\" style=\"width: 600px;height: 450px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the survival rate across different features?**\n",
    "\n",
    "In all of the following plot, we see that the survival rate is somewhat different between different values for every features. Therefore, we decide to include all the feature to build our model.\n",
    "\n",
    "> **Age**\n",
    "\n",
    "<img src=\"pic/Age vs Survival.png\" style=\"width: 600px;height: 450px;\"/>\n",
    "\n",
    "\n",
    "> **Sex**\n",
    "\n",
    "<img src=\"pic/Sex vs Survival.png\" style=\"width: 600px;height: 450px;\"/>\n",
    "\n",
    "\n",
    "> **Pclass**\n",
    "\n",
    "<img src=\"pic/Pclass vs Survival.png\" style=\"width: 600px;height: 450px;\"/>\n",
    "\n",
    "> **SibSp**\n",
    "\n",
    "<img src=\"pic/Sibsp vs Survival.png\" style=\"width: 600px;height: 450px;\"/>\n",
    "\n",
    "> **Parch**\n",
    "\n",
    "<img src=\"pic/Parch vs Survival.png\" style=\"width: 600px;height: 450px;\"/>\n",
    "\n",
    "> **Fare**\n",
    "\n",
    "<img src=\"pic/Fare vs Survival.png\" style=\"width: 600px;height: 450px;\"/>\n",
    "\n",
    "> **Cabin**\n",
    "\n",
    "<img src=\"pic/Cabin vs Survival.png\" style=\"width: 600px;height: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"preprocess\">Preprocessing</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sklearn ensemble models doesn't accept categorical features as input type, we firstly need to transform all the categorical features into numeric values using label encoder. We could also use one-hot-encoding. Let's save it for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(train, test, response, sel_feature, cat_feature):\n",
    "    \"\"\"\n",
    "    1. Preprocess and return Train and Test dataset. \n",
    "    2. Preprocessing including selecting feature according to the input feature list.\n",
    "    3. Performing label encoding using category feature list.\n",
    "    4. Imputing columns with missing value with median\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas dataframe\n",
    "        A dataframe containing train data that includes the response column\n",
    "\n",
    "    test : pandas dataframe\n",
    "        A dataframe containing train test\n",
    "\n",
    "    response : str\n",
    "        Features that have importance scores lower than this\n",
    "        threshold will not be presented in the plot, this assumes\n",
    "        the sum of the feature importance sum up to 1.\n",
    "\n",
    "    sel_feature : list\n",
    "        A list contains all the needed feature for later model training\n",
    "    \n",
    "    cat_feature : list\n",
    "        A list contains the categorical feature that need to be label encodes            \n",
    "    \"\"\"\n",
    "    \n",
    "    # create response data\n",
    "    y_train = train['Survived']\n",
    "    \n",
    "    # combine data to do label encoding\n",
    "    X_train = train[sel_feature]\n",
    "    X_train['isTrain']=1\n",
    "    X_test = test[sel_feature]\n",
    "    X_test['isTrain']=0\n",
    "    X_all = pd.concat([X_train,X_test])\n",
    "    \n",
    "    # perform label encoding\n",
    "    X_all_new = X_all.copy()\n",
    "    le = LabelEncoder()\n",
    "    for col in cat_feature:\n",
    "        X_all_new[col]= le.fit_transform(X_all[col])\n",
    "\n",
    "    X_all = X_all_new.copy()\n",
    "    \n",
    "    # Dealing with missing data from the selected feature. Impute missing value with Median\n",
    "    # pd.isnull(X_all).any(axis=0)\n",
    "    X_all['Age'] = X_all['Age'].fillna(X_all['Age'].median())\n",
    "    X_all['Fare'] = X_all['Fare'].fillna(X_all['Fare'].median())    \n",
    "        \n",
    "    # separate train and test after label encoding\n",
    "    X_train = X_all[X_all['isTrain']==1]\n",
    "    X_test = X_all[X_all['isTrain']==0]\n",
    "    \n",
    "    # delete unneeded columns\n",
    "    del X_train['isTrain']\n",
    "    del X_test['isTrain']\n",
    "    \n",
    "    return(X_train, y_train, X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "response='Survived'\n",
    "sel_feature=['Pclass','Sex','Age','SibSp','Parch','Fare']\n",
    "cat_feature = ['Pclass','Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnychiu/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/johnnychiu/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = preprocessor(train, test, response, sel_feature, cat_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"model\">Model and Score</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we are going to build model using Bagging, Random forest, and gradient boosting machine by treating our response variable, i.e., survive or not, as a categorical features. After fitting a base model, we also use GridSearchCV to do parameter tuning.\n",
    "\n",
    "In order to prevent overfitting, we use 10 fold cross-validation to obtain an estimate of the test error and use it as the criterion to select our best model. Note that the model with the best CV score is not neccessary the best model, since CV score is only an estimate of the test error. The actual test error rate can only be obtained using the actual test dataset on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning(model, X_train, y_train, param_grid):   \n",
    "    \"\"\"\n",
    "    Tune a tree based model using GridSearch, and return a model object with an updated parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: sklearn's ensemble tree model\n",
    "        the model we want to do the hyperparameter tuning.\n",
    "    \n",
    "    X_train: pandas DataFrame\n",
    "        Preprocessed training data. Note that all the columns should be in numeric format.\n",
    "    \n",
    "    y_train: pandas Series\n",
    "    \n",
    "    param_grid: dict\n",
    "        contains all the parameters that we want to tune for the responding model.    \n",
    "        \n",
    "\n",
    "    Note\n",
    "    ----------\n",
    "    * we use kfold in GridSearchCV in order to make sure the CV Score is consistent with the score \n",
    "      we get from all the other function, including fit_bagging, fit_randomforest and fit_gbm. \n",
    "      We use model_selection.KFold with fixed seed in order to make sure GridSearchCV uses the same seed as model_selection.cross_val_score.\n",
    "    \n",
    "    \"\"\"\n",
    "    seed=7\n",
    "    \n",
    "#     if 'n_estimators' in param_grid:\n",
    "#         model.set_params(warm_start=True) \n",
    "    \n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    gs_model = GridSearchCV(model, param_grid, cv=kfold)\n",
    "    gs_model.fit(X_train, y_train)\n",
    "    # best hyperparameter setting\n",
    "    print('best parameters:{}'.format(gs_model.best_params_)) \n",
    "    print('best score:{}'.format(gs_model.best_score_)) \n",
    "    \n",
    "    # refit model on best parameters\n",
    "    model.set_params(**gs_model.best_params_)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fit a base model using default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bagging(X_train, y_train):\n",
    "    \"\"\"Bagged Decision Trees for Classification\"\"\"\n",
    "    seed = 7\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cart = DecisionTreeClassifier()\n",
    "    model = BaggingClassifier(base_estimator=cart, random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "    print(results.mean())\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817153558052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_bagging(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Parameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "num_trees=100\n",
    "cart = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_bag_1 = {\n",
    "    'max_samples': [0.1, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'max_features': [0.3, 0.5, 0.7, 0.9, 1]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'max_features': 0.9, 'max_samples': 0.1}\n",
      "best score:0.8395061728395061\n"
     ]
    }
   ],
   "source": [
    "bag_2 = parameter_tuning(bag, X_train, y_train, param_grid_bag_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_bag_2 = {\n",
    "    'max_samples': [i/100 for i in range(5, 16, 1)],\n",
    "    'max_features': [i/100 for i in range(70, 90, 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'max_features': 0.84, 'max_samples': 0.1}\n",
      "best score:0.8395061728395061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnychiu/Library/Python/3.6/lib/python/site-packages/sklearn/ensemble/bagging.py:348: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "bag_3 = parameter_tuning(bag_2, X_train, y_train, param_grid_bag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_bag_3 = {\n",
    "    'n_estimators': [10, 20, 50, 100,101, 110, 200]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'n_estimators': 100}\n",
      "best score:0.8395061728395061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnychiu/Library/Python/3.6/lib/python/site-packages/sklearn/ensemble/bagging.py:348: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "bag_4 = parameter_tuning(bag_3, X_train, y_train, param_grid_bag_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fit a base model using default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_randomforest(X_train, y_train):\n",
    "    seed=7\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    model = RandomForestClassifier(random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "    print(results.mean())\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808152309613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_randomforest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Parameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "num_trees=100\n",
    "rf = RandomForestClassifier(n_estimators=num_trees, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf_1 = {\n",
    "    'max_depth': [None, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 3, 5, 7, 9],\n",
    "    'max_features': ['auto', 'log2', None]\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'max_depth': 8, 'max_features': None, 'min_samples_leaf': 1}\n",
      "best score:0.8428731762065096\n"
     ]
    }
   ],
   "source": [
    "rf_2 = parameter_tuning(rf, X_train, y_train, param_grid_rf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf_2 = {'max_depth': [6, 7, 8, 9, 10, None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'max_depth': 8}\n",
      "best score:0.8428731762065096\n"
     ]
    }
   ],
   "source": [
    "rf_3 = parameter_tuning(rf_2, X_train, y_train, param_grid_rf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf_3 = {'n_estimators': [100, 200, 300, 400, 500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'n_estimators': 100}\n",
      "best score:0.8428731762065096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnychiu/Library/Python/3.6/lib/python/site-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "rf_4 = parameter_tuning(rf_3, X_train, y_train, param_grid_rf_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting Machine for Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fit a base model using default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gbm(X_train, y_train):\n",
    "    \"\"\"Gradient Boosting Machine for Classification\"\"\"\n",
    "    seed = 7   \n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    model = GradientBoostingClassifier(random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "    print(results.mean())\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839550561798\n"
     ]
    }
   ],
   "source": [
    "gbm_base = fit_gbm(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=7, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Parameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick n_estimators as large as (computationally) possible (e.g. 3000)\n",
    "* Tune max_depth, learning_rate, min_samples_leaf, and max_features via grid search\n",
    "* Increase n_estimators even more and tune learning_rate again holding the other parameters fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "num_trees=100\n",
    "gbm = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm_1 = {'max_depth': [3,  5,  7,  9, 11],                    \n",
    "                    'min_samples_leaf': [1, 3, 5, 7, 9], \n",
    "                    'max_features': ['auto', 'log2', None, 0.1, 0.3, 0.5, 1],\n",
    "                    'subsample': [0.1, 0.5, 1]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'max_depth': 5, 'max_features': 'auto', 'min_samples_leaf': 1, 'subsample': 1}\n",
      "best score:0.8451178451178452\n"
     ]
    }
   ],
   "source": [
    "gbm_2 = parameter_tuning(gbm, X_train, y_train, param_grid_gbm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm_2 = {'max_depth': [3,4,5,6,7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'max_depth': 4}\n",
      "best score:0.8462401795735129\n"
     ]
    }
   ],
   "source": [
    "gbm_3 = parameter_tuning(gbm_2, X_train, y_train, param_grid_gbm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm_3 = {\n",
    "                    'n_estimators': [100, 200, 300, 400, 500, 1000],\n",
    "                    'learning_rate': [0.001, 0.01, 0.05, 0.1]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:{'learning_rate': 0.1, 'n_estimators': 100}\n",
      "best score:0.8462401795735129\n"
     ]
    }
   ],
   "source": [
    "gbm_4 = parameter_tuning(gbm_3, X_train, y_train, param_grid_gbm_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"predict\">Predicition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the submission file for kaggle using the best model from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, X_test):\n",
    "    \"\"\"generate a prediction dataframe in the correct format for Kaggle submission using the input model and test data\"\"\"\n",
    "    \n",
    "    predict_y = model.predict(X_test)   \n",
    "    submission = pd.DataFrame({'PassengerId': test.PassengerId,\n",
    "                          'Survived': predict_y})\n",
    "    \n",
    "    return(submission)\n",
    "\n",
    "def make_submission(df, title):\n",
    "    \"\"\"save submission file to disc titled with current date and time\"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    title_with_time = now.strftime(\"%m%d_%H%M\")+ \"_\" +title  +  \".csv\"\n",
    "    submission_dir = os.path.join('..', 'submission')\n",
    "    path_submissiotn = os.path.join(submission_dir, title_with_time)\n",
    "    df.to_csv(path_submissiotn, sep=',', index = False)\n",
    "    print(\"File Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    276\n",
       "1    142\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_base.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_base = make_prediction(gbm_base, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "make_submission(submission_base, 'submission_base_gbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the submission file for the best model from Bagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_bag_best = make_prediction(bag_4, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    281\n",
       "1    137\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_bag_best.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "make_submission(submission_bag_best, 'submission_bag_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the submission file for the best model from Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_rf_best = make_prediction(rf_4, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    280\n",
       "1    138\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_rf_best.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "make_submission(submission_rf_best, 'submission_rf_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the submission file for the best model from GBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_gbm_best = make_prediction(gbm_4, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    277\n",
       "1    141\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_gbm_best.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "make_submission(submission_gbm_best, 'submission_gbm_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"reference\">Reference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Sklearn Bagging Classifier Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "* [Sklearn Random Forest Classifier Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [Sklearn Gradient Boosting Classifier Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "* [Sklearn GridSearchCV Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "* [Sklearn Kfold Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
