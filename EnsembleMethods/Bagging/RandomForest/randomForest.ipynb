{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is one type of bagging method from ensemble learning. Here is how sklearn describe bagging and I found it useful and easy to understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since single tree are often suffer from high variance, which means when we use split the training data into 2 parts at random, the built model for each half can be quite different. Bagging is a procedure that used to reduce the variance of a statistical leanring method. \n",
    "\n",
    "Here is the explaination of the main idea and motivation behind it.\n",
    "Assume that we have $n$ different sets of observations, each sets of observations can be used as a training dataset. Suppose each training dataset is with variance $\\alpha^2$, which is the amount by which our model would change if we estimated it using a different training dataset. The variance of the mean of the sets of the observations is given by $\\frac{\\alpha^2}{n}$. In other words, averaging a set of observations reduces variance. In practice, it is usually not possible for us to have several training data sets. In Bagging, we generate several training dataset by bootstraping. We take repeated samples from the single training data set.\n",
    "\n",
    "In the previous section when we estimate the test error using cross validation. We divide the training data set into K folds, and then build our model of the kth fold and evaulate the test error using the left-out kth fold. Then, we get our final estimated test error by averaging the test error from the folds. \n",
    "\n",
    "In bagging, it turns out we can use another technique called **Out of Bag Error Estimation** to estimate the test error. Originally, if we also want to use cross validation to estimate the test error of our training data set, performing bagging on large data sets would be computationally expensive. Let's assume we want to use 10-fold cross validation and bootstrap 21 training data set when building a model. In this case, for each fold we'll need to build 20 models, i.e, we train our method on the **bth** bootstrapped training set in order to get our estimated function, denoted by $\\hat{f}^b(x)$. We then get our final prediction using the average of the predictions from each model, $\\hat{f}^1(x), \\hat{f}^2(x), \\dots \\hat{f}^21(x)$. Therefore, totally we will need to build $21 \\times 10 = 210$ models in order to get an estimation of the test error. \n",
    "\n",
    "It turns out that we can actually get our test error estimation for a bagged model in an intuitively way. Since we know that in bagging, essentially we fit our model on a subset of the original training data set. For illustration purpose, let's say in each subset, we randomly use 2/3 of the original data set, and we totally take 21 bootstrapped training sets. For the $i_{th}$ observation, it will be in the training set in 14 of the bootstrapped training sets. We can predict the response for the $i_{th}$ observation using each of the trees in which taht observation was out ob bag (OOB). This will yield around 7 predictions for the $i_{th}$ observation. To obtain a single prediction, we can average these predicted responses for regresssion tree or can take a mojority vote for classification tree. By using **Out of Bag Error Estimation**, we will only need to build 21 models but still get a fairly good estimation of the test error. It is particularly convenient when performing bagging on large data sets. \n",
    "\n",
    "Below are some description about bagging from Sklearn's documentaion\n",
    "```\n",
    "*In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).*\n",
    "\n",
    "*Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:*\n",
    "* *When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting.*\n",
    "* *When samples are drawn with replacement, then the method is known as Bagging.*\n",
    "* *When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces.*\n",
    "* *Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bagging, we make predictions using the bootstrapped training set from our original training data set. Suppose we have a very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of the tree that we built using each bootstrapped training set, that most strong predictor would likely be used in every of the bagged trees. If we are averaging many correlated trees, let's say in the worst case, all the trees are the same. Then, the averaged prediction will essentially be the same as a single tree. This means that bagging will not lead to a substaintial reduction in variance over a single tree if most of our bagged tree are correlated with each other. \n",
    "\n",
    "In randomforest, each time a split in a tree is considered, $m$ ramdom subset of the $p$ predictors can only be used to make a split. Typically, we choose $m \\approx \\sqrt{p}$. Randomforest is usually better than bagging since on average $\\frac{m-p}{p}$ of the splits will not even consider the most strong predictor that will make the bagged tree become more correlated. According the ISLR book, we can think of this process as decorrelating the trees.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use the [Pima Indians Diabetes Dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) and build up a classification model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters to adjust when using these methods is **n_estimators** and **max_features**. \n",
    "\n",
    "* **n_estimators** is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. \n",
    "\n",
    "* **max_features** is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. \n",
    "\n",
    "Empirical good default values are ```max_features=n_features for regression problems```, and ```max_features=sqrt(n_features)``` for classification tasks (where n_features is the number of features in the data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the most important parameter is the n_estimators and max_features, many other parameters are also available. Here are the some of it, and you can find the description of all the parameters [here].(http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier):\n",
    "\n",
    "* **n_estimators**: The number of trees in the forest.\n",
    "\n",
    "\n",
    "* **max_features**: int, float, string or None, optional (default=”auto”)\n",
    "\n",
    "    The number of features to consider when looking for the best split:\n",
    "    * If int, then consider max_features features at each split.\n",
    "    * If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "    * If “auto”, then max_features=sqrt(n_features).\n",
    "    * If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "    * If “log2”, then max_features=log2(n_features).\n",
    "    * If None, then max_features=n_features.\n",
    "\n",
    "\n",
    "* **max_depth**:The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Good results are often achieved when setting ```max_depth=None``` in combination with ```min_samples_split=2```\n",
    "\n",
    "\n",
    "* **min_samples_split**: int, float, optional (default=2)\n",
    "    \n",
    "    The minimum number of samples required to split an internal node:\n",
    "    * If int, then consider min_samples_split as the minimum number.\n",
    "    * If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "    \n",
    "    \n",
    "* **min_samples_leaf**:  int, float, optional (default=1)\n",
    "    \n",
    "    The minimum number of samples required to be at a leaf node:\n",
    "    * If int, then consider min_samples_leaf as the minimum number.\n",
    "    * If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "\n",
    "* **criterion**: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.\n",
    "\n",
    "\n",
    "* **bootstrap**: Whether bootstrap samples are used when building trees. For random forest, the default is True.\n",
    "\n",
    "\n",
    "* **oob_score**: Whether to use out-of-bag samples to estimate the generalization accuracy. (?)\n",
    "\n",
    "\n",
    "* **random_state**:If int, random_state is the seed used by the random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(514, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 8)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(514,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifer & Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we use cross validation to do model selection in case if we have multiple models that we don't know which one performs the best.\n",
    "\n",
    "Notice that the ```cross_val_score``` will automatically apply different score metric to different kinds of problems. Since it is binary classification in our case, ```cross_val_score``` apply ```f1 score``` as the score metrics. Therefore, the number *0.756787330317* is the average ```f1 score``` among all the folds that we build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bagging(X_train, y_train):\n",
    "    \"\"\"Bagged Decision Trees for Classification\"\"\"\n",
    "    seed = 7\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cart = DecisionTreeClassifier()\n",
    "    num_trees = 100\n",
    "    model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "    print(results.mean())\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "def fit_randomforest(X_train, y_train, max_features=\"auto\"):\n",
    "    seed=7\n",
    "    num_trees = 100\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features, random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "    print(results.mean())\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774245852187\n",
      "0.756787330317\n"
     ]
    }
   ],
   "source": [
    "model_bag = fit_bagging(X_train, y_train)\n",
    "model_rf = fit_randomforest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    165\n",
       "1.0     89\n",
       "dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bagging\n",
    "y_bag_pred = model_bag.predict(X_test)\n",
    "pd.Series(y_bag_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    171\n",
       "1.0     83\n",
       "dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "y_rf_pred = model_rf.predict(X_test)\n",
    "pd.Series(y_rf_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_test, y_pred):\n",
    "    \"\"\" A function that calculate all the evaluation metrics for classification problems. \"\"\" \n",
    "    # single line to calculate ```Precision, Recall, F1 Score```\n",
    "    # metrics.precision_recall_fscore_support(y_test, y_pred)\n",
    "    # metrics.precision_recall_fscore_support(y_test, y_pred, pos_label=1, average='binary')\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    \n",
    "    return {'Confusion Matrix': metrics.confusion_matrix(y_test, y_pred),\n",
    "           'precision_score': metrics.precision_score(y_test, y_pred),\n",
    "           'recall_score': metrics.recall_score(y_test, y_pred),\n",
    "           'f1_score': metrics.f1_score(y_test, y_pred),\n",
    "           'auc': metrics.auc(fpr, tpr)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': array([[134,  34],\n",
       "        [ 31,  55]]),\n",
       " 'auc': 0.71857696566998908,\n",
       " 'f1_score': 0.62857142857142856,\n",
       " 'precision_score': 0.6179775280898876,\n",
       " 'recall_score': 0.63953488372093026}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification(y_test, y_bag_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': array([[140,  28],\n",
       "        [ 31,  55]]),\n",
       " 'auc': 0.73643410852713176,\n",
       " 'f1_score': 0.65088757396449703,\n",
       " 'precision_score': 0.66265060240963858,\n",
       " 'recall_score': 0.63953488372093026}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification(y_test, y_rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that since bagging using all the feature to generate the forest, the AUC from the Bagging is lower than Random forest. It aligns with our knowledge that by using only $m$ ramdom subset of the $p$ predictors, random forest can lower the variance of the model, which leads to a better test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the collection of bagged trees is much more difficult to interpret than a single tree, we can obtain an overall summary of the importance of each predictor using the RSS(for bagging regression trees) or the Gini index(for bagging classification trees). \n",
    "\n",
    "In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all B trees.\n",
    "\n",
    "In the `feature_importances_` from *Sklearn*, the importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is calculated using Gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_importance(model, feature_names, n_features):\n",
    "    \"\"\"Print out the relative importance of predictors\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:        \n",
    "        importances = np.mean([tree.feature_importances_ \n",
    "                                       for tree in model.estimators_], axis=0)\n",
    "    \n",
    "    idx = np.argsort(importances)[::-1]\n",
    "    names = feature_names[idx]\n",
    "    scores = importances[idx]\n",
    "    \n",
    "    if hasattr(model, 'estimators_'):\n",
    "        tree_importances = np.asarray([tree.feature_importances_\n",
    "                                       for tree in model.estimators_])\n",
    "        importances_std = np.std(tree_importances, axis = 0)\n",
    "        scores_std = importances_std[idx]            \n",
    "    \n",
    "    y_pos = np.arange(1, n_features + 1)\n",
    "    plt.barh(y_pos, scores[::-1], align = 'center', xerr = scores_std[::-1])\n",
    "    plt.yticks(y_pos, names[::-1])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance Plot')    \n",
    "\n",
    "    print(\"Feature ranking:\")\n",
    "    for i in range(X.shape[1]):        \n",
    "        print(\"%d. feature %s (%f, var(%f))\" % (i + 1, names[i], scores[i], scores_std[i]))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature plas (0.374610, var(0.046965))\n",
      "2. feature mass (0.161982, var(0.038209))\n",
      "3. feature age (0.136701, var(0.040212))\n",
      "4. feature pedi (0.111333, var(0.038660))\n",
      "5. feature pres (0.068368, var(0.027248))\n",
      "6. feature preg (0.051263, var(0.021346))\n",
      "7. feature skin (0.048540, var(0.024249))\n",
      "8. feature test (0.047204, var(0.026215))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEPCAYAAABIut/fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1172f59b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_importance(model_bag, dataframe.columns[0:8], X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature plas (0.263358, var(0.080191))\n",
      "2. feature mass (0.164260, var(0.055310))\n",
      "3. feature age (0.158906, var(0.053437))\n",
      "4. feature pedi (0.113582, var(0.037402))\n",
      "5. feature pres (0.086806, var(0.031984))\n",
      "6. feature skin (0.074008, var(0.032079))\n",
      "7. feature preg (0.069775, var(0.030180))\n",
      "8. feature test (0.069305, var(0.032802))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEPCAYAAABIut/fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1173f1b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_importance(model_rf, dataframe.columns[0:8], X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the result of bagging and random forest, we can see that `plas` seems to be a strong predictor. Therefore, in bagging, the importance of it is much higher than all the other features. In constrast, in random forest, since each time we can only use a subset of all the features, the importance is more balanced.\n",
    "\n",
    "From the above plot, we can see that the feature `plas` has the highest importance across all the features, meaning that in all the splits using `plas` as the feature to make a split, the total decreasing amount averaged by the number of nodes that use `plas` as the feature to make a split is the highest. \n",
    "\n",
    "Every time when we use `plas` to make a split, there is a decreased amount, the calculated variance is the variance of the decreasing amount among all the nodes that use `plas` to make a split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "* https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "* http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "* http://scikit-learn.org/stable/modules/ensemble.html#\n",
    "* [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/trees/random_forest.ipynb\n",
    "* http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
