{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Content:\n",
    "\n",
    "* [Boosting Methodology for Classification](#methodology)\n",
    "\n",
    "* [Gradient Boosting Machine for Classification](#model)\n",
    "\n",
    "* [Examples of Gradient Boosting Machine for Classification](#example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='methodology'>Boosting Methodology for Classification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "* Change loss function into cross entropy\n",
    "* Calculate the probability of cross entropy using Softmax function\n",
    "* Get the partial derivative of the overall loss function J\n",
    "* The gradient descent function of GBM classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continued our previous discussion on [Gradient Boosting Machine for Regression](http://nbviewer.jupyter.org/github/johnnychiuchiu/Machine-Learning/blob/master/EnsembleMethods/Boosting/boosting.ipynb), the overall process for building a GBRT is as follows.\n",
    "1. use response as the original resudual\n",
    "\n",
    "2. Iterate until converge\n",
    "\n",
    "    2.1 fit a model h, using X and residual. For GBRT, the model we use is Decision Tree. Note that we will denoted our first model as F in step 2.2\n",
    "    \n",
    "    2.2 Using the shrinkage parameter and the model h to update on the residual. The updated residual is calculated as $residual = residual - shrinkage \\times h.predict(X)$. The model we built until this step is $F:= F + \\rho * h$.\n",
    "    \n",
    "Note:\n",
    "* The number of tree we generates is the number of iterations in the second step. In the second step, when building a model on residual, it is essentially the same as build a model on negative gradient. That's why it is called gradient boosting machine.\n",
    "* Also, the final model $F$, is the addition of all the models generates in each iteration. For example, if our number of iteration is 100, when making a prediction, we will need to firstly make prediction using the first model, and the predicted output from the first model will be $shrinkage \\times \\text{predicted output}$. The final prediction will be the sum of $shrinkage \\times \\text{predicted output}$ using the same X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Entropy & Softmax Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Gradient Boosting Machine for Classification problems. When doing gradient boosting, we are trying to minimize our overall cost function by using gradient descent. Gradient descent is executed by fitting a model on residual in each iteration. We have shown that if we use least square as the loss function, fitting a model on residual is essentially the same as fitting a model on negative gradient.\n",
    "\n",
    "For classification, instead of using least square as the loss function, we will introduce another loss function called cross entropy, it is a way to capture the distance between two probability vectors. The smaller the cross entropy of the two vectors is, the nearer the distance is. Also, we will need another function called Softmax function, which turns a vector of scores into probabilities that sum up to 1. \n",
    "\n",
    "Once again, the same as GBRT, our overall loss function is defined as:\n",
    "$$J = \\sum_{i}^{n} L(y_i, F(x_i))$$\n",
    "\n",
    "For each observation, we will calculate the loss by cross entropy. If we have two vectors, one is $y_k$, which is a dummy indicator of the response variable that takes the value of 1 if the ith observation belongs to class k and 0 otherwise; another is $p(x_i)$, which is a vector of length k with each element, denoted as $p_k(x_i)$, represent the probability of the observation $x_i$ belong to class k, then the formula is as follow:\n",
    "\n",
    "$$L(y_i, p(x_i)) = - \\sum_{k=1}^{K}y_k log(p_k(x_i))$$\n",
    "\n",
    "Another question in mind is: How do we get the predicted probability of each class for each observation? As mentioned earlier, we use [Softmax function](https://en.wikipedia.org/wiki/Softmax_function) to help us. It is defined as follows:\n",
    "\n",
    "$$p_k(x_i) = \\frac{e^{score_k(x_i)}}{\\sum_{j=1}^{K} e^{score_j(x_i)}}$$\n",
    "\n",
    "Let's give an example of Softmax function using simple numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mySoftmax(z):\n",
    "    \"\"\"Transform list of numbers into list of probabilities using Softmax function\"\"\"\n",
    "    z_exp = [math.exp(i) for i in z]\n",
    "    sum_z_exp = sum(z_exp)\n",
    "\n",
    "    softmax = [round(i / sum_z_exp, 3) for i in z_exp]\n",
    "    return(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\n"
     ]
    }
   ],
   "source": [
    "z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "print(mySoftmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='model'>Gradient Boosting Machine for Classification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paragraph, we are going to firstly introduce the overall process of Gradient Boosting Machine for Classification. We will see that the underlying algorithm is still a regression tree. Afterwards, we are going to explain the reason behind it in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume our toy example with target value Y is [1,1,2,1]. We denote our train data as X, with a shape of 4*3, i.e., 4 observations with 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X \n",
      "\n",
      "   feature1  feature2  feature3\n",
      "0         1         2         1\n",
      "1         2         2         2\n",
      "2         3         7         3\n",
      "3         2         3         1\n",
      "\n",
      "\n",
      "y \n",
      "\n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "y=pd.Series([1,1,2,1])\n",
    "X=pd.DataFrame({'feature1': [1,2,3,2],\n",
    "               'feature2': [2,2,7,3],\n",
    "               'feature3': [1,2,3,1]})\n",
    "print(\"X \\n\")\n",
    "print(X)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"y \\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform our Y using One hot encoder, in this case, each element of Y will be a dummy indicator of the response variable that takes the value of 1 if the ith observation belongs to class k and 0 otherwise.\n",
    "\n",
    "```\n",
    "[ 1.,  0.],\n",
    "[ 1.,  0.],\n",
    "[ 0.,  1.],\n",
    "[ 1.,  0.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start out with a predicted probability that is uniform over all classes, i.e., $p_k(x_i)$= 1/(#unique class). The probabilty of each observation to be each class is the same across all the observations.  \n",
    "\n",
    "```\n",
    "[ 0.5,  0.5],\n",
    "[ 0.5,  0.5],\n",
    "[ 0.5,  0.5],\n",
    "[ 0.5,  0.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the initial residual as the difference between the predicted probability and the class label. The gradient is the difference between the associated dummy variable and the predicted probability of belonging to that class. This is essentially the \"residuals\" from the classification gradient boosting\n",
    "\n",
    "\n",
    "```\n",
    "[ 0.5, -0.5],\n",
    "[ 0.5, -0.5],\n",
    "[-0.5,  0.5],\n",
    "[ 0.5, -0.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate **Step 4** until convergence\n",
    "\n",
    "**Step 4.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a regression tree model for each class using X and the residual for that class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the score of X using the h calculated for each class. Using the shrinkage parameter and the model h to update on the residual for each class. The updated residual is calculated as $residual = residual - shrinkage \\times h.predict(X)$. The model we built until this step is $F:= F + \\rho * h$.\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that \n",
    "* the overall process of training a regression tree has still not changed, only now we must deal with the dummy variables, $y_k$ and fit a regression tree on the negative gradient for each dummy variable. In our sample above, we have $y_1$ =[1,1,0,1] and $y_2$=[0,0,1,0]\n",
    "\n",
    "* When making a prediction using testing dataset, we will need to apply all the trees that we build in each iteration for each class. For example, in our example, we have 2 class and suppose we only run 10 iterations. We'll have 2*10=20 tree being built up in the model building process. The tree built up in iteration 1 for class 1 is used to calcuate the probability of every observation belong to class 1. The the predicted output from the first iteration will be $shrinkage \\times \\text{predicted output}$ for each class. The raw prediction will be the sum of $shrinkage \\times \\text{predicted output}$ using the same X for each class. To obtain the final prediction, we will need to apply softmax function to make the prediction of each class for each observation summing up to 1.\n",
    "\n",
    "For example, the raw prediction can be something like\n",
    "```\n",
    "[ 0.49984154, -0.49984154],\n",
    "[ 0.49984154, -0.49984154],\n",
    "[-0.49952463,  0.49952463],\n",
    "[ 0.49984154, -0.49984154]\n",
    "```\n",
    "\n",
    "The first list represent the raw probabilty of 1st observation being in each class. After applying softmax function to each list, we get:\n",
    "\n",
    "```\n",
    "[ 0.73099627,  0.26900373],\n",
    "[ 0.73099627,  0.26900373],\n",
    "[ 0.26912839,  0.73087161],\n",
    "[ 0.73099627,  0.26900373]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some question in mind:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why do we build our regression tree model on the \"residual\", which is defined by the difference between predicted probability and the dummy vector? What's the relationship of the residual to gradient descent?\n",
    "\n",
    "> We can show that if the overall loss function is the sum of all the cross entropies, then the partial derivative of J is equals to \"residual\". That's why we use the build models on residual in each iteration. The derivation is [here](https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='example'>Examples of Gradient Boosting Machine for Classification</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, when fitting a GBM classifier, the most important parameters are similar to GBM Regressor:\n",
    "\n",
    "* **n_estimators**: The number of boosting stages to perform. One of the most important parameter.\n",
    "* **max_depth**: Depth of each individual tree. One of the most important parameter. \n",
    "* **loss**: Loss function. One of the most important parameter.\n",
    "* **learning_rate**: default=0.1. learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators. This parameter is the the shrinkage parameter **$\\rho$** mentioned prevoisly.\n",
    "* **min_samples_split**: The minimum number of samples required to split an internal node.\n",
    "* **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
    "* **subsample**: The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "* **max_features**: The number of features to consider when looking for the best split. Choosing max_features < n_features leads to a reduction of variance and an increase in bias. The idea of choosing max_features < n_features is the same as random forest.\n",
    "* **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default 0.\n",
    "\n",
    "Some parameter that we wouldn't change are:\n",
    "* **criterion**: The function to measure the quality of a split. Supported criteria are “friedman_mse” for the mean squared error with improvement score by Friedman, “mse” for mean squared error, and “mae” for the mean absolute error. The default value of “friedman_mse” is generally the best as it can provide a better approximation in some cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_binary(y_test, y_pred):\n",
    "    \"\"\" A function that calculate all the evaluation metrics for classification problems. \"\"\" \n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    \n",
    "    return {'Confusion Matrix': metrics.confusion_matrix(y_test, y_pred),\n",
    "           'accuracy': metrics.accuracy_score(y_test, y_pred),\n",
    "           'precision_score': metrics.precision_score(y_test, y_pred),\n",
    "           'recall_score': metrics.recall_score(y_test, y_pred),\n",
    "           'f1_score': metrics.f1_score(y_test, y_pred),\n",
    "           'auc': metrics.auc(fpr, tpr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_importance(model, feature_names, n_features):\n",
    "    \"\"\"Print out the relative importance of predictors\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:        \n",
    "        importances = np.mean([tree.feature_importances_ \n",
    "                                       for tree in model.estimators_], axis=0)\n",
    "    \n",
    "    idx = np.argsort(importances)[::-1]\n",
    "    names = feature_names[idx]\n",
    "    scores = importances[idx]\n",
    "    \n",
    "    if hasattr(model, 'estimators_'):\n",
    "        tree_importances = np.asarray([tree[0].feature_importances_\n",
    "                                       for tree in model.estimators_])\n",
    "        importances_std = np.std(tree_importances, axis = 0)\n",
    "        scores_std = importances_std[idx]            \n",
    "    \n",
    "    print(\"Feature ranking:\")\n",
    "    for i in range(X.shape[1]):        \n",
    "        print(\"%d. feature %s (%f, var(%f))\" % (i + 1, names[i], scores[i], scores_std[i]))\n",
    "    \n",
    "    y_pos = np.arange(1, n_features + 1)\n",
    "    plt.barh(y_pos, scores[::-1], align = 'center', xerr = scores_std[::-1])\n",
    "    plt.yticks(y_pos, names[::-1])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance Plot')   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_boosting_with_cv(X_train, y_train):\n",
    "    \"\"\"Gradient Boosting Machine for Classification\"\"\"\n",
    "    seed = 7   \n",
    "    num_trees = 100\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "    print(results.mean())\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for Binary Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [Pima Indians onset of Diabetes dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes). It is a binary classficaiton problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Train and Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 8)\n",
      "(254, 8)\n",
      "(514,)\n",
      "(254,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764630467572\n"
     ]
    }
   ],
   "source": [
    "model = fit_boosting_with_cv(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Prediction & Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    162\n",
       "1.0     92\n",
       "dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': array([[131,  37],\n",
       "        [ 31,  55]]),\n",
       " 'accuracy': 0.73228346456692917,\n",
       " 'auc': 0.70964839424141746,\n",
       " 'f1_score': 0.6179775280898876,\n",
       " 'precision_score': 0.59782608695652173,\n",
       " 'recall_score': 0.63953488372093026}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_binary(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature plas (0.232535, var(0.213334))\n",
      "2. feature pedi (0.183005, var(0.224189))\n",
      "3. feature mass (0.143236, var(0.133171))\n",
      "4. feature age (0.137555, var(0.136614))\n",
      "5. feature preg (0.090516, var(0.136693))\n",
      "6. feature pres (0.077394, var(0.119008))\n",
      "7. feature skin (0.068498, var(0.146230))\n",
      "8. feature test (0.067260, var(0.107529))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEWCAYAAACAOivfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhbNsKmKmNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3e1390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_importance(model, dataframe.columns[0:8], X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, feature importance is the average decrease in MSE due to spliting over a given predictor, averaged over the number of trees, which equals the value of the parameter n_estimators.\n",
    "\n",
    "In the plot above, we see that `pedi` has the highest mean feature importance. However, we see that the variance of all the feature importance are very large. It seems weird at the first look, but it actually make sense after a second thought. We could imagine the importance to change with different amount of iterations as well as different combinations of features. It would make sense to have variable importance change with using different features and over boosting iterations. In some iteration, weak features might outperform a single strong features. In other words, boosting will change its focus during iterations. That's why we see the variance is so large for all of our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for Multiple Label Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement gradient boosting for multiple label classification, we are going to use the [Wine Quality Data Set](https://archive.ics.uci.edu/ml/datasets/wine+quality). You can download the data from this [link](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv). The goal is to model wine quality based on the features we have in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine=pd.read_csv('../_data/winequality-red.csv', sep=';')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split the features and response column\n",
    "X = wine.drop('quality', axis = 1).values\n",
    "y = wine['quality'].values\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.645041830709\n"
     ]
    }
   ],
   "source": [
    "model_wine = fit_boosting_with_cv(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Prediction & Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    140\n",
       "6    136\n",
       "7     33\n",
       "8      5\n",
       "4      5\n",
       "3      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_wine.predict(X_test)\n",
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm library:  0.678125\n"
     ]
    }
   ],
   "source": [
    "print('gbm library: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature volatile acidity (0.145143, var(0.246263))\n",
      "2. feature alcohol (0.109728, var(0.119414))\n",
      "3. feature sulphates (0.099735, var(0.081396))\n",
      "4. feature pH (0.091595, var(0.095320))\n",
      "5. feature total sulfur dioxide (0.091060, var(0.119889))\n",
      "6. feature chlorides (0.086596, var(0.231213))\n",
      "7. feature density (0.086553, var(0.134173))\n",
      "8. feature residual sugar (0.081767, var(0.175914))\n",
      "9. feature fixed acidity (0.081529, var(0.093929))\n",
      "10. feature citric acid (0.078819, var(0.191199))\n",
      "11. feature free sulfur dioxide (0.047474, var(0.155757))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEWCAYAAAAXa4wFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nitXjHnoNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e44b710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_importance(model_wine, wine.columns[0:11], X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* [Ensemble Machine Learning Algorithms blog post from Machine Learning Mastery](https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/)\n",
    "* [Feature importance in gradient boosted trees](https://stats.stackexchange.com/questions/219769/feature-importance-in-gradient-boosted-trees)\n",
    "* [GRADIENT BOOSTING MACHINE (GBM) from Ethen](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/trees/gbm/gbm.ipynb)\n",
    "* [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
