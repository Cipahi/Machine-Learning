{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Notes on Word Embedding and Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* [What is word embedding?](#intro)\n",
    "* [Learn word embedding](#learn)\n",
    "    * [Train neural network models from Scratch](#example0)\n",
    "    * [Word2Vec](#word2vec)\n",
    "        * Skip Gram & CBOW\n",
    "        * [Example 1: training word2vec on dummy sentences](#example1)\n",
    "        * [Example 2: word2vec using pretrain weights on dummy sentences](#example2)\n",
    "            * Using the pre-train weights from Google new data\n",
    "            * Using the pre-train weights from GloVe\n",
    "    * [Text Classificaiton by using pre-trained weights](#example3)\n",
    "* [Reference](#refer)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='intro'>What is word embedding?</a>\n",
    "\n",
    "The goal of word embedding is to represent word in a vector space that can be used to know which words are semantically more similar to the others. For example, we can use word embedding to know *man* vs *woman* is equal to *king* to *??*. \n",
    "\n",
    "If we have a document with 10,000 words, the most simple way that we can represent word as a vector is by using one-hot encoding. However, doing this doesn't help our goal since the distance between all of the words are the same. *Orange* to *Apple* would be as different as *Orange* to *Table*.\n",
    "\n",
    "If we have several documents, it may seem that we could leverage TF-IDF to obtain word embedding. However, the output of TF-IDF is a vector for each word in a document. The same word in different documents can be with different value.  Hence, this can not be the word embedding that we would like to use.\n",
    "\n",
    "Most dominating ways to obtain good embedding are by using neural networks. There are several ways we can obtain the embeddings:\n",
    "\n",
    "1. train neural network models from Scratch\n",
    "2. train Word2Vec model\n",
    "3. train model with pretrain weights\n",
    "\n",
    "The word \"embedding\" comes from the concept that when each word is represented in a hyper-dimensional space, it is somehow \"embedded\" inside the cube, as we can seen from the following picture\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/linear-relationships.png\" style=\"width: 500px;height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='learn'>Learn Word Embedding</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='example0'>Train neural network models from scratch</a>\n",
    "\n",
    "If we have the following document\n",
    "\n",
    "```\n",
    "['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!',\n",
    "'Weak',\n",
    "'Poor effort!',\n",
    "'not good',\n",
    "'poor work',\n",
    "'Could have done better.']\n",
    "```\n",
    "\n",
    "with label \n",
    "[1,1,1,1,1,0,0,0,0,0]\n",
    "\n",
    "The input of the model would be a one hot vector representing each word, and goal is to use those one-hot vectors to predict the outcome, which in this case is 1 (positive sentiment) or 0 (negative sentiment).\n",
    "\n",
    "The goal here, however, it's not to actually predict the label when a new word comes in. The goal is to get the embedding matrix trained from the model. This \"fake\" model can help us know which two words are similar with each other in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 38], [39, 25], [44, 34], [12, 25], [8], [49], [36, 34], [17, 39], [36, 25], [31, 48, 38, 36]]\n",
      "[[19 38  0  0]\n",
      " [39 25  0  0]\n",
      " [44 34  0  0]\n",
      " [12 25  0  0]\n",
      " [ 8  0  0  0]\n",
      " [49  0  0  0]\n",
      " [36 34  0  0]\n",
      " [17 39  0  0]\n",
      " [36 25  0  0]\n",
      " [31 48 38 36]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 38],\n",
       " [39, 25],\n",
       " [44, 34],\n",
       " [12, 25],\n",
       " [8],\n",
       " [49],\n",
       " [36, 34],\n",
       " [17, 39],\n",
       " [36, 25],\n",
       " [31, 48, 38, 36]]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 8)\n"
     ]
    }
   ],
   "source": [
    "# access the embedding layer through the constructed model \n",
    "# first `0` refers to the position of embedding layer in the `model`\n",
    "# The shape of our embedding matrix is 50*8 with 50 words be represented in a 8 dimensional space\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751288831234\n",
      "-0.362344622612\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "# good & well are with high cosine similarity\n",
    "print(1 - spatial.distance.cosine(model.get_weights()[0][19], model.get_weights()[0][39]))\n",
    "\n",
    "# good & poor are with negative cosine similarity\n",
    "print(1 - spatial.distance.cosine(model.get_weights()[0][19], model.get_weights()[0][36]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='word2vec'>Word2Vec</a>\n",
    "\n",
    "Word2Vec is a famous and widely used algorithm to learn word embedding, and is developed by [Tomas Mikolov](https://www.linkedin.com/in/tomas-mikolov-59831188/) while he worked at Google. \n",
    "\n",
    "There are 2 method proposed by word2vec: Skip Grams & CBOW\n",
    "Both of them have a concept of context and target word. Let's what it means and what the input/output of the model is\n",
    "The following paragraph is borrowed from the [blog post by Lilian Weng](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)\n",
    "\n",
    "The following example demonstrates multiple pairs of target and context words as training samples, generated by a 5-word window sliding along the sentence.\n",
    "\n",
    "**“The man who passes the sentence should swing the sword.” – Ned Stark**\n",
    "\n",
    "| Sliding window (size = 5)| \tTarget word |\tContext|\n",
    "| --- | --- | ---| \n",
    "|[The man who]|\tthe\t|man, who|\n",
    "|[The man who passes]|\tman|\tthe, who, passes|\n",
    "|[The man who passes the]|\twho|\tthe, man, passes, the|\n",
    "|[man who passes the sentence]|\tpasses|\tman, who, the, sentence|\n",
    "|…\t|…\t|…|\n",
    "|[sentence should swing the sword]|\tswing|\tsentence, should, the, sword|\n",
    "|[should swing the sword]|\tthe|\tshould, swing, sword|\n",
    "|[swing the sword]|\tsword|\tswing, the|\n",
    "\n",
    "Each context-target pair is treated as a new observation in the data. For example, the target word “swing” in the above case produces four training samples: (“swing”, “sentence”), (“swing”, “should”), (“swing”, “the”), and (“swing”, “sword”).\n",
    "\n",
    "When training these nn models, the input is the context words and the output is the target words. The length of both vecters equals to the total number of uniqle words that we have in the document. We basically want to use the context word to predict the target words. The goal, however, is not to actually predict the target words, but to obtain the embedding matrix with, for example, 300*10k, using this matrix as a look-up table for each word. The column of this matrix is word embedding that we can use to see which word are semantically similar to each other. \n",
    "\n",
    "The concept of \"word\" to the \"document/sentence\" can be generalized to \"Song\" to \"playlist\". We can using the same approach to know which songs are similar to each other. The assumption is that songs that people listen not far away to each other are the songs similar to each other (or people would like to listen to). These \"song embedding\" can also be rolled up to a user level, by average all the songs that a user has listened to during a certain period of time and use the vector to represent their \"taste\". \n",
    "\n",
    "> **Skip Gram**\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png\" style=\"width: 500px;height: 240px;\"/>\n",
    "\n",
    "This is the actual architecture of the skip-gram model. Both the input vector x and the output y are one-hot encoded word representations. The hidden layer is the word embedding of size N.\n",
    "\n",
    "The hidden layer neurons just copy the weighted sum of inputs to the next layer. There is no activation like sigmoid, tanh or ReLU. The only non-linearity is the softmax calculations in the output layer.\n",
    "\n",
    "Note on the details of the Skip Gram method\n",
    "* If we have 10k words, the input would be an one-hot vector with 1*10k. The number of nodes in the hidden layer are the dimension we want to have for the embedding, for example, 300 is a number that is used in the original [paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). The output is also a `1*10k` vector, each value is a probability (the value for the target word should be close to 1).\n",
    "* Model structure: `1*10k` -> #params (10K, 300) -> hidden (300 nodes) -> #params (300, 10k) -> `1*10k` -> softmax activation to obtain prob -> `1*10k`\n",
    "* each input is a pair of the (context, target) word that we randomly select\n",
    "* There are heuristics to select context words to prevent us from having most of the context words being common words such as *the*, *or*, ...etc.\n",
    "* The assumption of the Skip Gram is that words that are more similar to each other should appear not far away from each other.\n",
    "\n",
    "Notice that when using this architecture, the number of #weights we have is very large, and all of which would be updated slightly by every one of our billions of training samples.\n",
    "\n",
    "That's why there are method to reduce the burden we have when training a word2vec model.\n",
    "Approached proposed by the author includes\n",
    "* Treating common word pairs or phrases as single “words” in their model.\n",
    "* Subsampling frequent words to decrease the number of training examples.\n",
    "* Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights.\n",
    "    * Some more note copied from [Chris McCormick's post](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "    * With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).\n",
    "    * The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.\n",
    "    * Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!\n",
    "    * In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).\n",
    "\n",
    "> **Continuous Bag of Words (CBOW)**\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" style=\"width: 500px;height: 280px;\"/>\n",
    "\n",
    "The Continuous Bag-of-Words (CBOW) is another similar model for learning word vectors. It predicts the target word (i.e. “swing”) from source context words (i.e., “sentence should the sword”).\n",
    "\n",
    "As we can seen from the picture above, because there are multiple contextual words, we average their corresponding word vectors, constructed by the multiplication of the input vector and the matrix W. Because the averaging stage smoothes over a lot of the distributional information, some people believe the CBOW model is better for small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='example1'>Example 1: training word2vec on dummy sentences</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gensim](https://radimrehurek.com/gensim/models/word2vec.html) package made training a word2vec model very simple. There are a few parameters we can use for the `Word2Vec` function provided by the package.\n",
    "\n",
    "* **size** (int, optional) – Dimensionality of the word vectors.\n",
    "* **window** (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "* **min_count** (int, optional) – Ignores all words with total frequency lower than this.\n",
    "* **sg** ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW. Default as 0 (CBOW)\n",
    "* **hs** ({0, 1}, optional) – If 1, hierarchical softmax (a method to make the training faster) will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "* **negative** (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "* **cbow_mean** ({0, 1}, optional) – If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "* **iter** (int, optional) – Number of iterations (epochs) over the corpus. Default as 5\n",
    "* **compute_loss** (bool, optional) – If True, computes and stores loss value which can be retrieved using get_latest_training_loss(). Can set to True if we want to plot out the interation by loss curve to see how the training works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "\n",
      "\n",
      "Embedding vector for the word \"sentence\"\n",
      "[ 0.00048939  0.00408542 -0.00023339 -0.00396113 -0.00327537 -0.00412497\n",
      " -0.00178292 -0.00437832  0.00489555 -0.00091878 -0.00305983 -0.00026808\n",
      " -0.00035443 -0.00381484 -0.00484873  0.00288246  0.00254953  0.00249098\n",
      "  0.00020342 -0.00273202  0.00166556  0.00313711 -0.0017676   0.00158689\n",
      "  0.00158976  0.00427394 -0.00336939  0.003442    0.0021848  -0.00069696\n",
      "  0.00439488  0.00206062 -0.00347071 -0.00197479  0.00442823  0.00494515\n",
      "  0.00415632  0.00094811  0.00350606  0.0021377  -0.0013946  -0.00313342\n",
      " -0.00186622 -0.00114939 -0.00458247  0.00200281 -0.00364599  0.00444268\n",
      "  0.00164236 -0.00240062 -0.00029585  0.00401085 -0.00405918 -0.00022826\n",
      " -0.00112923 -0.0015245   0.00293028  0.00094623  0.00427847  0.00032669\n",
      " -0.0021502  -0.0049019   0.00012381 -0.00447032  0.00367364 -0.00439407\n",
      " -0.00253174 -0.00132663 -0.00424001  0.00051208 -0.00172683 -0.00140225\n",
      "  0.00127032 -0.00075154  0.00101162 -0.00115589 -0.00098234 -0.00241391\n",
      " -0.00345926  0.00278816 -0.00169832  0.00140547 -0.00090343  0.00350404\n",
      " -0.00012841 -0.00027852 -0.00442721 -0.00296106  0.00127944  0.00469627\n",
      "  0.00331793 -0.00481954  0.00441015 -0.00068952  0.00277862  0.00176304\n",
      " -0.00391973  0.00375487 -0.00419286 -0.00231   ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# train model\n",
    "# we'll set is to specify use all possible cpu to train the model\n",
    "workers = cpu_count()\n",
    "model = Word2Vec(sentences, min_count=1, workers = workers)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "print('\\n')\n",
    "\n",
    "# access vector for one word\n",
    "print('Embedding vector for the word \"sentence\"')\n",
    "print(model.wv['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vector dimension:  (14, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>-0.003961</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.004125</td>\n",
       "      <td>-0.001783</td>\n",
       "      <td>-0.004378</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>-0.004820</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>-0.003920</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>-0.004193</td>\n",
       "      <td>-0.002310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.002589</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001787</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>-0.003923</td>\n",
       "      <td>-0.002521</td>\n",
       "      <td>-0.001706</td>\n",
       "      <td>-0.004513</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>-0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>-0.003675</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>-0.004843</td>\n",
       "      <td>-0.003620</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>-0.003818</td>\n",
       "      <td>-0.002045</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>-0.003775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>-0.001222</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.004712</td>\n",
       "      <td>-0.004576</td>\n",
       "      <td>-0.003952</td>\n",
       "      <td>-0.001933</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.001481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>-0.000846</td>\n",
       "      <td>-0.001470</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>-0.000778</td>\n",
       "      <td>-0.003111</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>-0.004706</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>-0.004653</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>-0.003270</td>\n",
       "      <td>-0.003773</td>\n",
       "      <td>0.003823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "sentence  0.000489  0.004085 -0.000233 -0.003961 -0.003275 -0.004125   \n",
       "the       0.002589 -0.004835 -0.000137  0.002902 -0.001457 -0.001600   \n",
       "this      0.001551  0.003661 -0.003675 -0.000043  0.003793  0.000991   \n",
       "is       -0.001222  0.002017 -0.000802 -0.004712 -0.004576 -0.003952   \n",
       "first    -0.000299 -0.001017  0.003187  0.000733  0.004506 -0.003052   \n",
       "\n",
       "                6         7         8         9     ...           90  \\\n",
       "sentence -0.001783 -0.004378  0.004896 -0.000919    ...     0.003318   \n",
       "the      -0.001007  0.001201  0.001009  0.001874    ...    -0.001787   \n",
       "this     -0.004843 -0.003620  0.004297  0.000726    ...     0.002480   \n",
       "is       -0.001933  0.002939 -0.002747 -0.001481    ...     0.002226   \n",
       "first    -0.000668 -0.000840 -0.001082  0.004420    ...     0.003569   \n",
       "\n",
       "                91        92        93        94        95        96  \\\n",
       "sentence -0.004820  0.004410 -0.000690  0.002779  0.001763 -0.003920   \n",
       "the      -0.002109 -0.003923 -0.002521 -0.001706 -0.004513  0.001444   \n",
       "this      0.000030  0.003841 -0.003818 -0.002045  0.000897  0.003276   \n",
       "is       -0.000846 -0.001470  0.002327 -0.000778 -0.003111  0.003591   \n",
       "first    -0.004474 -0.004653 -0.000127  0.002700  0.000683  0.001394   \n",
       "\n",
       "                97        98        99  \n",
       "sentence  0.003755 -0.004193 -0.002310  \n",
       "the      -0.000483  0.001370 -0.002054  \n",
       "this      0.001593  0.004817 -0.003775  \n",
       "is        0.002167 -0.004706  0.004500  \n",
       "first    -0.003270 -0.003773  0.003823  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain the learned word vectors (.wv.syn0)\n",
    "# and the vocabulary/word that corresponds to each word vector\n",
    "word_vectors = pd.DataFrame(model.wv.vectors, index = model.wv.index2word)\n",
    "print('word vector dimension: ', word_vectors.shape)\n",
    "word_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='example2'>Example 2: word2vec using pretrain weights on dummy sentences</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following content are from the post [How to Develop Word Embeddings in Python with Gensim, by Jason](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/). Big thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Using the pre-train weights from Google news data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the paper and code for word2vec, Google also published a pre-trained word2vec model on the Word2Vec Google Code Project.\n",
    "\n",
    "A pre-trained model is nothing more than a file containing tokens and their associated word vectors. The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors.\n",
    "\n",
    "It is a 1.53 Gigabytes file. You can download it from [here, GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/uc?export=download&confirm=0-86&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM). Unzipped, the binary file (GoogleNews-vectors-negative300.bin) is 3.4 Gigabytes.\n",
    "\n",
    "The Gensim library provides tools to load this file. Specifically, you can call the KeyedVectors.load_word2vec_format() function to load this model into memory, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = './data/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Using the pre-train weights from GloVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford researchers also have their own word embedding algorithm like word2vec called Global Vectors for Word Representation, or GloVe for short.\n",
    "\n",
    "Generally, NLP practitioners seem to prefer GloVe at the moment based on results.\n",
    "\n",
    "Like word2vec, the GloVe researchers also provide pre-trained word vectors, in this case, a great selection to choose from.\n",
    "\n",
    "You can download the GloVe pre-trained word vectors ([glove.6B.zip](https://nlp.stanford.edu/projects/glove/)) and load them easily with gensim.\n",
    "\n",
    "The first step is to convert the GloVe file format to the word2vec file format. The only difference is the addition of a small header line. This can be done by calling the glove2word2vec() function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = './data/glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = './data/glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7698541283607483)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = './data/glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='example3'>Text Classification by using pre-trained weights from GloVe</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage the pre-train weight to help us perform a text classification problem. The concept is the same as transfer learning, which we freeze the weight of the embedding matrix and only train on the final dense layer. Notice that the goal here is to perform text classfication. \n",
    "\n",
    "If we only want to know the word embedding, we don't need to train any model. All we need to do is to obtain the weights directly, just like the previous exmaples.\n",
    "\n",
    "In the following example, we can see that the accuracy has increased from ~90% to 100% by using the pre-trained weights from GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 100)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The wegihts from the embedding matrix are freezed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80795609951\n",
      "0.432923734188\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "# well & good are with high cosine similarity\n",
    "print(1 - spatial.distance.cosine(model.get_weights()[0][6], model.get_weights()[0][3]))\n",
    "\n",
    "# excellent & weak are with lower cosine similarity\n",
    "print(1 - spatial.distance.cosine(model.get_weights()[0][9], model.get_weights()[0][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80795609951\n",
      "0.432923734188\n"
     ]
    }
   ],
   "source": [
    "# the cosine similarity of these words are the same before and after the training\n",
    "print(1 - spatial.distance.cosine(embeddings_index.get('well'), embeddings_index.get('good')))\n",
    "print(1 - spatial.distance.cosine(embeddings_index.get('excellent'), embeddings_index.get('weak')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='refer'>Reference</a>\n",
    "\n",
    "* machinelearningmastery, Jason Brownlee\n",
    "    * [How to Use Word Embedding Layers for Deep Learning with Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
    "    * [How to Develop Word Embeddings in Python with Gensim](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)\n",
    "* [Chris Mccormick](http://mccormickml.com/)\n",
    "    * [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "    * [Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "    * [Applying word2vec to Recommenders and Advertising](http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/)\n",
    "* [Deep Learning with Python: 6.1-using-word-embeddings.ipynb](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb)\n",
    "* [Andrew Ng, deeplearning.ai video: word2vec](https://www.coursera.org/learn/nlp-sequence-models/lecture/8CZiw/word2vec)\n",
    "* [Cross Validated: How does Keras 'Embedding' layer work?](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)\n",
    "    * Useful explaination of the shape from output of the embedding layer.\n",
    "* [Word2vec (Skipgram) post by Ethen Liu](https://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/deep_learning/word2vec/word2vec_detailed.ipynb#Word2vec-(Skipgram)\n",
    "* [Machine learning @ Spotify - Madison Big Data Meetup](https://www.slideshare.net/AndySloane/machine-learning-spotify-madison-big-data-meetup)\n",
    "    * See page 34 for how spotify leverage word2vec in building it's recommender system\n",
    "* Useful resources contraining the actual architecture and explaining how Skip Gram and CBOW works.\n",
    "    * [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
    "    * [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)\n",
    "* [Gensim word2vec documentation](https://radimrehurek.com/gensim/models/word2vec.html)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
