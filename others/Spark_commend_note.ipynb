{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Command Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* [Intro](#intro)\n",
    "* [Installation](#install)\n",
    "* [Spark RDD API](#rdd)\n",
    "* [Spark DataFrame API](#df)\n",
    "* [Spark SQL](#sql)\n",
    "* [Machine Learning in PySpark](#ml)\n",
    "* [Reference](#refer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='intro'>Intro</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has two different kinds of APIs\n",
    "\n",
    "* **APIs**\n",
    "    * RDD API: lower level, we should use this when we deal with unstructured data\n",
    "    * DataFrame API: can be related to pandas dataframe in python.\n",
    "        * SparkSQL    \n",
    "* **2 modes**\n",
    "    * shell\n",
    "    * script\n",
    "    \n",
    "each block in the hadoop concept correspond to a partition of RDD.\n",
    "One file correspond to a RDD.\n",
    "\n",
    "* **Pros of Spark to MapReduce**: The main advantage of using Spark is that it can hold a portion of the original data in memory. It's easier to wrote any kinds of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='install'>Installation on Mac</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Steps**:\n",
    "    1. download version spark-2.3.0-bin-hadoop2.7.tgz from [Spark downloads page](http://spark.apache.org/downloads.html)\n",
    "    2. manually unzip the file\n",
    "    3. sudo mv spark-2.3.0-bin-hadoop2.7 /opt/spark-2.3.0-bin-hadoop2.7.tgz\n",
    "        * used sudo because we need permission to move files into opt folder\n",
    "    4. ln -s /opt/spark-2.3.0-bin-hadoop2.7 /opt/sparkÌ€\n",
    "        * create a shortcut to the actual folder\n",
    "    5. vi ~/.zshrc\n",
    "        * in other Linux the file should be ~/.bashrc\n",
    "        * I downloaded zsh for my command line, that's why I use this file\n",
    "    6. adding the following lines in zshrc file\n",
    "        * export SPARK_HOME=/opt/spark\n",
    "        * export PATH=$SPARK_HOME/bin:$PATH\n",
    "    7. create a new terminal tab, type `pyspark`\n",
    "    8. Used the second method to link pyspark to my jupyter notebook\n",
    "        * There is another and more generalized way to use PySpark in a Jupyter Notebook: use findSpark package to make a Spark Context available in your code.\n",
    "    9. run the sample code in below and it works.    \n",
    "      \n",
    "* **Reference**: [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14176064\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stackoverflow](https://stackoverflow.com/questions/47665491/pyspark-throws-typeerror-textfile-missing-1-required-positional-argument-na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not print anything unless it's an error\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='rdd'>Spark RDD API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing is going to happen without an action\n",
    "\n",
    "* **Transformations**: RDD to RDD\n",
    "    * **map**: takes every elements of a partition in RDD and do something\n",
    "    * **filter**:\n",
    "    * **reduceByKey**: since it will reduce by key, it's a transformation\n",
    "    * **flatMap**: one element with multiple output\n",
    "    * **sortByKey**: Sort (key, value) RDD by key\n",
    "    * **sortBy**: sort by custom condition or self select value\n",
    "    * **distinct**: get distinct values in RDD\n",
    "    * **groupBy**\n",
    "    * **groupByKey**    \n",
    "    * **mapValues**\n",
    "    * **join**: inner join\n",
    "        * **leftOuterJoin**: left join    \n",
    "        * **rightOuterJoin**: right join    \n",
    "        * **fullOuterJoin**: outer join  \n",
    "    * **mapPartitions**: do map function for each partition. The input is all the records in each partition and the output is a single element\n",
    "* **Actions**: RDD to various things\n",
    "    * **collect**: take all the data into memory\n",
    "    * **take**: return the first n elements of RDD\n",
    "    * **first**: return the first element of RDD\n",
    "    * **reduce**: reduce function without a key\n",
    "\n",
    "* To take a\n",
    "\n",
    "There are two ways to create RDD.\n",
    "* Read from file\n",
    "* create from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create RDD from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the second parameter indicates the number of parititions in the RDD\n",
    "myRDD = sc.parallelize([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the number of records in each partition using mapParititions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countRecord(x):\n",
    "    count = 0\n",
    "    for i in x:\n",
    "        count = count + 1\n",
    "    return [count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(countRecord).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isEven(x):\n",
    "    \"\"\"get all even number and output as list\"\"\"\n",
    "    resultList = []\n",
    "    for i in x:\n",
    "        if i%2==0:\n",
    "            resultList.append(i)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(isEven).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Return vs Yield using mapPartitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return: returns an object that we can be iterated as many time as we want\n",
    "def customMapPartitions(x):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        res.append(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yield: returns an object that can only be iterated once\n",
    "def customMapPartitions2(x):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        res.append(i)\n",
    "    yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(customMapPartitions).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(customMapPartitions2).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **broadcast variable in spark, similar to distributed cache in map reduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # after running this, the variable is also been broadcasted in the memory of each server\n",
    "toShare = sc.broadcst([1,2,3,4])\n",
    "toShare.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD2 = sc.textFile(\"data/Crimes_-_2001_to_present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location',\n",
       " '10078659,HY267429,05/19/2015 11:57:00 PM,010XX E 79TH ST,143A,WEAPONS VIOLATION,UNLAWFUL POSS OF HANDGUN,STREET,true,false,0624,006,8,44,15,1184626,1852799,2015,05/26/2015 12:42:06 PM,41.751242944,-87.599004724,\"(41.751242944, -87.599004724)\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **map (transformation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row will be mapped to one output \n",
    "\n",
    "* row1 -> list1\n",
    "* row2 -> list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.map(lambda x:x+1)\n",
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.map(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ID',\n",
       "  'Case Number',\n",
       "  'Date',\n",
       "  'Block',\n",
       "  'IUCR',\n",
       "  'Primary Type',\n",
       "  'Description',\n",
       "  'Location Description',\n",
       "  'Arrest',\n",
       "  'Domestic',\n",
       "  'Beat',\n",
       "  'District',\n",
       "  'Ward',\n",
       "  'Community Area',\n",
       "  'FBI Code',\n",
       "  'X Coordinate',\n",
       "  'Y Coordinate',\n",
       "  'Year',\n",
       "  'Updated On',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'Location'],\n",
       " ['10078659',\n",
       "  'HY267429',\n",
       "  '05/19/2015 11:57:00 PM',\n",
       "  '010XX E 79TH ST',\n",
       "  '143A',\n",
       "  'WEAPONS VIOLATION',\n",
       "  'UNLAWFUL POSS OF HANDGUN',\n",
       "  'STREET',\n",
       "  'true',\n",
       "  'false',\n",
       "  '0624',\n",
       "  '006',\n",
       "  '8',\n",
       "  '44',\n",
       "  '15',\n",
       "  '1184626',\n",
       "  '1852799',\n",
       "  '2015',\n",
       "  '05/26/2015 12:42:06 PM',\n",
       "  '41.751242944',\n",
       "  '-87.599004724',\n",
       "  '\"(41.751242944',\n",
       "  ' -87.599004724)\"']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.map(lambda x: x.split(\",\")).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **flatMap (transformation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of row will be one output\n",
    "\n",
    "Each row will be mapped to one output \n",
    "\n",
    "* element1 in row1 -> output1\n",
    "* element2 in row1 -> output2\n",
    "* element3 in row1 -> output3\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(['This is great great great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split(\" \")).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 1), ('is', 1), ('great', 1), ('great', 1), ('great', 1)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split(\" \")).map(lambda x: (x,1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **SortBy multiple columns/values conditions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [StackOverFlow](https://stackoverflow.com/questions/36963319/spark-rdd-sort-by-two-values)\n",
    "* [takeOrdered descending Pyspark](https://stackoverflow.com/questions/30787635/takeordered-descending-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sc.parallelize([['panda',(3,4)],['dog',(2,7)],['dog',(3,6)],['panda',(2,2)],['dog',(1,2)]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['panda', (3, 4)],\n",
       " ['dog', (2, 7)],\n",
       " ['dog', (3, 6)],\n",
       " ['panda', (2, 2)],\n",
       " ['dog', (1, 2)]]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog', (1, 2)],\n",
       " ['dog', (2, 7)],\n",
       " ['dog', (3, 6)],\n",
       " ['panda', (2, 2)],\n",
       " ['panda', (3, 4)]]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ascending order\n",
    "temp.sortBy(lambda r: (r[0][0], int(r[1][0]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog', (3, 6)],\n",
       " ['dog', (2, 7)],\n",
       " ['dog', (1, 2)],\n",
       " ['panda', (3, 4)],\n",
       " ['panda', (2, 2)]]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descending order of the second value\n",
    "temp.sortBy(lambda r: (r[0][0], -int(r[1][0]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **reduce (action)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reduce function, one is current element and another is running sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Calculate average/mean for each key**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/40087483/spark-average-of-values-instead-of-sum-in-reducebykey-using-scala?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sc.parallelize([['panda',10],['panda',8],['dog',9],['cat',5],['dog',12]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp2 = temp.map(lambda x: (x[0],(x[1],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (10, 1)),\n",
       " ('panda', (8, 1)),\n",
       " ('dog', (9, 1)),\n",
       " ('cat', (5, 1)),\n",
       " ('dog', (12, 1))]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x: current value, in our case, it's (1,1) or (2,1)...etc\n",
    "* y: running sum. \n",
    "\n",
    "* x[0]: indicate the current value using to calculate the sum\n",
    "* x[1]: indicate the current value using to calculate the total count\n",
    "\n",
    "Therefore, in the key level for panda\n",
    "* In the first iteration\n",
    "    * x[0] = 10, x[1] = 1, y[0] = 0, y[1] = 0\n",
    "* In the second iteration\n",
    "    * x[0] = 8, x[1] = 1, y[0] = 10, y[1] = 1\n",
    "* No more records\n",
    "    * y[0] = 18, y[1] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp3 = temp2.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (18, 2)), ('cat', (5, 1)), ('dog', (21, 2))]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp4 = temp3.map(lambda r: (r[0], r[1][0]/r[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', 9.0), ('cat', 5.0), ('dog', 10.5)]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', 9.0), ('cat', 5.0), ('dog', 10.5)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in one line\n",
    "temp.map(lambda x: (x[0],(x[1],1))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).map(lambda r: (r[0], r[1][0]/r[1][1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **groupByKey, mapValues: append values into a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sc.parallelize([['panda',10],['panda',8],['dog',12],['cat',5],['dog',1]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['panda', 10], ['panda', 8], ['dog', 12], ['cat', 5], ['dog', 1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', [10, 8]), ('cat', [5]), ('dog', [12, 1])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', [5]), ('panda', [8, 10]), ('dog', [1, 12])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.sortBy(lambda x: x[1]).groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', [10, 8]), ('cat', [5]), ('dog', [12, 1])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use may and reduceByKey. It looks more messy \n",
    "# https://stackoverflow.com/questions/27002161/reduce-a-key-value-pair-into-a-key-list-pair-with-apache-spark\n",
    "temp.map(lambda x: (x[0], [x[1]])).reduceByKey(lambda p,q: p+q).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **produce RDD[(X, X)] of all possible combinations from RDD[X], cartesian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/26557873/spark-produce-rddx-x-of-all-possible-combinations-from-rddx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['panda', 10], ['lion', 8], ['dog', 12], ['cat', 5], ['bird', 1]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sc.parallelize([['panda',10],['lion',8],['dog',12],['cat',5],['bird',1]], 2)\n",
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['panda', 10], ['lion', 8]),\n",
       " (['lion', 8], ['panda', 10]),\n",
       " (['panda', 10], ['dog', 12]),\n",
       " (['panda', 10], ['cat', 5]),\n",
       " (['lion', 8], ['dog', 12]),\n",
       " (['lion', 8], ['cat', 5]),\n",
       " (['panda', 10], ['bird', 1]),\n",
       " (['lion', 8], ['bird', 1]),\n",
       " (['dog', 12], ['panda', 10]),\n",
       " (['dog', 12], ['lion', 8]),\n",
       " (['cat', 5], ['panda', 10]),\n",
       " (['cat', 5], ['lion', 8]),\n",
       " (['bird', 1], ['panda', 10]),\n",
       " (['bird', 1], ['lion', 8]),\n",
       " (['dog', 12], ['cat', 5]),\n",
       " (['cat', 5], ['dog', 12]),\n",
       " (['dog', 12], ['bird', 1]),\n",
       " (['cat', 5], ['bird', 1]),\n",
       " (['bird', 1], ['dog', 12]),\n",
       " (['bird', 1], ['cat', 5])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it will still have duplicated rows. Figure it out later\n",
    "temp.cartesian(temp).filter(lambda x: x[0]!=x[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1 = sc.parallelize([['panda',10],['lion',8],['dog',12],['cat',5],['bird',1]], 2)\n",
    "temp2 = sc.parallelize([['panda',10],['panda',8],['dog',12],['cat',5],['dog',1]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (10, 10)),\n",
       " ('panda', (10, 8)),\n",
       " ('cat', (5, 5)),\n",
       " ('dog', (12, 12)),\n",
       " ('dog', (12, 1))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.join(temp2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp3 = sc.parallelize([['panda',(10,'black')],['lion',(8,'yellow')],['dog',(12,'white')],['cat',(5,'grey')],['bird',(1,'green')]], 2)\n",
    "temp4 = sc.parallelize([['panda',10],['dog',20],['horse',20]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (10, (10, 'black'))), ('dog', (20, (12, 'white')))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner join\n",
    "temp3.join(temp4).collect()\n",
    "temp4.join(temp3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', ((10, 'black'), 10)),\n",
       " ('lion', ((8, 'yellow'), None)),\n",
       " ('cat', ((5, 'grey'), None)),\n",
       " ('bird', ((1, 'green'), None)),\n",
       " ('dog', ((12, 'white'), 20))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# left join\n",
    "temp3.leftOuterJoin(temp4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', ((10, 'black'), 10)),\n",
       " ('horse', (None, 20)),\n",
       " ('dog', ((12, 'white'), 20))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right join\n",
    "temp3.rightOuterJoin(temp4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', ((10, 'black'), 10)),\n",
       " ('lion', ((8, 'yellow'), None)),\n",
       " ('cat', ((5, 'grey'), None)),\n",
       " ('bird', ((1, 'green'), None)),\n",
       " ('horse', (None, 20)),\n",
       " ('dog', ((12, 'white'), 20))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right join\n",
    "temp3.fullOuterJoin(temp4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='df'>Spark DataFrame API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read csv file into Spark DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/Crimes_-_2001_to_present.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[databricks/spark-csv](https://github.com/databricks/spark-csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801844"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the actual #rows is \n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create DataFrame from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "scratch_df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scratch_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read Json into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customer = sqlContext.read.json(\"data/customer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(address=Row(city='New Orleans', state='LA', street='6649 N Blue Gum St', zip='70116'), first_name='James', last_name='Butterburg'),\n",
       " Row(address=Row(city='Brighton', state='MI', street='4 B Blue Ridge Blvd', zip='48116'), first_name='Josephine', last_name='Darakjy'),\n",
       " Row(address=Row(city='Bridgeport', state='NJ', street='8 W Cerritos Ave #54', zip='08014'), first_name='Art', last_name='Chemel')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **show records, head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|                Date|              Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|    010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...|067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|    026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|\n",
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)'),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **take the first 1000 rows of a Spark Dataframe and return a new dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stackoverflow](https://stackoverflow.com/questions/34206508/is-there-a-way-to-take-the-first-1000-rows-of-a-spark-dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.limit(10).alias(\"test\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get Column Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: boolean, Domestic: boolean, Beat: int, District: int, Ward: int, Community Area: string, FBI Code: string, X Coordinate: int, Y Coordinate: int, Year: int, Updated On: string, Latitude: double, Longitude: double, Location: string]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'int'),\n",
       " ('Case Number', 'string'),\n",
       " ('Date', 'string'),\n",
       " ('Block', 'string'),\n",
       " ('IUCR', 'string'),\n",
       " ('Primary Type', 'string'),\n",
       " ('Description', 'string'),\n",
       " ('Location Description', 'string'),\n",
       " ('Arrest', 'boolean'),\n",
       " ('Domestic', 'boolean'),\n",
       " ('Beat', 'int'),\n",
       " ('District', 'int'),\n",
       " ('Ward', 'int'),\n",
       " ('Community Area', 'string'),\n",
       " ('FBI Code', 'string'),\n",
       " ('X Coordinate', 'int'),\n",
       " ('Y Coordinate', 'int'),\n",
       " ('Year', 'int'),\n",
       " ('Updated On', 'string'),\n",
       " ('Latitude', 'double'),\n",
       " ('Longitude', 'double'),\n",
       " ('Location', 'string')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the shape of dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801844"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Describe the dataset; Get summary statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|summary|                ID|       Case Number|                Date|               Block|             IUCR|     Primary Type|    Description|Location Description|              Beat|          District|              Ward|    Community Area|          FBI Code|      X Coordinate|      Y Coordinate|              Year|          Updated On|           Latitude|          Longitude|            Location|\n",
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|  count|           5801844|           5801841|             5801844|             5801842|          5801844|          5801844|        5801844|             5801009|           5801844|           4601628|           5186940|           5186069|           5801844|           5753924|           5753924|           5801844|             5801678|            5753924|            5753924|             5753924|\n",
      "|   mean| 5478809.816915622|       302402.4375|                null|                null|1134.731230109342|             null|           null|                null|1199.6529617135518|11.304880142419162|22.585335284387327|37.721575177442126|12.340449301040367| 1164505.101389417|1885629.2320944802|2006.9770088613207|                null|  41.84176525667321| -87.67185536981503|                null|\n",
      "| stddev|2587006.9227430555|136858.87616128215|                null|                null|807.5054229686023|             null|           null|                null| 704.5764004952774|6.9362124463080885|13.794330067588891|21.550102471312908|  7.41121340178233|16200.899281313013| 31439.98034154956|4.0104357430230335|                null|0.08646691031758802|0.05895851757394647|                null|\n",
      "|    min|               634|         01G050460|01/01/2001 01:00:...|  0000X  I94/EXIT 12|             0110|            ARSON| $300 AND UNDER|\"CTA \"\"L\"\" PLATFORM\"|               111|                 1|                 1|                  |               01A|           1092697|           1813930|              2001|01/01/2000 09:32:...|       41.644580105|      -87.934324986|(41.644580105, -8...|\n",
      "|    max|          10086206|         ZZZ199957|12/31/2014 12:59:...|175XX W WINSTON C...|             9901|WEAPONS VIOLATION|WIREROOM/SPORTS|                YMCA|              2535|                31|                50|                 9|                26|           1205152|           1951664|              2015|12/31/2014 12:39:...|       42.023024908|      -87.524388789|(42.023024908, -8...|\n",
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Select Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|      ID|                Date|\n",
      "+--------+--------------------+\n",
      "|10078659|05/19/2015 11:57:...|\n",
      "|10078598|05/19/2015 11:50:...|\n",
      "|10078625|05/19/2015 11:47:...|\n",
      "|10078662|05/19/2015 11:46:...|\n",
      "|10078584|05/19/2015 11:45:...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ID','Date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Rename column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('oldName', 'newName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get unique value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|District|\n",
      "+--------+\n",
      "|      31|\n",
      "|      12|\n",
      "|      22|\n",
      "|    null|\n",
      "|       1|\n",
      "|      13|\n",
      "|       6|\n",
      "|      16|\n",
      "|       3|\n",
      "|      20|\n",
      "|       5|\n",
      "|      19|\n",
      "|      15|\n",
      "|       9|\n",
      "|      17|\n",
      "|       4|\n",
      "|       8|\n",
      "|      23|\n",
      "|       7|\n",
      "|      10|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('District').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('District').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Explode array data into rows in spark; Dividing complex rows of dataframe to simple rows in Pyspark; Explode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default into of explode method should be a list [1,2,3]. If the value in the row is not in list format, we need to specify how do we want to split the value. For exmaple, if the format is a string \"1,2,3\". We need to write it as `explode(split(df.col3, \",\"))`\n",
    "\n",
    "* **First way**: use **withColumn**+**explode**\n",
    "    * Note: if we want to add a new column, use this.\n",
    "\n",
    "[StackOverFlow: Explode array data into rows in spark](https://stackoverflow.com/questions/44436856/explode-array-data-into-rows-in-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second way**: use **select**+**explode**+**alias**\n",
    "    * select\n",
    "    * explode: to separate \n",
    "    * alias: to specify the name of new column\n",
    "    * Note: if we want to select and add a new column at the same time.\n",
    "\n",
    "[StackOverFlow: Explode in PySpark](https://stackoverflow.com/questions/38210507/explode-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "explode_df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   A|   1|\n",
      "|   1|   A|   2|\n",
      "|   1|   A|   3|\n",
      "|   2|   B|   3|\n",
      "|   2|   B|   5|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+---------+-------+\n",
      "|col1|col2|     col3|newcol3|\n",
      "+----+----+---------+-------+\n",
      "|   1|   A|[1, 2, 3]|      1|\n",
      "|   1|   A|[1, 2, 3]|      2|\n",
      "|   1|   A|[1, 2, 3]|      3|\n",
      "|   2|   B|   [3, 5]|      3|\n",
      "|   2|   B|   [3, 5]|      5|\n",
      "+----+----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cover out the original col3\n",
    "explode_df.withColumn(\"col3\", explode(explode_df.col3)).show()\n",
    "# create a new column\n",
    "explode_df.withColumn(\"newcol3\", explode(explode_df.col3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|first_name|       city|\n",
      "+----------+-----------+\n",
      "|     James|New Orleans|\n",
      "| Josephine|   Brighton|\n",
      "|       Art| Bridgeport|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.select(\"first_name\",\n",
    "                explode(\"address\").alias(\"contactInfo\")\n",
    "                \"address.city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n",
      "|col1|col2|newcol3|\n",
      "+----+----+-------+\n",
      "|   1|   A|      1|\n",
      "|   1|   A|      2|\n",
      "|   1|   A|      3|\n",
      "|   2|   B|      3|\n",
      "|   2|   B|      5|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df.select(\"col1\",\n",
    "                  \"col2\",\n",
    "                  explode(explode_df.col3).alias(\"newcol3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of non-list format column value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|col1|col2| col3|\n",
      "+----+----+-----+\n",
      "|   1|   A|1,2,3|\n",
      "|   2|   B|  3,5|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df2 = sqlContext.createDataFrame([(1, \"A\", \"1,2,3\"), (2, \"B\", \"3,5\")],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-------+\n",
      "|col1|col2| col3|newcol3|\n",
      "+----+----+-----+-------+\n",
      "|   1|   A|1,2,3|      1|\n",
      "|   1|   A|1,2,3|      2|\n",
      "|   1|   A|1,2,3|      3|\n",
      "|   2|   B|  3,5|      3|\n",
      "|   2|   B|  3,5|      5|\n",
      "+----+----+-----+-------+\n",
      "\n",
      "+----+----+-------+\n",
      "|col1|col2|newcol3|\n",
      "+----+----+-------+\n",
      "|   1|   A|      1|\n",
      "|   1|   A|      2|\n",
      "|   1|   A|      3|\n",
      "|   2|   B|      3|\n",
      "|   2|   B|      5|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df2.withColumn(\"newcol3\", explode(split(explode_df2.col3, \",\"))).show()\n",
    "\n",
    "explode_df2.select(\"col1\",\n",
    "                   \"col2\",\n",
    "                   explode(split(explode_df2.col3, \",\")).alias(\"newcol3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get unique list for each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------------+\n",
      "|col1|col2|           col3|\n",
      "+----+----+---------------+\n",
      "|   1|   A|[1, 1, 1, 2, 3]|\n",
      "|   2|   B|      [3, 5, 5]|\n",
      "|   3|   C|             []|\n",
      "+----+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "explode_df = sqlContext.createDataFrame([(1, \"A\", [1,1,1,2,3]), (2, \"B\", [3,5,5]), (3, \"C\", [])],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_list(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------------+---------+\n",
      "|col1|col2|           col3|     col4|\n",
      "+----+----+---------------+---------+\n",
      "|   1|   A|[1, 1, 1, 2, 3]|[1, 2, 3]|\n",
      "|   2|   B|      [3, 5, 5]|   [3, 5]|\n",
      "|   3|   C|             []|       []|\n",
      "+----+----+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf1 = udf(get_unique_list, ArrayType(StringType()))\n",
    "explode_df.withColumn('col4',udf1('col3')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Convert pyspark string to date format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow: Convert pyspark string to date format](https://stackoverflow.com/questions/38080748/convert-pyspark-string-to-date-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for spark version > 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|      ID|                 dt|\n",
      "+--------+-------------------+\n",
      "|10078659|2015-05-19 11:57:00|\n",
      "|10078598|2015-05-19 11:50:00|\n",
      "|10078625|2015-05-19 11:47:00|\n",
      "|10078662|2015-05-19 11:46:00|\n",
      "|10078584|2015-05-19 11:45:00|\n",
      "|10078629|2015-05-19 11:40:00|\n",
      "|10079225|2015-05-19 11:30:00|\n",
      "|10078594|2015-05-19 11:30:00|\n",
      "|10080768|2015-05-19 11:30:00|\n",
      "|10078618|2015-05-19 11:30:00|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"ID\",\n",
    "           to_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss').alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.withColumn(\"datetime\",\n",
    "           to_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|           datetime|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|2015-05-19 11:57:00|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|2015-05-19 11:50:00|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|2015-05-19 11:47:00|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|2015-05-19 11:46:00|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|2015-05-19 11:45:00|\n",
      "|10078629|   HY267393|05/19/2015 11:40:...|013XX S LAWNDALE AVE|0454|             BATTERY|AGG PO HANDS NO/M...|              STREET|  true|   false|1011|      10|  24|            29|     08B|     1151957|     1893696|2015|05/26/2015 12:42:...|41.864172884|-87.717647622|(41.864172884, -8...|2015-05-19 11:40:00|\n",
      "|10079225|   HY267395|05/19/2015 11:30:...|   064XX S LAFLIN ST|0497|             BATTERY|AGGRAVATED DOMEST...|           APARTMENT| false|    true| 725|       7|  17|            67|     04B|     1167397|     1862229|2015|05/26/2015 12:42:...|41.777506284|-87.661870632|(41.777506284, -8...|2015-05-19 11:30:00|\n",
      "|10078594|   HY267388|05/19/2015 11:30:...|   021XX W NORTH AVE|1305|     CRIMINAL DAMAGE| CRIMINAL DEFACEMENT|            SIDEWALK|  true|   false|1434|      14|  32|            24|      14|     1162022|     1910669|2015|05/26/2015 12:42:...|  41.9105442|-87.680225036|(41.9105442, -87....|2015-05-19 11:30:00|\n",
      "|10080768|   HY269123|05/19/2015 11:30:...| 008XX N KILDARE AVE|0820|               THEFT|      $500 AND UNDER|           RESIDENCE| false|   false|1111|      11|  37|            23|      06|     1147592|     1905491|2015|05/26/2015 12:42:...|41.896624505|-87.733368852|(41.896624505, -8...|2015-05-19 11:30:00|\n",
      "|10078618|   HY267392|05/19/2015 11:30:...|062XX S KOMENSKY AVE|0320|             ROBBERY|STRONGARM - NO WE...|            SIDEWALK| false|   false| 813|       8|  13|            65|      03|     1150396|     1863112|2015|05/26/2015 12:42:...|41.780276746|-87.724174049|(41.780276746, -8...|2015-05-19 11:30:00|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Spark version < 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|      ID|                 dt|\n",
      "+--------+-------------------+\n",
      "|10078659|2015-05-19 11:57:00|\n",
      "|10078598|2015-05-19 11:50:00|\n",
      "|10078625|2015-05-19 11:47:00|\n",
      "|10078662|2015-05-19 11:46:00|\n",
      "|10078584|2015-05-19 11:45:00|\n",
      "|10078629|2015-05-19 11:40:00|\n",
      "|10079225|2015-05-19 11:30:00|\n",
      "|10078594|2015-05-19 11:30:00|\n",
      "|10080768|2015-05-19 11:30:00|\n",
      "|10078618|2015-05-19 11:30:00|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"ID\",\n",
    "           from_unixtime(unix_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss')).alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)', datetime='2015-05-19 11:57:00'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)', datetime='2015-05-19 11:50:00'),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)', datetime='2015-05-19 11:47:00')]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.withColumn(\"datetime\",\n",
    "           from_unixtime(unix_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss'))).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Spark DataFrame TimestampType - how to get Year, Month, Day values from field?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [StackOverFlow: year/month/dayofmonth](https://stackoverflow.com/questions/30949202/spark-dataframe-timestamptype-how-to-get-year-month-day-values-from-field)\n",
    "* [StackOverFlow: weekday](https://stackoverflow.com/questions/38928919/how-to-get-the-weekday-from-day-of-month-using-pyspark/38931967)\n",
    "* [pyspark sql function](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get year/month/day of month\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, to_date\n",
    "df4 = df3.withColumn(\"year\", year(df3.datetime))\n",
    "df4 = df4.withColumn(\"month\", month(df4.datetime))\n",
    "df4 = df4.withColumn(\"dayofmonth\", dayofmonth(df4.datetime))\n",
    "df4 = df4.withColumn(\"hour\", hour(df4.datetime))\n",
    "df4 = df4.withColumn(\"minute\", minute(df4.datetime))                                        \n",
    "df4 = df4.withColumn(\"todate\", to_date(df4.datetime))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)', datetime=datetime.datetime(2015, 5, 19, 11, 57), month=5, dayofmonth=19, hour=11, minute=57, todate=datetime.date(2015, 5, 19)),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)', datetime=datetime.datetime(2015, 5, 19, 11, 50), month=5, dayofmonth=19, hour=11, minute=50, todate=datetime.date(2015, 5, 19)),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)', datetime=datetime.datetime(2015, 5, 19, 11, 47), month=5, dayofmonth=19, hour=11, minute=47, todate=datetime.date(2015, 5, 19))]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+\n",
      "|           datetime|dow_number|dow_string|\n",
      "+-------------------+----------+----------+\n",
      "|2015-05-19 11:57:00|         2|       Tue|\n",
      "|2015-05-19 11:50:00|         2|       Tue|\n",
      "|2015-05-19 11:47:00|         2|       Tue|\n",
      "|2015-05-19 11:46:00|         2|       Tue|\n",
      "|2015-05-19 11:45:00|         2|       Tue|\n",
      "|2015-05-19 11:40:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "+-------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get weekday string /number\n",
    "from pyspark.sql.functions import date_format\n",
    "df4.select('datetime', date_format('datetime', 'u').alias('dow_number'), date_format('datetime', 'E').alias('dow_string')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|year|          Updated On|    Latitude|    Longitude|            Location|           datetime|month|dayofmonth|dow_string|dow_number|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|2015-05-19 11:57:00|    5|        19|       Tue|         2|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|2015-05-19 11:50:00|    5|        19|       Tue|         2|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|2015-05-19 11:47:00|    5|        19|       Tue|         2|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|2015-05-19 11:46:00|    5|        19|       Tue|         2|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|2015-05-19 11:45:00|    5|        19|       Tue|         2|\n",
      "|10078629|   HY267393|05/19/2015 11:40:...|013XX S LAWNDALE AVE|0454|             BATTERY|AGG PO HANDS NO/M...|              STREET|  true|   false|1011|      10|  24|            29|     08B|     1151957|     1893696|2015|05/26/2015 12:42:...|41.864172884|-87.717647622|(41.864172884, -8...|2015-05-19 11:40:00|    5|        19|       Tue|         2|\n",
      "|10079225|   HY267395|05/19/2015 11:30:...|   064XX S LAFLIN ST|0497|             BATTERY|AGGRAVATED DOMEST...|           APARTMENT| false|    true| 725|       7|  17|            67|     04B|     1167397|     1862229|2015|05/26/2015 12:42:...|41.777506284|-87.661870632|(41.777506284, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10078594|   HY267388|05/19/2015 11:30:...|   021XX W NORTH AVE|1305|     CRIMINAL DAMAGE| CRIMINAL DEFACEMENT|            SIDEWALK|  true|   false|1434|      14|  32|            24|      14|     1162022|     1910669|2015|05/26/2015 12:42:...|  41.9105442|-87.680225036|(41.9105442, -87....|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10080768|   HY269123|05/19/2015 11:30:...| 008XX N KILDARE AVE|0820|               THEFT|      $500 AND UNDER|           RESIDENCE| false|   false|1111|      11|  37|            23|      06|     1147592|     1905491|2015|05/26/2015 12:42:...|41.896624505|-87.733368852|(41.896624505, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10078618|   HY267392|05/19/2015 11:30:...|062XX S KOMENSKY AVE|0320|             ROBBERY|STRONGARM - NO WE...|            SIDEWALK| false|   false| 813|       8|  13|            65|      03|     1150396|     1863112|2015|05/26/2015 12:42:...|41.780276746|-87.724174049|(41.780276746, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"dow_string\", date_format(df4.datetime, 'E')).withColumn(\"dow_number\", date_format(df4.datetime, 'u')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create new column and name it; groupby agg pyspark rename**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# countDistnct can be any function from pyspark.sql\n",
    "df_indegree = df_sample.groupby('user2').agg(countDistinct('user1').alias(\"in_degree\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **create new column using max pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/37838361/withcolumn-not-allowing-me-to-use-max-function-to-generate-a-new-column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import greatest\n",
    "\n",
    "a.withColumn(\"max_col\", greatest(a[\"one\"], a[\"two\"], a[\"three\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Convert 12 hours to 24 hours; Convert 12 hour into 24 hour times**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://codereview.stackexchange.com/questions/111596/12-hours-to-24-hours-conversion-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_24(time):\n",
    "    \"\"\"str -> str\n",
    "    converts 12 hours time format to 24 hours\n",
    "    \"\"\"\n",
    "    if time[-2:]==\"PM\":\n",
    "        if int(time[11:13]) == 12:\n",
    "            final_time = time[:-3]\n",
    "        else:    \n",
    "            final_time = time[:11] + str(int(time[11:13]) + 12) + time[13:19]\n",
    "    elif time[-2:]==\"AM\":\n",
    "        if int(time[11:13]) == 12:\n",
    "            final_time = time[:11] + str(\"00\") + time[13:19]\n",
    "        else:\n",
    "            final_time = time[:-3]\n",
    "    else:    \n",
    "        final_time = time\n",
    "    return final_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf1 = udf(convert_to_24,StringType())\n",
    "df = df.withColumn('myDate',udf1('Date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Define output type for user defined function(UDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [UDF](http://changhsinlee.com/pyspark-udf/)\n",
    "* [UDF Return types](https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/sql/types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    \"DataType\", \"NullType\", \"StringType\", \"BinaryType\", \"BooleanType\", \"DateType\",\n",
    "    \"TimestampType\", \"DecimalType\", \"DoubleType\", \"FloatType\", \"ByteType\", \"IntegerType\",\n",
    "    \"LongType\", \"ShortType\", \"ArrayType\", \"MapType\", \"StructField\", \"StructType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# udf output as list\n",
    "\n",
    "# emoji related link\n",
    "# https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text\n",
    "# http://worthavisit.blogspot.com/2014/07/string-punctuation-in-python.html\n",
    "# https://stackoverflow.com/questions/43727583/expected-string-or-bytes-like-object\n",
    "def emoji_extract(des):\n",
    "    return re.findall(r'[^\\w\\s!,]', str(des))\n",
    "\n",
    "udf1 = udf(emoji_extract, ArrayType(StringType()))\n",
    "df2 = df_sample.withColumn('emoji',udf1('description'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **calculate pair wise frequency of categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|Beat_District| 10| 11| 14| 24|  3|  4|  6|  7|  8|  9|\n",
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|         2432|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|          421|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
      "|         1111|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|          725|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
      "|         1434|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|\n",
      "|         1011|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|          314|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|          813|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
      "|          624|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|          935|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.crosstab('Beat', 'District')\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Drop rows with NA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Change Column Type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[change a Dataframe column from String type to Double type in pyspark](https://stackoverflow.com/questions/32284620/how-to-change-a-dataframe-column-from-string-type-to-double-type-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Beat_District: string, 10: bigint, 11: bigint, 14: bigint, 24: bigint, 3: bigint, 4: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Beat_District: double, 10: bigint, 11: bigint, 14: bigint, 24: bigint, 3: bigint, 4: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.withColumn(\"Beat_District\", df5[\"Beat_District\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **GroupBy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax is similar to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.groupby('Age').max(\"height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Count Distinct**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/36064777/get-the-distinct-elements-of-each-group-by-other-field-on-a-spark-1-6-dataframe/36137905)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_outdegree = df.groupby('user1').agg(countDistinct('user2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort using single column\n",
    "df.sort(df.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **order by: sorting using multiple column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.orderBy([\"age\",\"city\"],ascending=[0,1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the NULL/NaN percentage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/33900726/count-number-of-non-nan-entries-in-each-column-of-spark-dataframe-with-pyspark?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, lit, sum\n",
    "\n",
    "def count_not_null(c, nan_as_null=False):\n",
    "    \"\"\"Use conversion between boolean and integer\n",
    "    - False -> 0\n",
    "    - True ->  1\n",
    "    \"\"\"\n",
    "    pred = col(c).isNotNull() & (~isnan(c) if nan_as_null else lit(True))\n",
    "    return sum(pred.cast(\"integer\")).alias(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get number of not null values in each column\n",
    "df.agg(*[count_not_null(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get percentage of not null values in each column\n",
    "exprs = [(count_not_null(c) / count(\"*\")).alias(c) for c in df.columns]\n",
    "df.agg(*exprs).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filter NULL rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.where(col(\"month\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count number of rows with NULL\n",
    "df.where(col(\"month\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get only not null rows\n",
    "df.where(col(\"month\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Fill na values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/42312042/how-to-replace-all-null-values-of-a-dataframe-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df['age']>24).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use == for equal\n",
    "df.filter(df['age']==24).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **filter by length of a column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/33695389/filtering-dataframe-using-the-length-of-a-column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   tokens|\n",
      "+---------+\n",
      "|[S, U, S]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    ([\"L\", \"S\", \"Y\", \"S\"],  ),\n",
    "    ([\"L\", \"V\", \"I\", \"S\"],  ),\n",
    "    ([\"I\", \"A\", \"N\", \"A\"],  ),\n",
    "    ([\"I\", \"L\", \"S\", \"A\"],  ),\n",
    "    ([\"E\", \"N\", \"N\", \"Y\"],  ),\n",
    "    ([\"E\", \"I\", \"M\", \"A\"],  ),\n",
    "    ([\"O\", \"A\", \"N\", \"A\"],  ),\n",
    "    ([\"S\", \"U\", \"S\"],  )], \n",
    "    (\"tokens\", ))\n",
    "\n",
    "df.where(size(col(\"tokens\")) <= 3).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Replace values in a certain column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace AM, PM with space in Date column\n",
    "df.withColumn('Date', regexp_replace('Date', ' PM', '')).withColumn('Date', regexp_replace('Date', ' AM', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create lag/shift column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/34295642/spark-add-new-column-to-dataframe-with-value-from-previous-row)\n",
    "\n",
    "[StackOverFlow](https://stackoverflow.com/questions/37754704/how-to-implement-lead-and-lag-in-spark-scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when using window function, for some reason we need to use HiveContext instead of SQLContext on wolf\n",
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "| id|num|new_col|\n",
      "+---+---+-------+\n",
      "|  1|5.0|   null|\n",
      "|  2|3.0|    5.0|\n",
      "|  3|7.0|    3.0|\n",
      "|  4|9.0|    7.0|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, col, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = sc.parallelize([(4, 9.0), (3, 7.0), (2, 3.0), (1, 5.0)]).toDF([\"id\", \"num\"])\n",
    "w = Window().partitionBy().orderBy(col(\"id\"))\n",
    "df.select(\"*\", lag(\"num\").over(w).alias(\"new_col\")).show() #.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "| id|num|new_col|\n",
      "+---+---+-------+\n",
      "|  1|5.0|    3.0|\n",
      "|  2|3.0|    7.0|\n",
      "|  3|7.0|    9.0|\n",
      "|  4|9.0|   null|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\", lead(\"num\").over(w).alias(\"new_col\")).show() #.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **dataframe join**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**how** â€“ str, default â€˜innerâ€™. One of **inner**, **outer**, **left_outer**, **right_outer**, **leftsemi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.join(df2, 'name', 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://forums.databricks.com/questions/876/is-there-a-better-method-to-join-two-dataframes-an.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join two dataframes and not have a duplicated column?\n",
    "df1.join(df2, df1.district == df2.DISTRICT, 'left_outer').drop(df2.DISTRICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will make sure that we don't have duplicated output key at least for pyspark version 1.4\n",
    "df.join(df4, ['name', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataframe to RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_rdd = df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **keep the row with max value for a column and keep all columns (for max records per group)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to max value and keep all columns (for max records per group)](https://stackoverflow.com/questions/42636179/how-to-max-value-and-keep-all-columns-for-max-records-per-group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_max = df_all.groupBy(\"beat\").max(\"district\")\n",
    "df_new = df_max.join(df_all, [\"beat\"])\n",
    "df_new = df_new.filter(df_new['max(district)']==df_new['district']).select('beat','zip','district')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Delete Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(df.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiple columns\n",
    "df = df.drop(\"address\", \"phoneNumber\")\n",
    "df = df.drop(df.address).drop(df.phoneNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **change column type to double**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/32284620/how-to-change-a-dataframe-column-from-string-type-to-double-type-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDf = df.withColumn(\"colName\", df[\"oldName\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDf = df.withColumn(\"colName\", df[\"oldName\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Substring, get part of the string from original column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf #pyspark df column manipulation\n",
    "\n",
    "udf1 = udf(lambda x:x[0:2],StringType())\n",
    "df = df.withColumn('month',udf1('Date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **random sample dataframe; subsample datarame by row**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pyspark sql doc](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample(withReplacement, fraction, seed=None)\n",
    "df.sample(False, 0.5, 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Transform pyspark column into numpy array**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://www.quora.com/How-do-I-convert-image-data-from-2D-array-to-1D-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q1_indegree = np.array(df4.select('in_degree').collect())\n",
    "\n",
    "# reshape 2D numpy array into 1D array\n",
    "q1_indegree = q1_indegree.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **groupby keep top 5, top n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/38397796/retrieve-top-n-in-each-group-of-a-dataframe-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"user_1\",  \"object_1\",  3), \n",
    "                      (\"user_1\",  \"object_2\",  2), \n",
    "                      (\"user_2\",  \"object_1\",  5), \n",
    "                      (\"user_2\",  \"object_2\",  2), \n",
    "                      (\"user_2\",  \"object_2\",  6)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user_id\", \"object_id\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|user_id|object_id|score|\n",
      "+-------+---------+-----+\n",
      "| user_1| object_1|    3|\n",
      "| user_1| object_2|    2|\n",
      "| user_2| object_1|    5|\n",
      "| user_2| object_2|    2|\n",
      "| user_2| object_2|    6|\n",
      "+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+----+\n",
      "|user_id|object_id|score|rank|\n",
      "+-------+---------+-----+----+\n",
      "| user_1| object_1|    3|   1|\n",
      "| user_1| object_2|    2|   2|\n",
      "| user_2| object_2|    6|   1|\n",
      "| user_2| object_1|    5|   2|\n",
      "+-------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "window = Window.partitionBy(df['user_id']).orderBy(df['score'].desc())\n",
    "\n",
    "df.select('*', rank().over(window).alias('rank')) .filter(col('rank') <= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **groupby join as list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|Name|         Age|\n",
      "+----+------------+\n",
      "|Mark|    [31, 32]|\n",
      "|John|[41, 42, 43]|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = sqlContext.createDataFrame([(31, \"Mark\"), (32, \"Mark\"), (41, \"John\"), (42, \"John\"), (43, \"John\")],[ 'Age', 'Name'])\n",
    "\n",
    "df_grouped = df.groupBy(\"Name\").agg(F.collect_list(F.col(\"Age\")).alias(\"Age\"))\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Union and intersect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"A\",  \"B\",  1), \n",
    "                      (\"A\",  \"C\",  1), \n",
    "                      (\"D\",  \"C\",  1), \n",
    "                      (\"B\",  \"A\",  2)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user_1\", \"user_2\", \"period\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|user_1|user_2|\n",
      "+------+------+\n",
      "|     A|     B|\n",
      "|     A|     C|\n",
      "|     D|     C|\n",
      "|     B|     A|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_1','user_2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|user_1|user_2|\n",
      "+------+------+\n",
      "|     D|     C|\n",
      "|     A|     C|\n",
      "|     C|     A|\n",
      "|     C|     D|\n",
      "|     B|     A|\n",
      "|     A|     B|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_1','user_2').union(df.select('user_2','user_1')).drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|user_1|user_2|\n",
      "+------+------+\n",
      "|     B|     A|\n",
      "|     A|     B|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_1','user_2').intersect(df.select('user_2','user_1')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Calculate reciprocal of a network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"A\",  \"B\",  1), \n",
    "                      (\"A\",  \"B\",  2),                       \n",
    "                      (\"A\",  \"C\",  1), \n",
    "                      (\"D\",  \"C\",  1), \n",
    "                      (\"B\",  \"A\",  2)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user1\", \"user2\", \"period\"])\n",
    "# answer \n",
    "# period 1: 0/3\n",
    "# period 2: 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|user1|user2|period|\n",
      "+-----+-----+------+\n",
      "|    A|    B|     1|\n",
      "|    A|    B|     2|\n",
      "|    A|    C|     1|\n",
      "|    D|    C|     1|\n",
      "|    B|    A|     2|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reci_number = df.select('user1','user2').intersect(df.select('user2','user1')).count()/2\n",
    "reci_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reciprocal_ratio = reci_number/(df.drop_duplicates().count()-reci_number)\n",
    "reciprocal_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reci_list = []\n",
    "\n",
    "for i in range(1,3):\n",
    "    filter_df = df.filter(df['period']<=i)\n",
    "    \n",
    "    reci_number = filter_df.select('user1','user2').intersect(filter_df.select('user2','user1')).count()/2\n",
    "    reciprocal_ratio = reci_number/(df.count()-reci_number)\n",
    "    \n",
    "    reci_list.append(reciprocal_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.3333333333333333]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reci_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Drop Duplicated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"A\",  \"B\",  1), \n",
    "                      (\"A\",  \"B\",  1), \n",
    "                      (\"D\",  \"C\",  1), \n",
    "                      (\"B\",  \"A\",  1)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user_1\", \"user_2\", \"period\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|user_1|user_2|period|\n",
      "+------+------+------+\n",
      "|     A|     B|     1|\n",
      "|     A|     B|     1|\n",
      "|     D|     C|     1|\n",
      "|     B|     A|     1|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|user_1|user_2|period|\n",
      "+------+------+------+\n",
      "|     D|     C|     1|\n",
      "|     B|     A|     1|\n",
      "|     A|     B|     1|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **get len create new column pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/44541605/how-to-get-the-lists-length-in-one-column-in-dataframe-spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+--------+\n",
      "|col1|col2|     col3|col3_len|\n",
      "+----+----+---------+--------+\n",
      "|   1|   A|[1, 2, 3]|       3|\n",
      "|   2|   B|   [3, 5]|       2|\n",
      "+----+----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('col3_len',size('col3')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+------------+\n",
      "|col1|col2|     col3|col3_is_zero|\n",
      "+----+----+---------+------------+\n",
      "|   1|   A|[1, 2, 3]|       false|\n",
      "|   2|   B|   [3, 5]|       false|\n",
      "+----+----+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('col3_is_zero',size('col3')==0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get string length as new column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/36604951/add-new-column-string-length-to-df-by-userdefinedfunction-in-spark-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn('your_column_length', F.length(your_column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get column sum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/47812526/pyspark-sum-a-column-in-dataframe-and-return-results-as-int?rq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will return a list\n",
    "df.groupBy().sum().collect()\n",
    "\n",
    "# return a single number\n",
    "df.groupBy().sum().collect()[0][0]\n",
    "\n",
    "# get the sum from the second column\n",
    "df.groupBy().sum().collect()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/43968946/calculating-percentages-on-a-pyspark-dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create Ratio for categorical feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = df6.groupby().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df7 = df6.withColumn('ratio', (df6[\"count\"] / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **create new column using the sum multiple columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/31955309/add-column-sum-as-new-column-in-pyspark-dataframe?rq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf = df.withColumn('total', sum(df[col] for col in df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Save list as txt in pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_as_txt(mylist, filename):\n",
    "    \"\"\"save list as txt file\"\"\" \n",
    "\n",
    "    thefile = open(filename, 'w')\n",
    "    for item in mylist:\n",
    "        thefile.write(\"%s\\n\" % str(item))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **get percentage of categories in a single column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   A|\n",
      "|   2|   B|\n",
      "|   2|   B|\n",
      "|   2|   B|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame([(1, \"A\"), (2, \"B\"), (2, \"B\"), (2, \"B\")],[\"col1\", \"col2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|col1|count|             ratio|\n",
      "+----+-----+------------------+\n",
      "|   2|    3|               1.0|\n",
      "|   1|    1|0.3333333333333333|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get ratio for a numerical column\n",
    "df1 = df.groupby('col1').count()\n",
    "df1 = df1.sort(df1['count'].desc())\n",
    "df1 = df1.withColumn('ratio', (df1[\"count\"] / df1.groupby().sum().collect()[0][0]))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ratio for a categorical column\n",
    "df2 = df.groupby('col2').count()\n",
    "df2 = df2.sort(df2['count'].desc())\n",
    "df2 = df2.withColumn('ratio', (df2[\"count\"] / df2.groupby().sum().collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|col2|count|ratio|\n",
      "+----+-----+-----+\n",
      "|   B|    3| 0.75|\n",
      "|   A|    1| 0.25|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='sql'>Spark SQL</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sparkSQL work with dataframe api, we need to firstly transform our file/RDD into spark dataframe\n",
    "* RDD -> dataframe\n",
    "* file -> dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Transform RDD into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location',\n",
       " '10078659,HY267429,05/19/2015 11:57:00 PM,010XX E 79TH ST,143A,WEAPONS VIOLATION,UNLAWFUL POSS OF HANDGUN,STREET,true,false,0624,006,8,44,15,1184626,1852799,2015,05/26/2015 12:42:06 PM,41.751242944,-87.599004724,\"(41.751242944, -87.599004724)\"',\n",
       " '10078598,HY267408,05/19/2015 11:50:00 PM,067XX N SHERIDAN RD,3731,INTERFERENCE WITH PUBLIC OFFICER,OBSTRUCTING IDENTIFICATION,STREET,true,false,2432,024,49,1,24,1167071,1944859,2015,05/26/2015 12:42:06 PM,42.004255918,-87.660691083,\"(42.004255918, -87.660691083)\"',\n",
       " '10078625,HY267417,05/19/2015 11:47:00 PM,026XX E 77TH ST,2170,NARCOTICS,POSSESSION OF DRUG EQUIPMENT,STREET,true,false,0421,004,7,43,18,1195299,1854463,2015,05/26/2015 12:42:06 PM,41.755552462,-87.559839339,\"(41.755552462, -87.559839339)\"',\n",
       " '10078662,HY267423,05/19/2015 11:46:00 PM,015XX E 62ND ST,051A,ASSAULT,AGGRAVATED: HANDGUN,APARTMENT,false,true,0314,003,5,42,04A,1187377,1864316,2015,05/26/2015 12:42:06 PM,41.782781732,-87.588558362,\"(41.782781732, -87.588558362)\"']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD3 = myRDD2.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ID',\n",
       "  'Case Number',\n",
       "  'Date',\n",
       "  'Block',\n",
       "  'IUCR',\n",
       "  'Primary Type',\n",
       "  'Description',\n",
       "  'Location Description',\n",
       "  'Arrest',\n",
       "  'Domestic',\n",
       "  'Beat',\n",
       "  'District',\n",
       "  'Ward',\n",
       "  'Community Area',\n",
       "  'FBI Code',\n",
       "  'X Coordinate',\n",
       "  'Y Coordinate',\n",
       "  'Year',\n",
       "  'Updated On',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'Location'],\n",
       " ['10078659',\n",
       "  'HY267429',\n",
       "  '05/19/2015 11:57:00 PM',\n",
       "  '010XX E 79TH ST',\n",
       "  '143A',\n",
       "  'WEAPONS VIOLATION',\n",
       "  'UNLAWFUL POSS OF HANDGUN',\n",
       "  'STREET',\n",
       "  'true',\n",
       "  'false',\n",
       "  '0624',\n",
       "  '006',\n",
       "  '8',\n",
       "  '44',\n",
       "  '15',\n",
       "  '1184626',\n",
       "  '1852799',\n",
       "  '2015',\n",
       "  '05/26/2015 12:42:06 PM',\n",
       "  '41.751242944',\n",
       "  '-87.599004724',\n",
       "  '\"(41.751242944',\n",
       "  ' -87.599004724)\"']]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string, _16: string, _17: string, _18: string, _19: string, _20: string, _21: string, _22: string]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: string, Domestic: string, Beat: string, District: string, Ward: string, Community Area: string, FBI Code: string, X Coordinate: string, Y Coordinate: string, Year: string, Updated On: string, Latitude: string, Longitude: string, Location: string]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.toDF(['ID','Case Number','Date','Block','IUCR','Primary Type','Description','Location Description','Arrest','Domestic','Beat','District','Ward','Community Area','FBI Code','X Coordinate','Y Coordinate','Year','Updated On','Latitude','Longitude','Location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Write SQL code in SparkSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to write SQL code on SparkSQL\n",
    "df.registerTempTable(\"mydf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it will return a dataframe not a sql tempTable\n",
    "sqlContect.sql(\"select * from mydf\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ml'>Machine Learning in PySpark</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Basic Statistics - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-statistics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we want to use Mllib, we need to convert our data into a specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(['0,1,2,3','1,4,3,4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,1,2,3', '1,4,3,4']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(\",\")]\n",
    "    return Vectors.dense(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 1.0, 2.0, 3.0]), DenseVector([1.0, 4.0, 3.0, 4.0])]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use pretty much anything provided in mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  2.5  2.5  3.5]\n",
      "[ 0.5  4.5  0.5  0.5]\n",
      "[ 1.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "summary = Statistics.colStats(rdd2)\n",
    "print(summary.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary.variance())  # column-wise variance\n",
    "print(summary.numNonzeros())  # number of nonzeros in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using mllib RDD based API**\n",
    "\n",
    "[Spark2.2: Linear Methods - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression)\n",
    "\n",
    "[Spark2.1: Evaluation Metrics - RDD-based API](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#regression-model-evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 3.401714059633831\n",
      "RMSE = 1.8443736225705005\n",
      "R-squared = -3.5295821141199974\n",
      "MAE = 1.6874236102185893\n",
      "Explained variance = 2.575165766694828\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.replace(',', ' ').split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/lpsa.data\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=10000, step=0.01)\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "# MSE = valuesAndPreds \\\n",
    "#     .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "#     .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "# print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# mymean = valuesAndPreds.map(lambda x: x[0]).mean()\n",
    "# MST = valuesAndPreds.map(lambda vp: (vp[0] - mymean)**2).reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "# print(\"My R2 = \"+ str(1-(MSE/MST)))\n",
    "\n",
    "# Get predictions\n",
    "valuesAndPreds = parsedData.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "\n",
    "# Squared Error\n",
    "print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "# R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)\n",
    "\n",
    "# Mean absolute error\n",
    "print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % metrics.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(-0.4307829, [-1.63735562648,-2.00621178481,-1.86242597251,-1.02470580167,-0.522940888712,-0.863171185426,-1.04215728919,-0.864466507337]),\n",
       " LabeledPoint(-0.1625189, [-1.98898046127,-0.722008756122,-0.787896192088,-1.02470580167,-0.522940888712,-0.863171185426,-1.04215728919,-0.864466507337])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mllib using DataFrame api**\n",
    "\n",
    "[Spark Doc](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#linear-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.322925166774,-0.343854803456,1.91560170235,0.0528805868039,0.76596272046,0.0,-0.151053926692,-0.215879303609,0.220253691888]\n",
      "Intercept: 0.1598936844239736\n",
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053554|\n",
      "|  -5.204019455758823|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719486|\n",
      "|  -10.00431602969873|\n",
      "|   2.062397807050484|\n",
      "|  3.1117508432954772|\n",
      "| -15.893608229419382|\n",
      "|  -5.036284254673026|\n",
      "|   6.483215876994333|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "| -2.0049838218725005|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = sqlContext.read.format(\"libsvm\").load(\"data/sample_linear_regression_data.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the response column should be with the name **label**, features should be all in **feature** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=-9.490009878824548, features=SparseVector(10, {0: 0.4551, 1: 0.3664, 2: -0.3826, 3: -0.4458, 4: 0.3311, 5: 0.8067, 6: -0.2624, 7: -0.4485, 8: -0.0727, 9: 0.5658})),\n",
       " Row(label=0.2577820163584905, features=SparseVector(10, {0: 0.8387, 1: -0.127, 2: 0.4998, 3: -0.2269, 4: -0.6452, 5: 0.1887, 6: -0.5805, 7: 0.6519, 8: -0.6556, 9: 0.1749}))]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label| x1| x2|\n",
      "+-----+---+---+\n",
      "|    1|  2|  3|\n",
      "|   -2|  1| 14|\n",
      "|    2|  9| 10|\n",
      "+-----+---+---+\n",
      "\n",
      "+-----+---+---+----------+\n",
      "|label| x1| x2|  features|\n",
      "+-----+---+---+----------+\n",
      "|    1|  2|  3| [2.0,3.0]|\n",
      "|   -2|  1| 14|[1.0,14.0]|\n",
      "|    2|  9| 10|[9.0,10.0]|\n",
      "+-----+---+---+----------+\n",
      "\n",
      "Coefficients: [0.299694346941,-0.176452130024]\n",
      "Intercept: 0.7226251157843492\n",
      "numIterations: 5\n",
      "objectiveHistory: [0.5000000000000001, 0.43797741912312027, 0.19304675519723025, 0.1930464869604053, 0.19304648527659074]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|0.20734258040505105|\n",
      "|-0.5519896423887509|\n",
      "| 0.3446470619837001|\n",
      "+-------------------+\n",
      "\n",
      "RMSE: 0.394320\n",
      "r2: 0.946177\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = sqlContext.createDataFrame([(1,2,3),(-2,1,14),(2,9,10)],['label','x1','x2'])\n",
    "df.show()\n",
    "\n",
    "\n",
    "##### Create train dataframe\n",
    "# to all the features in the column feature using vectorAssembler \n",
    "vectorAssembler = VectorAssembler(inputCols = ['x1','x2'], outputCol = 'features')\n",
    "train_df = vectorAssembler.transform(df)\n",
    "train_df.show()\n",
    "\n",
    "\n",
    "##### Fit model\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_df)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **using pipeline to do data transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conceptual\n",
    "pip = [OneHot, Standardize, assembler, ols]\n",
    "pip.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Map Categorical feature into numeric value**\n",
    "\n",
    "StringIndexer encodes a string column of labels to a column of label indices\n",
    "\n",
    "[Spark Doc](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder-deprecated-since-230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **One Hot Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark Doc](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder-deprecated-since-230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "# the number of inputCols must be the same as #outputCols\n",
    "# input is the column that we want to do the one hot encoding\n",
    "# output is about the name of the one-hot-encoding feautre, which is in sparseVector foramt\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                                 outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive Command Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Access Hive**: in cmd, type *`hive`*\n",
    "* **Run hive script**: xxx.hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **STRUCT**: mystruct.first\n",
    "    * `<first, second>`\n",
    "* **ARRAY**: myarray[0] \n",
    "    * index based\n",
    "* **MAP**: myMap['KEY']\n",
    "    * key based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. employees.csv -> HDFS\n",
    "2. create table & load employees.csv\n",
    "3. drop employees table (Be careful that by dropping the table, HIVE will actually delete the original csv not just the table itself). Instead, we can create an external table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOAD DATA INPATH '...' INTO TABLE employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CREATE TABLE myemployees( \n",
    "name STRING, \n",
    "salary FLOAT, \n",
    "subordinates ARRAY<STRING>, \n",
    "deductions MAP<STRING, FLOAT>, \n",
    "address STRUCT<street:STRING, city:STRING, state:STRING,zip:INT>)\n",
    "ROW FORMAT DELIMITED #\n",
    "FIELDS TERMINATED BY ',' #\n",
    "COLLECTION ITEMS TERMINATED BY '#' # split the struct type item by `#`\n",
    "MAP KEYS TERMINATED BY '-' # split the map type column by `-`\n",
    "LINES TERMINATED BY '\\N'; # separate line by `\\N`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an external table\n",
    "CREATE EXTERNAL TABLE myemployees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select address.city from employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Database in HIVE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each database is a collection of tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](http://www.tutorialspoint.com/hive/hive_create_database.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create database\n",
    "CREATE DATABASE [IF NOT EXISTS] userdb;\n",
    "\n",
    "# show all the databases\n",
    "show databases;\n",
    "\n",
    "# use a certain database, every table we create afterwards will be within the database\n",
    "use databaseName;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create User Defined Fucntions (UDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "* write in java\n",
    "* jar file\n",
    "* import jar file\n",
    "* use UDF as query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='refer'>Reference</a>\n",
    "\n",
    "* [Complete Guide on DataFrame Operations in PySpark](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)\n",
    "* [Introduction to DataFrames - Python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
