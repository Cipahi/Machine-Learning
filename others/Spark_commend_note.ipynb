{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Command Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has two different kinds of APIs\n",
    "\n",
    "* **APIs**\n",
    "    * RDD API: lower level, we should use this when we deal with unstructured data\n",
    "    * DataFrame API: can be related to pandas dataframe in python.\n",
    "        * SparkSQL    \n",
    "* **2 modes**\n",
    "    * shell\n",
    "    * script\n",
    "    \n",
    "each block in the hadoop concept correspond to a partition of RDD.\n",
    "One file correspond to a RDD.\n",
    "\n",
    "* **Pros of Spark to MapReduce**: The main advantage of using Spark is that it can hold a portion of the original data in memory. It's easier to wrote any kinds of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation on Mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Steps**:\n",
    "    1. download version spark-2.3.0-bin-hadoop2.7.tgz from [Spark downloads page](http://spark.apache.org/downloads.html)\n",
    "    2. manually unzip the file\n",
    "    3. sudo mv spark-2.3.0-bin-hadoop2.7 /opt/spark-2.3.0-bin-hadoop2.7.tgz\n",
    "        * used sudo because we need permission to move files into opt folder\n",
    "    4. ln -s /opt/spark-2.3.0-bin-hadoop2.7 /opt/sparkÌ€\n",
    "        * create a shortcut to the actual folder\n",
    "    5. vi ~/.zshrc\n",
    "        * in other Linux the file should be ~/.bashrc\n",
    "        * I downloaded zsh for my command line, that's why I use this file\n",
    "    6. adding the following lines in zshrc file\n",
    "        * export SPARK_HOME=/opt/spark\n",
    "        * export PATH=$SPARK_HOME/bin:$PATH\n",
    "    7. create a new terminal tab, type `pyspark`\n",
    "    8. Used the second method to link pyspark to my jupyter notebook\n",
    "        * There is another and more generalized way to use PySpark in a Jupyter Notebook: use findSpark package to make a Spark Context available in your code.\n",
    "    9. run the sample code in below and it works.    \n",
    "      \n",
    "* **Reference**: [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14219904\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stackoverflow](https://stackoverflow.com/questions/47665491/pyspark-throws-typeerror-textfile-missing-1-required-positional-argument-na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not print anything unless it's an error\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing is going to happen without an action\n",
    "\n",
    "* **Transformations**: RDD to RDD\n",
    "    * **map**: takes every elements of a partition in RDD and do something\n",
    "    * **filter**:\n",
    "    * **reduceByKey**: since it will reduce by key, it's a transformation\n",
    "    * **flatMap**: one element with multiple output\n",
    "    * **sortByKey**: Sort (key, value) RDD by key\n",
    "    * **distinct**: get distinct values in RDD\n",
    "* **Actions**: RDD to various things\n",
    "    * **collect**: take all the data into memory\n",
    "    * **take**: return the first n elements of RDD\n",
    "    * **first**: return the first element of RDD\n",
    "    * **reduce**: reduce function without a key\n",
    "\n",
    "* To take a\n",
    "\n",
    "There are two ways to create RDD.\n",
    "* Read from file\n",
    "* create from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create RDD from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the second parameter indicates the number of parititions in the RDD\n",
    "myRDD = sc.parallelize([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD2 = sc.textFile(\"data/Crimes_-_2001_to_present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location',\n",
       " '10078659,HY267429,05/19/2015 11:57:00 PM,010XX E 79TH ST,143A,WEAPONS VIOLATION,UNLAWFUL POSS OF HANDGUN,STREET,true,false,0624,006,8,44,15,1184626,1852799,2015,05/26/2015 12:42:06 PM,41.751242944,-87.599004724,\"(41.751242944, -87.599004724)\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **map (transformation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row will be mapped to one output \n",
    "\n",
    "* row1 -> list1\n",
    "* row2 -> list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.map(lambda x:x+1)\n",
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.map(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ID',\n",
       "  'Case Number',\n",
       "  'Date',\n",
       "  'Block',\n",
       "  'IUCR',\n",
       "  'Primary Type',\n",
       "  'Description',\n",
       "  'Location Description',\n",
       "  'Arrest',\n",
       "  'Domestic',\n",
       "  'Beat',\n",
       "  'District',\n",
       "  'Ward',\n",
       "  'Community Area',\n",
       "  'FBI Code',\n",
       "  'X Coordinate',\n",
       "  'Y Coordinate',\n",
       "  'Year',\n",
       "  'Updated On',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'Location'],\n",
       " ['10078659',\n",
       "  'HY267429',\n",
       "  '05/19/2015 11:57:00 PM',\n",
       "  '010XX E 79TH ST',\n",
       "  '143A',\n",
       "  'WEAPONS VIOLATION',\n",
       "  'UNLAWFUL POSS OF HANDGUN',\n",
       "  'STREET',\n",
       "  'true',\n",
       "  'false',\n",
       "  '0624',\n",
       "  '006',\n",
       "  '8',\n",
       "  '44',\n",
       "  '15',\n",
       "  '1184626',\n",
       "  '1852799',\n",
       "  '2015',\n",
       "  '05/26/2015 12:42:06 PM',\n",
       "  '41.751242944',\n",
       "  '-87.599004724',\n",
       "  '\"(41.751242944',\n",
       "  ' -87.599004724)\"']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.map(lambda x: x.split(\",\")).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **flatMap (transformation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of row will be one output\n",
    "\n",
    "Each row will be mapped to one output \n",
    "\n",
    "* element1 in row1 -> output1\n",
    "* element2 in row1 -> output2\n",
    "* element3 in row1 -> output3\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Case Number']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.flatMap(lambda x: x.split(\",\")).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 1), ('Case Number', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.flatMap(lambda x: x.split(\",\")).map(lambda x: (x,1)).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **reduce (action)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reduce function, one is current element and another is running sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read csv file into Spark DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/Crimes_-_2001_to_present.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[databricks/spark-csv](https://github.com/databricks/spark-csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801844"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the actual #rows is \n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create DataFrame from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "scratch_df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scratch_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read Json into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customer = sqlContext.read.json(\"data/customer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(address=Row(city='New Orleans', state='LA', street='6649 N Blue Gum St', zip='70116'), first_name='James', last_name='Butterburg'),\n",
       " Row(address=Row(city='Brighton', state='MI', street='4 B Blue Ridge Blvd', zip='48116'), first_name='Josephine', last_name='Darakjy'),\n",
       " Row(address=Row(city='Bridgeport', state='NJ', street='8 W Cerritos Ave #54', zip='08014'), first_name='Art', last_name='Chemel')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **show records, head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|                Date|              Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|    010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...|067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|    026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|\n",
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)'),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **take the first 1000 rows of a Spark Dataframe and return a new dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stackoverflow](https://stackoverflow.com/questions/34206508/is-there-a-way-to-take-the-first-1000-rows-of-a-spark-dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.limit(10).alias(\"test\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get Column Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: boolean, Domestic: boolean, Beat: int, District: int, Ward: int, Community Area: string, FBI Code: string, X Coordinate: int, Y Coordinate: int, Year: int, Updated On: string, Latitude: double, Longitude: double, Location: string]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'int'),\n",
       " ('Case Number', 'string'),\n",
       " ('Date', 'string'),\n",
       " ('Block', 'string'),\n",
       " ('IUCR', 'string'),\n",
       " ('Primary Type', 'string'),\n",
       " ('Description', 'string'),\n",
       " ('Location Description', 'string'),\n",
       " ('Arrest', 'boolean'),\n",
       " ('Domestic', 'boolean'),\n",
       " ('Beat', 'int'),\n",
       " ('District', 'int'),\n",
       " ('Ward', 'int'),\n",
       " ('Community Area', 'string'),\n",
       " ('FBI Code', 'string'),\n",
       " ('X Coordinate', 'int'),\n",
       " ('Y Coordinate', 'int'),\n",
       " ('Year', 'int'),\n",
       " ('Updated On', 'string'),\n",
       " ('Latitude', 'double'),\n",
       " ('Longitude', 'double'),\n",
       " ('Location', 'string')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the shape of dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801844"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Describe the dataset; Get summary statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|summary|                ID|       Case Number|                Date|               Block|             IUCR|     Primary Type|    Description|Location Description|              Beat|          District|              Ward|    Community Area|          FBI Code|      X Coordinate|      Y Coordinate|              Year|          Updated On|           Latitude|          Longitude|            Location|\n",
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|  count|           5801844|           5801841|             5801844|             5801842|          5801844|          5801844|        5801844|             5801009|           5801844|           4601628|           5186940|           5186069|           5801844|           5753924|           5753924|           5801844|             5801678|            5753924|            5753924|             5753924|\n",
      "|   mean| 5478809.816915622|       302402.4375|                null|                null|1134.731230109342|             null|           null|                null|1199.6529617135518|11.304880142419162|22.585335284387327|37.721575177442126|12.340449301040367| 1164505.101389417|1885629.2320944802|2006.9770088613207|                null|  41.84176525667321| -87.67185536981503|                null|\n",
      "| stddev|2587006.9227430555|136858.87616128215|                null|                null|807.5054229686023|             null|           null|                null| 704.5764004952774|6.9362124463080885|13.794330067588891|21.550102471312908|  7.41121340178233|16200.899281313013| 31439.98034154956|4.0104357430230335|                null|0.08646691031758802|0.05895851757394647|                null|\n",
      "|    min|               634|         01G050460|01/01/2001 01:00:...|  0000X  I94/EXIT 12|             0110|            ARSON| $300 AND UNDER|\"CTA \"\"L\"\" PLATFORM\"|               111|                 1|                 1|                  |               01A|           1092697|           1813930|              2001|01/01/2000 09:32:...|       41.644580105|      -87.934324986|(41.644580105, -8...|\n",
      "|    max|          10086206|         ZZZ199957|12/31/2014 12:59:...|175XX W WINSTON C...|             9901|WEAPONS VIOLATION|WIREROOM/SPORTS|                YMCA|              2535|                31|                50|                 9|                26|           1205152|           1951664|              2015|12/31/2014 12:39:...|       42.023024908|      -87.524388789|(42.023024908, -8...|\n",
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Select Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|      ID|                Date|\n",
      "+--------+--------------------+\n",
      "|10078659|05/19/2015 11:57:...|\n",
      "|10078598|05/19/2015 11:50:...|\n",
      "|10078625|05/19/2015 11:47:...|\n",
      "|10078662|05/19/2015 11:46:...|\n",
      "|10078584|05/19/2015 11:45:...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ID','Date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get unique value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|District|\n",
      "+--------+\n",
      "|      31|\n",
      "|      12|\n",
      "|      22|\n",
      "|    null|\n",
      "|       1|\n",
      "|      13|\n",
      "|       6|\n",
      "|      16|\n",
      "|       3|\n",
      "|      20|\n",
      "|       5|\n",
      "|      19|\n",
      "|      15|\n",
      "|       9|\n",
      "|      17|\n",
      "|       4|\n",
      "|       8|\n",
      "|      23|\n",
      "|       7|\n",
      "|      10|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('District').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('District').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Explode array data into rows in spark; Dividing complex rows of dataframe to simple rows in Pyspark; Explode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default into of explode method should be a list [1,2,3]. If the value in the row is not in list format, we need to specify how do we want to split the value. For exmaple, if the format is a string \"1,2,3\". We need to write it as `explode(split(df.col3, \",\"))`\n",
    "\n",
    "* **First way**: use **withColumn**+**explode**\n",
    "    * Note: if we want to add a new column, use this.\n",
    "\n",
    "[StackOverFlow: Explode array data into rows in spark](https://stackoverflow.com/questions/44436856/explode-array-data-into-rows-in-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second way**: use **select**+**explode**+**alias**\n",
    "    * select\n",
    "    * explode: to separate \n",
    "    * alias: to specify the name of new column\n",
    "    * Note: if we want to select and add a new column at the same time.\n",
    "\n",
    "[StackOverFlow: Explode in PySpark](https://stackoverflow.com/questions/38210507/explode-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "explode_df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   A|   1|\n",
      "|   1|   A|   2|\n",
      "|   1|   A|   3|\n",
      "|   2|   B|   3|\n",
      "|   2|   B|   5|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+---------+-------+\n",
      "|col1|col2|     col3|newcol3|\n",
      "+----+----+---------+-------+\n",
      "|   1|   A|[1, 2, 3]|      1|\n",
      "|   1|   A|[1, 2, 3]|      2|\n",
      "|   1|   A|[1, 2, 3]|      3|\n",
      "|   2|   B|   [3, 5]|      3|\n",
      "|   2|   B|   [3, 5]|      5|\n",
      "+----+----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cover out the original col3\n",
    "explode_df.withColumn(\"col3\", explode(explode_df.col3)).show()\n",
    "# create a new column\n",
    "explode_df.withColumn(\"newcol3\", explode(explode_df.col3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|first_name|       city|\n",
      "+----------+-----------+\n",
      "|     James|New Orleans|\n",
      "| Josephine|   Brighton|\n",
      "|       Art| Bridgeport|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.select(\"first_name\",\n",
    "                explode(\"address\").alias(\"contactInfo\")\n",
    "                \"address.city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n",
      "|col1|col2|newcol3|\n",
      "+----+----+-------+\n",
      "|   1|   A|      1|\n",
      "|   1|   A|      2|\n",
      "|   1|   A|      3|\n",
      "|   2|   B|      3|\n",
      "|   2|   B|      5|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df.select(\"col1\",\n",
    "                  \"col2\",\n",
    "                  explode(explode_df.col3).alias(\"newcol3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of non-list format column value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|col1|col2| col3|\n",
      "+----+----+-----+\n",
      "|   1|   A|1,2,3|\n",
      "|   2|   B|  3,5|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df2 = sqlContext.createDataFrame([(1, \"A\", \"1,2,3\"), (2, \"B\", \"3,5\")],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-------+\n",
      "|col1|col2| col3|newcol3|\n",
      "+----+----+-----+-------+\n",
      "|   1|   A|1,2,3|      1|\n",
      "|   1|   A|1,2,3|      2|\n",
      "|   1|   A|1,2,3|      3|\n",
      "|   2|   B|  3,5|      3|\n",
      "|   2|   B|  3,5|      5|\n",
      "+----+----+-----+-------+\n",
      "\n",
      "+----+----+-------+\n",
      "|col1|col2|newcol3|\n",
      "+----+----+-------+\n",
      "|   1|   A|      1|\n",
      "|   1|   A|      2|\n",
      "|   1|   A|      3|\n",
      "|   2|   B|      3|\n",
      "|   2|   B|      5|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df2.withColumn(\"newcol3\", explode(split(explode_df2.col3, \",\"))).show()\n",
    "\n",
    "explode_df2.select(\"col1\",\n",
    "                   \"col2\",\n",
    "                   explode(split(explode_df2.col3, \",\")).alias(\"newcol3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Convert pyspark string to date format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow: Convert pyspark string to date format](https://stackoverflow.com/questions/38080748/convert-pyspark-string-to-date-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|      ID|                 dt|\n",
      "+--------+-------------------+\n",
      "|10078659|2015-05-19 11:57:00|\n",
      "|10078598|2015-05-19 11:50:00|\n",
      "|10078625|2015-05-19 11:47:00|\n",
      "|10078662|2015-05-19 11:46:00|\n",
      "|10078584|2015-05-19 11:45:00|\n",
      "|10078629|2015-05-19 11:40:00|\n",
      "|10079225|2015-05-19 11:30:00|\n",
      "|10078594|2015-05-19 11:30:00|\n",
      "|10080768|2015-05-19 11:30:00|\n",
      "|10078618|2015-05-19 11:30:00|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"ID\",\n",
    "           to_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss').alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn(\"datetime\",\n",
    "           to_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|           datetime|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|2015-05-19 11:57:00|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|2015-05-19 11:50:00|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|2015-05-19 11:47:00|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|2015-05-19 11:46:00|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|2015-05-19 11:45:00|\n",
      "|10078629|   HY267393|05/19/2015 11:40:...|013XX S LAWNDALE AVE|0454|             BATTERY|AGG PO HANDS NO/M...|              STREET|  true|   false|1011|      10|  24|            29|     08B|     1151957|     1893696|2015|05/26/2015 12:42:...|41.864172884|-87.717647622|(41.864172884, -8...|2015-05-19 11:40:00|\n",
      "|10079225|   HY267395|05/19/2015 11:30:...|   064XX S LAFLIN ST|0497|             BATTERY|AGGRAVATED DOMEST...|           APARTMENT| false|    true| 725|       7|  17|            67|     04B|     1167397|     1862229|2015|05/26/2015 12:42:...|41.777506284|-87.661870632|(41.777506284, -8...|2015-05-19 11:30:00|\n",
      "|10078594|   HY267388|05/19/2015 11:30:...|   021XX W NORTH AVE|1305|     CRIMINAL DAMAGE| CRIMINAL DEFACEMENT|            SIDEWALK|  true|   false|1434|      14|  32|            24|      14|     1162022|     1910669|2015|05/26/2015 12:42:...|  41.9105442|-87.680225036|(41.9105442, -87....|2015-05-19 11:30:00|\n",
      "|10080768|   HY269123|05/19/2015 11:30:...| 008XX N KILDARE AVE|0820|               THEFT|      $500 AND UNDER|           RESIDENCE| false|   false|1111|      11|  37|            23|      06|     1147592|     1905491|2015|05/26/2015 12:42:...|41.896624505|-87.733368852|(41.896624505, -8...|2015-05-19 11:30:00|\n",
      "|10078618|   HY267392|05/19/2015 11:30:...|062XX S KOMENSKY AVE|0320|             ROBBERY|STRONGARM - NO WE...|            SIDEWALK| false|   false| 813|       8|  13|            65|      03|     1150396|     1863112|2015|05/26/2015 12:42:...|41.780276746|-87.724174049|(41.780276746, -8...|2015-05-19 11:30:00|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Spark DataFrame TimestampType - how to get Year, Month, Day values from field?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [StackOverFlow: year/month/dayofmonth](https://stackoverflow.com/questions/30949202/spark-dataframe-timestamptype-how-to-get-year-month-day-values-from-field)\n",
    "* [StackOverFlow: weekday](https://stackoverflow.com/questions/38928919/how-to-get-the-weekday-from-day-of-month-using-pyspark/38931967)\n",
    "* [pyspark sql function](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year/month/day of month\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, to_date\n",
    "df4 = df3.withColumn(\"year\", year(df3.datetime))\n",
    "df4 = df4.withColumn(\"month\", month(df4.datetime))\n",
    "df4 = df4.withColumn(\"dayofmonth\", dayofmonth(df4.datetime))\n",
    "df4 = df4.withColumn(\"hour\", hour(df4.datetime))\n",
    "df4 = df4.withColumn(\"dayofmonth\", dayofmonth(df4.datetime))\n",
    "df4 = df4.withColumn(\"minute\", minute(df4.datetime))                                        \n",
    "df4 = df4.withColumn(\"todate\", to_date(df4.datetime))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)', datetime=datetime.datetime(2015, 5, 19, 11, 57), month=5, dayofmonth=19, hour=11, minute=57, todate=datetime.date(2015, 5, 19)),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)', datetime=datetime.datetime(2015, 5, 19, 11, 50), month=5, dayofmonth=19, hour=11, minute=50, todate=datetime.date(2015, 5, 19)),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)', datetime=datetime.datetime(2015, 5, 19, 11, 47), month=5, dayofmonth=19, hour=11, minute=47, todate=datetime.date(2015, 5, 19))]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+\n",
      "|           datetime|dow_number|dow_string|\n",
      "+-------------------+----------+----------+\n",
      "|2015-05-19 11:57:00|         2|       Tue|\n",
      "|2015-05-19 11:50:00|         2|       Tue|\n",
      "|2015-05-19 11:47:00|         2|       Tue|\n",
      "|2015-05-19 11:46:00|         2|       Tue|\n",
      "|2015-05-19 11:45:00|         2|       Tue|\n",
      "|2015-05-19 11:40:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "+-------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get weekday string /number\n",
    "from pyspark.sql.functions import date_format\n",
    "df4.select('datetime', date_format('datetime', 'u').alias('dow_number'), date_format('datetime', 'E').alias('dow_string')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|year|          Updated On|    Latitude|    Longitude|            Location|           datetime|month|dayofmonth|dow_string|dow_number|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|2015-05-19 11:57:00|    5|        19|       Tue|         2|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|2015-05-19 11:50:00|    5|        19|       Tue|         2|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|2015-05-19 11:47:00|    5|        19|       Tue|         2|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|2015-05-19 11:46:00|    5|        19|       Tue|         2|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|2015-05-19 11:45:00|    5|        19|       Tue|         2|\n",
      "|10078629|   HY267393|05/19/2015 11:40:...|013XX S LAWNDALE AVE|0454|             BATTERY|AGG PO HANDS NO/M...|              STREET|  true|   false|1011|      10|  24|            29|     08B|     1151957|     1893696|2015|05/26/2015 12:42:...|41.864172884|-87.717647622|(41.864172884, -8...|2015-05-19 11:40:00|    5|        19|       Tue|         2|\n",
      "|10079225|   HY267395|05/19/2015 11:30:...|   064XX S LAFLIN ST|0497|             BATTERY|AGGRAVATED DOMEST...|           APARTMENT| false|    true| 725|       7|  17|            67|     04B|     1167397|     1862229|2015|05/26/2015 12:42:...|41.777506284|-87.661870632|(41.777506284, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10078594|   HY267388|05/19/2015 11:30:...|   021XX W NORTH AVE|1305|     CRIMINAL DAMAGE| CRIMINAL DEFACEMENT|            SIDEWALK|  true|   false|1434|      14|  32|            24|      14|     1162022|     1910669|2015|05/26/2015 12:42:...|  41.9105442|-87.680225036|(41.9105442, -87....|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10080768|   HY269123|05/19/2015 11:30:...| 008XX N KILDARE AVE|0820|               THEFT|      $500 AND UNDER|           RESIDENCE| false|   false|1111|      11|  37|            23|      06|     1147592|     1905491|2015|05/26/2015 12:42:...|41.896624505|-87.733368852|(41.896624505, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10078618|   HY267392|05/19/2015 11:30:...|062XX S KOMENSKY AVE|0320|             ROBBERY|STRONGARM - NO WE...|            SIDEWALK| false|   false| 813|       8|  13|            65|      03|     1150396|     1863112|2015|05/26/2015 12:42:...|41.780276746|-87.724174049|(41.780276746, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"dow_string\", date_format(df4.datetime, 'E')).withColumn(\"dow_number\", date_format(df4.datetime, 'u')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **calculate pair wise frequency of categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|Beat_District| 10| 11| 14| 24|  3|  4|  6|  7|  8|  9|\n",
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|         2432|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|          421|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
      "|         1111|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|          725|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
      "|         1434|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|\n",
      "|         1011|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|          314|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|          813|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
      "|          624|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|          935|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.crosstab('Beat', 'District')\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Drop rows with NA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Change Column Type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[change a Dataframe column from String type to Double type in pyspark](https://stackoverflow.com/questions/32284620/how-to-change-a-dataframe-column-from-string-type-to-double-type-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Beat_District: string, 10: bigint, 11: bigint, 14: bigint, 24: bigint, 3: bigint, 4: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Beat_District: double, 10: bigint, 11: bigint, 14: bigint, 24: bigint, 3: bigint, 4: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.withColumn(\"Beat_District\", df5[\"Beat_District\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **GroupBy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax is similar to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort(df.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the NULL/NaN percentage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/33900726/count-number-of-non-nan-entries-in-each-column-of-spark-dataframe-with-pyspark?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, lit, sum\n",
    "\n",
    "def count_not_null(c, nan_as_null=False):\n",
    "    \"\"\"Use conversion between boolean and integer\n",
    "    - False -> 0\n",
    "    - True ->  1\n",
    "    \"\"\"\n",
    "    pred = col(c).isNotNull() & (~isnan(c) if nan_as_null else lit(True))\n",
    "    return sum(pred.cast(\"integer\")).alias(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get number of not null values in each column\n",
    "df.agg(*[count_not_null(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get percentage of not null values in each column\n",
    "exprs = [(count_not_null(c) / count(\"*\")).alias(c) for c in df.columns]\n",
    "df.agg(*exprs).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filter NULL rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.where(col(\"month\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df['age']>24).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Replace values in a certain column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace AM, PM with space in Date column\n",
    "df.withColumn('Date', regexp_replace('Date', ' PM', '')).withColumn('Date', regexp_replace('Date', ' AM', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sparkSQL work with dataframe api, we need to firstly transform our file/RDD into spark dataframe\n",
    "* RDD -> dataframe\n",
    "* file -> dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Transform RDD into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location',\n",
       " '10078659,HY267429,05/19/2015 11:57:00 PM,010XX E 79TH ST,143A,WEAPONS VIOLATION,UNLAWFUL POSS OF HANDGUN,STREET,true,false,0624,006,8,44,15,1184626,1852799,2015,05/26/2015 12:42:06 PM,41.751242944,-87.599004724,\"(41.751242944, -87.599004724)\"',\n",
       " '10078598,HY267408,05/19/2015 11:50:00 PM,067XX N SHERIDAN RD,3731,INTERFERENCE WITH PUBLIC OFFICER,OBSTRUCTING IDENTIFICATION,STREET,true,false,2432,024,49,1,24,1167071,1944859,2015,05/26/2015 12:42:06 PM,42.004255918,-87.660691083,\"(42.004255918, -87.660691083)\"',\n",
       " '10078625,HY267417,05/19/2015 11:47:00 PM,026XX E 77TH ST,2170,NARCOTICS,POSSESSION OF DRUG EQUIPMENT,STREET,true,false,0421,004,7,43,18,1195299,1854463,2015,05/26/2015 12:42:06 PM,41.755552462,-87.559839339,\"(41.755552462, -87.559839339)\"',\n",
       " '10078662,HY267423,05/19/2015 11:46:00 PM,015XX E 62ND ST,051A,ASSAULT,AGGRAVATED: HANDGUN,APARTMENT,false,true,0314,003,5,42,04A,1187377,1864316,2015,05/26/2015 12:42:06 PM,41.782781732,-87.588558362,\"(41.782781732, -87.588558362)\"']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD3 = myRDD2.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ID',\n",
       "  'Case Number',\n",
       "  'Date',\n",
       "  'Block',\n",
       "  'IUCR',\n",
       "  'Primary Type',\n",
       "  'Description',\n",
       "  'Location Description',\n",
       "  'Arrest',\n",
       "  'Domestic',\n",
       "  'Beat',\n",
       "  'District',\n",
       "  'Ward',\n",
       "  'Community Area',\n",
       "  'FBI Code',\n",
       "  'X Coordinate',\n",
       "  'Y Coordinate',\n",
       "  'Year',\n",
       "  'Updated On',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'Location'],\n",
       " ['10078659',\n",
       "  'HY267429',\n",
       "  '05/19/2015 11:57:00 PM',\n",
       "  '010XX E 79TH ST',\n",
       "  '143A',\n",
       "  'WEAPONS VIOLATION',\n",
       "  'UNLAWFUL POSS OF HANDGUN',\n",
       "  'STREET',\n",
       "  'true',\n",
       "  'false',\n",
       "  '0624',\n",
       "  '006',\n",
       "  '8',\n",
       "  '44',\n",
       "  '15',\n",
       "  '1184626',\n",
       "  '1852799',\n",
       "  '2015',\n",
       "  '05/26/2015 12:42:06 PM',\n",
       "  '41.751242944',\n",
       "  '-87.599004724',\n",
       "  '\"(41.751242944',\n",
       "  ' -87.599004724)\"']]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string, _16: string, _17: string, _18: string, _19: string, _20: string, _21: string, _22: string]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: string, Domestic: string, Beat: string, District: string, Ward: string, Community Area: string, FBI Code: string, X Coordinate: string, Y Coordinate: string, Year: string, Updated On: string, Latitude: string, Longitude: string, Location: string]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.toDF(['ID','Case Number','Date','Block','IUCR','Primary Type','Description','Location Description','Arrest','Domestic','Beat','District','Ward','Community Area','FBI Code','X Coordinate','Y Coordinate','Year','Updated On','Latitude','Longitude','Location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Write SQL code in SparkSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to write SQL code on SparkSQL\n",
    "df.registerTempTable(\"mydf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it will return a dataframe not a sql tempTable\n",
    "sqlContect.sql(\"select * from mydf\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Basic Statistics - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-statistics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we want to use Mllib, we need to convert our data into a specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(['0,1,2,3','1,4,3,4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,1,2,3', '1,4,3,4']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(\",\")]\n",
    "    return Vectors.dense(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 1.0, 2.0, 3.0]), DenseVector([1.0, 4.0, 3.0, 4.0])]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use pretty much anything provided in mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  2.5  2.5  3.5]\n",
      "[ 0.5  4.5  0.5  0.5]\n",
      "[ 1.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "summary = Statistics.colStats(rdd2)\n",
    "print(summary.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary.variance())  # column-wise variance\n",
    "print(summary.numNonzeros())  # number of nonzeros in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "* [Complete Guide on DataFrame Operations in PySpark](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)\n",
    "* [Introduction to DataFrames - Python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
