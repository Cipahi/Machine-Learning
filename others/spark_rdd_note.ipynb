{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD Command Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* [Intro](#intro)\n",
    "* [Spark RDD API](#rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='intro'>Intro</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has two different kinds of APIs\n",
    "\n",
    "* **APIs**\n",
    "    * RDD API: lower level, we should use this when we deal with unstructured data\n",
    "    * DataFrame API: can be related to pandas dataframe in python.\n",
    "        * SparkSQL    \n",
    "* **2 modes**\n",
    "    * shell\n",
    "    * script\n",
    "    \n",
    "each block in the hadoop concept correspond to a partition of RDD.\n",
    "One file correspond to a RDD.\n",
    "\n",
    "* **Pros of Spark to MapReduce**: The main advantage of using Spark is that it can hold a portion of the original data in memory. It's easier to wrote any kinds of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='rdd'>Spark RDD API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing is going to happen without an action\n",
    "\n",
    "* **Transformations**: RDD to RDD\n",
    "    * **map**: takes every elements of a partition in RDD and do something\n",
    "    * **filter**:\n",
    "    * **reduceByKey**: since it will reduce by key, it's a transformation\n",
    "    * **flatMap**: one element with multiple output\n",
    "    * **sortByKey**: Sort (key, value) RDD by key\n",
    "    * **sortBy**: sort by custom condition or self select value\n",
    "    * **distinct**: get distinct values in RDD\n",
    "    * **groupBy**\n",
    "    * **groupByKey**    \n",
    "    * **mapValues**\n",
    "    * **join**: inner join\n",
    "        * **leftOuterJoin**: left join    \n",
    "        * **rightOuterJoin**: right join    \n",
    "        * **fullOuterJoin**: outer join  \n",
    "    * **mapPartitions**: do map function for each partition. The input is all the records in each partition and the output is a single element\n",
    "* **Actions**: RDD to various things\n",
    "    * **collect**: take all the data into memory\n",
    "    * **take**: return the first n elements of RDD\n",
    "    * **first**: return the first element of RDD\n",
    "    * **reduce**: reduce function without a key\n",
    "\n",
    "* To take a\n",
    "\n",
    "There are two ways to create RDD.\n",
    "* Read from file\n",
    "* create from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create RDD from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the second parameter indicates the number of parititions in the RDD\n",
    "myRDD = sc.parallelize([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the number of records in each partition using mapParititions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countRecord(x):\n",
    "    count = 0\n",
    "    for i in x:\n",
    "        count = count + 1\n",
    "    return [count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(countRecord).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isEven(x):\n",
    "    \"\"\"get all even number and output as list\"\"\"\n",
    "    resultList = []\n",
    "    for i in x:\n",
    "        if i%2==0:\n",
    "            resultList.append(i)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(isEven).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Return vs Yield using mapPartitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return: returns an object that we can be iterated as many time as we want\n",
    "def customMapPartitions(x):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        res.append(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yield: returns an object that can only be iterated once\n",
    "def customMapPartitions2(x):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        res.append(i)\n",
    "    yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(customMapPartitions).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.mapPartitions(customMapPartitions2).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **broadcast variable in spark, similar to distributed cache in map reduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # after running this, the variable is also been broadcasted in the memory of each server\n",
    "toShare = sc.broadcst([1,2,3,4])\n",
    "toShare.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD2 = sc.textFile(\"data/Crimes_-_2001_to_present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location',\n",
       " '10078659,HY267429,05/19/2015 11:57:00 PM,010XX E 79TH ST,143A,WEAPONS VIOLATION,UNLAWFUL POSS OF HANDGUN,STREET,true,false,0624,006,8,44,15,1184626,1852799,2015,05/26/2015 12:42:06 PM,41.751242944,-87.599004724,\"(41.751242944, -87.599004724)\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **map (transformation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row will be mapped to one output \n",
    "\n",
    "* row1 -> list1\n",
    "* row2 -> list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.map(lambda x:x+1)\n",
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.map(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ID',\n",
       "  'Case Number',\n",
       "  'Date',\n",
       "  'Block',\n",
       "  'IUCR',\n",
       "  'Primary Type',\n",
       "  'Description',\n",
       "  'Location Description',\n",
       "  'Arrest',\n",
       "  'Domestic',\n",
       "  'Beat',\n",
       "  'District',\n",
       "  'Ward',\n",
       "  'Community Area',\n",
       "  'FBI Code',\n",
       "  'X Coordinate',\n",
       "  'Y Coordinate',\n",
       "  'Year',\n",
       "  'Updated On',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'Location'],\n",
       " ['10078659',\n",
       "  'HY267429',\n",
       "  '05/19/2015 11:57:00 PM',\n",
       "  '010XX E 79TH ST',\n",
       "  '143A',\n",
       "  'WEAPONS VIOLATION',\n",
       "  'UNLAWFUL POSS OF HANDGUN',\n",
       "  'STREET',\n",
       "  'true',\n",
       "  'false',\n",
       "  '0624',\n",
       "  '006',\n",
       "  '8',\n",
       "  '44',\n",
       "  '15',\n",
       "  '1184626',\n",
       "  '1852799',\n",
       "  '2015',\n",
       "  '05/26/2015 12:42:06 PM',\n",
       "  '41.751242944',\n",
       "  '-87.599004724',\n",
       "  '\"(41.751242944',\n",
       "  ' -87.599004724)\"']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.map(lambda x: x.split(\",\")).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **flatMap (transformation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of row will be one output\n",
    "\n",
    "Each row will be mapped to one output \n",
    "\n",
    "* element1 in row1 -> output1\n",
    "* element2 in row1 -> output2\n",
    "* element3 in row1 -> output3\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(['This is great great great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split(\" \")).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 1), ('is', 1), ('great', 1), ('great', 1), ('great', 1)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split(\" \")).map(lambda x: (x,1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **SortBy multiple columns/values conditions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [StackOverFlow](https://stackoverflow.com/questions/36963319/spark-rdd-sort-by-two-values)\n",
    "* [takeOrdered descending Pyspark](https://stackoverflow.com/questions/30787635/takeordered-descending-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sc.parallelize([['panda',(3,4)],['dog',(2,7)],['dog',(3,6)],['panda',(2,2)],['dog',(1,2)]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['panda', (3, 4)],\n",
       " ['dog', (2, 7)],\n",
       " ['dog', (3, 6)],\n",
       " ['panda', (2, 2)],\n",
       " ['dog', (1, 2)]]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog', (1, 2)],\n",
       " ['dog', (2, 7)],\n",
       " ['dog', (3, 6)],\n",
       " ['panda', (2, 2)],\n",
       " ['panda', (3, 4)]]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ascending order\n",
    "temp.sortBy(lambda r: (r[0][0], int(r[1][0]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog', (3, 6)],\n",
       " ['dog', (2, 7)],\n",
       " ['dog', (1, 2)],\n",
       " ['panda', (3, 4)],\n",
       " ['panda', (2, 2)]]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descending order of the second value\n",
    "temp.sortBy(lambda r: (r[0][0], -int(r[1][0]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **reduce (action)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reduce function, one is current element and another is running sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Calculate average/mean for each key**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/40087483/spark-average-of-values-instead-of-sum-in-reducebykey-using-scala?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sc.parallelize([['panda',10],['panda',8],['dog',9],['cat',5],['dog',12]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp2 = temp.map(lambda x: (x[0],(x[1],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (10, 1)),\n",
       " ('panda', (8, 1)),\n",
       " ('dog', (9, 1)),\n",
       " ('cat', (5, 1)),\n",
       " ('dog', (12, 1))]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x: current value, in our case, it's (1,1) or (2,1)...etc\n",
    "* y: running sum. \n",
    "\n",
    "* x[0]: indicate the current value using to calculate the sum\n",
    "* x[1]: indicate the current value using to calculate the total count\n",
    "\n",
    "Therefore, in the key level for panda\n",
    "* In the first iteration\n",
    "    * x[0] = 10, x[1] = 1, y[0] = 0, y[1] = 0\n",
    "* In the second iteration\n",
    "    * x[0] = 8, x[1] = 1, y[0] = 10, y[1] = 1\n",
    "* No more records\n",
    "    * y[0] = 18, y[1] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp3 = temp2.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (18, 2)), ('cat', (5, 1)), ('dog', (21, 2))]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp4 = temp3.map(lambda r: (r[0], r[1][0]/r[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', 9.0), ('cat', 5.0), ('dog', 10.5)]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', 9.0), ('cat', 5.0), ('dog', 10.5)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in one line\n",
    "temp.map(lambda x: (x[0],(x[1],1))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).map(lambda r: (r[0], r[1][0]/r[1][1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **groupByKey, mapValues: append values into a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sc.parallelize([['panda',10],['panda',8],['dog',12],['cat',5],['dog',1]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['panda', 10], ['panda', 8], ['dog', 12], ['cat', 5], ['dog', 1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', [10, 8]), ('cat', [5]), ('dog', [12, 1])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', [5]), ('panda', [8, 10]), ('dog', [1, 12])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.sortBy(lambda x: x[1]).groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', [10, 8]), ('cat', [5]), ('dog', [12, 1])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use may and reduceByKey. It looks more messy \n",
    "# https://stackoverflow.com/questions/27002161/reduce-a-key-value-pair-into-a-key-list-pair-with-apache-spark\n",
    "temp.map(lambda x: (x[0], [x[1]])).reduceByKey(lambda p,q: p+q).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **produce RDD[(X, X)] of all possible combinations from RDD[X], cartesian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/26557873/spark-produce-rddx-x-of-all-possible-combinations-from-rddx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['panda', 10], ['lion', 8], ['dog', 12], ['cat', 5], ['bird', 1]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sc.parallelize([['panda',10],['lion',8],['dog',12],['cat',5],['bird',1]], 2)\n",
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['panda', 10], ['lion', 8]),\n",
       " (['lion', 8], ['panda', 10]),\n",
       " (['panda', 10], ['dog', 12]),\n",
       " (['panda', 10], ['cat', 5]),\n",
       " (['lion', 8], ['dog', 12]),\n",
       " (['lion', 8], ['cat', 5]),\n",
       " (['panda', 10], ['bird', 1]),\n",
       " (['lion', 8], ['bird', 1]),\n",
       " (['dog', 12], ['panda', 10]),\n",
       " (['dog', 12], ['lion', 8]),\n",
       " (['cat', 5], ['panda', 10]),\n",
       " (['cat', 5], ['lion', 8]),\n",
       " (['bird', 1], ['panda', 10]),\n",
       " (['bird', 1], ['lion', 8]),\n",
       " (['dog', 12], ['cat', 5]),\n",
       " (['cat', 5], ['dog', 12]),\n",
       " (['dog', 12], ['bird', 1]),\n",
       " (['cat', 5], ['bird', 1]),\n",
       " (['bird', 1], ['dog', 12]),\n",
       " (['bird', 1], ['cat', 5])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it will still have duplicated rows. Figure it out later\n",
    "temp.cartesian(temp).filter(lambda x: x[0]!=x[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1 = sc.parallelize([['panda',10],['lion',8],['dog',12],['cat',5],['bird',1]], 2)\n",
    "temp2 = sc.parallelize([['panda',10],['panda',8],['dog',12],['cat',5],['dog',1]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (10, 10)),\n",
       " ('panda', (10, 8)),\n",
       " ('cat', (5, 5)),\n",
       " ('dog', (12, 12)),\n",
       " ('dog', (12, 1))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.join(temp2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp3 = sc.parallelize([['panda',(10,'black')],['lion',(8,'yellow')],['dog',(12,'white')],['cat',(5,'grey')],['bird',(1,'green')]], 2)\n",
    "temp4 = sc.parallelize([['panda',10],['dog',20],['horse',20]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (10, (10, 'black'))), ('dog', (20, (12, 'white')))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner join\n",
    "temp3.join(temp4).collect()\n",
    "temp4.join(temp3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', ((10, 'black'), 10)),\n",
       " ('lion', ((8, 'yellow'), None)),\n",
       " ('cat', ((5, 'grey'), None)),\n",
       " ('bird', ((1, 'green'), None)),\n",
       " ('dog', ((12, 'white'), 20))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# left join\n",
    "temp3.leftOuterJoin(temp4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', ((10, 'black'), 10)),\n",
       " ('horse', (None, 20)),\n",
       " ('dog', ((12, 'white'), 20))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right join\n",
    "temp3.rightOuterJoin(temp4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', ((10, 'black'), 10)),\n",
       " ('lion', ((8, 'yellow'), None)),\n",
       " ('cat', ((5, 'grey'), None)),\n",
       " ('bird', ((1, 'green'), None)),\n",
       " ('horse', (None, 20)),\n",
       " ('dog', ((12, 'white'), 20))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right join\n",
    "temp3.fullOuterJoin(temp4).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
